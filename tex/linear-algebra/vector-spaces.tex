\chapter{Vector Spaces}\label{chap:vector-spaces}
\section{Definition of Vector Space}
\begin{notation}
A field is denoted by $\FF$, which can mean either $\RR$ or $\CC$. $\FF^n$ is the set of $n$-tuples whose elements belong to $\FF$:
\[\FF^n\coloneqq\{(x_1,\dots,x_n)\mid x_i\in\FF\}\]
For $(x_1,\dots,x_n)\in\FF^n$ and $i=1,\dots,n$, we say that $x_i$ is the $i$-th coordinate of $(x_1,\dots,x_n)$.
\end{notation}

\begin{definition}[Vector space]
$V$ is a \vocab{vector space}\index{vector space} over $\FF$ if the following properties hold:
\begin{enumerate}[label=(\roman*)]
\item Addition is commutative: $u+v=v+u$ for all $u,v\in V$
\item Addition is associative: $(u+v)+w=u+(v+w)$ for all $u,v,w\in V$

Multiplication is associative: $(ab)v=a(bv)$ for all $v\in V$, $a,b\in\FF$
\item Additive identity: there exists $\vb{0}\in V$ such that $v+\vb{0}=v$ for all $v\in V$
\item Additive inverse: for every $v\in V$, there exists $w\in V$ such that $v+w=\vb{0}$
\item Multiplicative identity: $1v=v$ for all $v\in V$
\item Distributive properties: $a(u+v)=au+av$ and $(a+b)v=av+bv$ for all $a,b,\in\FF$ and $u,v\in V$
\end{enumerate}
\end{definition}

\begin{notation}
For the rest of this text, $V$ denotes a vector space over $\FF$.
\end{notation}

\begin{example}
$\RR^n$ is a vector space over $\RR$, $\CC^n$ is a vector space over $\CC$.
\end{example}

Elements of a vector space are called \emph{vectors} or \emph{points}.

The scalar multiplication in a vector space depends on $\FF$. Thus when we
need to be precise, we will say that $V$ is a vector space over $\FF$ instead of saying simply that $V$ is a vector space. For example, $\RR^n$ is a vector space over $\RR$, and $\CC^n$ is a vector space over $\CC$. A vector space over $\RR$ is called a \emph{real vector space}\index{vector space!real vector space}; a vector space over $\CC$ is called a \emph{complex vector space}\index{vector space!complex vector space}.

\begin{proposition}[Uniqueness of additive identity]
A vector space has a unique additive identity.
\end{proposition}

\begin{proof}
Suppose otherwise, then $\vb{0}$ and $\vb{0}^\prime$ are additive identities of $V$. Then
\[\vb{0}^\prime=\vb{0}^\prime+\vb{0}=\vb{0}+\vb{0}^\prime=\vb{0}\]
where the first equality holds because $\vb{0}$ is an additive identity, the second equality comes from commutativity, and the third equality holds because $\vb{0}^\prime$ is an additive identity. Thus $\vb{0}^\prime=\vb{0}$.
\end{proof}

\begin{proposition}[Uniqueness of additive inverse]
Every element in a vector space has a unique additive inverse.
\end{proposition}

\begin{proof}
Suppose otherwise, then for $v\in V$, $w$ and $w^\prime$ are additive inverses of $v$. Then
\[w=w+\vb{0}=w+(v+w^\prime)=(w+v)+w^\prime=\vb{0}+w^\prime=w^\prime.\]
Thus $w=w^\prime$.
\end{proof}

Because additive inverses are unique, the following notation now makes sense.

\begin{notation}
Let $v,w\in V$. Then $-v$ denotes the additive inverse of $v$; $w-v$ is defined to be $w+(-v)$.
\end{notation}

We now prove some seemingly trivial facts.

\begin{proposition} \
\begin{enumerate}[label=(\roman*)]
\item For every $v\in V$, $0v=\vb{0}$.
\item For every $a\in\FF$, $a\vb{0}=\vb{0}$.
\item For every $v\in V$, $(-1)v=-v$.
\end{enumerate}
\end{proposition}

\begin{proof} \
\begin{enumerate}[label=(\roman*)]
\item For $v\in V$, we have
\[0v=(0+0)v=0v+0v.\]
Adding the additive inverse of $0v$ to both sides of the equation gives $\vb{0}=0v$.

\item For $a\in\FF$, we have
\[a\vb{0}=a(\vb{0}+\vb{0})=a\vb{0}+a\vb{0}.\]
Adding the additive inverse of $a\vb{0}$ to both sides of the equation gives $\vb{0}=a\vb{0}$.

\item For $v\in V$, we have
\[v+(-1)v=1v+(-1)v=(1+(-1))v=0v=\vb{0}.\]
Since $v+(-1)v=\vb{0}$, $(-1)v$ is the additive inverse of $v$.
\end{enumerate}
\end{proof}

\begin{example}
$\FF^\infty$ is defined to be the set of all sequences of elements of $\FF$:
\[\FF^\infty\coloneqq\{(x_1,x_2,\dots)\mid x_i\in\FF\}\]
\begin{itemize}
\item Addition on $\FF^\infty$ is defined by
\[(x_1,x_2,\dots)+(y_1,y_2,\dots)=(x_1+y_1,x_2+y_2,\dots)\]
\item Scalar multiplication on $\FF^\infty$ is defined by
\[\lambda(x_1,x_2,\dots)=(\lambda x_1,\lambda x_2,\dots)\]
\end{itemize}

Verify that $\FF^\infty$ becomes a vector space over $\FF$. Also verify that the additive identity in $\FF^\infty$ is $\vb{0}=(0,0,\dots)$.
\end{example}

Our next example of a vector space involves a set of functions.

\begin{example}
If $S$ is a set, $\FF^S\coloneqq\{f\mid f:S\to\FF\}$.
\begin{itemize}
\item Addition on $\FF^S$ is defined by 
\[(f+g)(x)=f(x)+g(x)\quad(\forall x\in S)\]
for all $f,g\in\FF^S$.
\item Multiplication on $\FF^S$ is defined by
\[(\lambda f)(x)=\lambda f(x)\quad(\forall x\in S)\]
for all $\lambda\in\FF$, $f\in\FF^S$.
\end{itemize}

Verify that if $S$ is a non-empty set, then $\FF^S$ is a vector space over $\FF$.

Also verify that the additive identity of $\FF^S$ is the function $0:S\to\FF$ defined by
\[0(x)=0\quad(\forall x\in S)\]
and for $f\in\FF^S$, additive inverse of $f$ is the function $-f:S\to\FF$ defined by
\[(-f)(x)=-f(x)\quad(\forall x\in S)\]
\end{example}

\begin{remark}
$\FF^n$ and $\FF^\infty$ are special cases of the vector space $\FF^S$; think of $\FF^n$ as $\FF^{\{1,2,\dots,n\}}$, and $\FF^\infty$ as $\FF^{\{1,2,\dots\}}$.
\end{remark}

\begin{example}[Complexification]
Suppose $V$ is a real vector space. The \emph{complexifcation} of $V$, denoted by $V_\CC$, equals $V\times V$. An element of $V_\CC$ is an ordered pair $(u,v)$, where $u,v\in V$, which we write as $u+iv$.
\begin{itemize}
\item Addition on $V_\CC$ is defined by
\[(u_1+iv_1)+(u_2+iv_2)=(u_1+u_2)+i(v_1+v_2)\]
for all $u_1,v_2,u_2,v_2\in V$.
\item Complex scalar multiplication on $V_\CC$ is defined by
\[(a+bi)(u+iv)=(au-bv)+i(av+bu)\]
for all $a,b\in\RR$ and all $u,v\in V$.
\end{itemize}
You should verify that with the defnitions of addition and scalar multiplication as above, $V_\CC$ is a (complex) vector space.
\end{example}
\pagebreak

\section{Subspaces}
Whenever we have a mathematical object with some structure, we want to consider subsets that also have the same structure.

\begin{definition}[Subspace]
$U\subset V$ is a \vocab{subspace}\index{vector space!subspace} of $V$ if $U$ is also a vector space (with the same addition and scalar multiplication as on $V$). We denote this as $U\le V$.
\end{definition}

The sets $\{\vb{0}\}$ and $V$ are always subspaces of $V$. The subspace $\{\vb{0}\}$ is called the \emph{zero subspace} or \emph{trivial subspace}. Subspaces other than $V$ are called \emph{proper subspaces}.

The following result is useful in determining whether a given subset of $V$ is a subspace of $V$.

\begin{lemma}[Subspace test]\label{lemma:subspace-conditions}
Suppose $U\subset V$. Then $U\le V$ if and only if $U$ satisfies the following conditions:
\begin{enumerate}[label=(\roman*)]
\item Additive identity: $\vb{0}\in U$
\item Closed under addition: $u+w\in U$ for all $u,w\in U$
\item Closed under scalar multiplication: $\lambda u\in U$ for all $\lambda\in\FF$, $u\in U$
\end{enumerate}
\end{lemma}

\begin{proof} \

\fbox{$\implies$} If $U\le V$, then $U$ satisfies the three conditions above by the definition of vector space.

\fbox{$\impliedby$} Suppose $U$ satisfies the three conditions above. (i) ensures that the additive identity of $V$ is in $U$. (ii) ensures that addition makes sense on $U$. (iii) ensures that scalar multiplication makes sense on $U$.

If $u\in U$, then $-u=(-1)u\in U$ by (iii). Hence every element of $U$ has an additive inverse in $U$.

The other parts of the definition of a vector space, such as associativity and commutativity, are automatically satisfied for $U$ because they hold on the larger space $V$. Thus $U$ is a vector space and hence is a subspace of $V$.
\end{proof}

\begin{proposition}
Suppose $U\le V$. Then
\begin{enumerate}[label=(\roman*)]
\item $U$ is a vector space over $\FF$. In fact, the only subsets of $V$ that are vector spaces over $\FF$ are the subspaces of $V$;
\item if $W\le U$, then $W\le V$ (``a subspace of a subspace is a subspace'').
\end{enumerate}
\end{proposition}

\begin{proof} \
\begin{enumerate}[label=(\roman*)]
\item We first check that we have legitimate operations. Since $U$ is closed under addition, the operation $+$ restricted to $U$ gives a map $U\times U\to U$. Likewise since $U$ is closed under scalar multiplication, that operation restricted to $U$ gives a map $\FF\times U\to U$.

We now check that $U$ satisfies the vector space axioms.
\begin{enumerate}[label=(\roman*)]
\item Commutativity and associativity of addition are inherited from $V$.
\item There is an additive identity (by the subspace test).
\item There are additive inverses: if $u\in U$ then multiplying by $-1\in\FF$ and shows that $-u=(-1)u\in U$.
\item The remaining four properties are all inherited from $V$. That is, they apply to general vectors of $V$ and vectors in $U$ are vectors in $V$.
\end{enumerate}

\item This is immediate from the definition of a subspace.
\end{enumerate}
\end{proof}

\begin{definition}[Sum of subsets]
Suppose $U_1,\dots,U_n\subset V$. The sum of $U_1,\dots,U_n$ is the set of all possible sums of elements of $U_1,\dots,U_n$:
\[U_1+\cdots+U_n\coloneqq\{u_1+\cdots+u_n\mid u_i\in U_i\}.\]
\end{definition}

\begin{example}
Suppose that $U=\{(x,0,0)\in\FF^3\mid x\in F\}$ and $W=\{(0,y,0)\in\FF^3\mid y\in\FF\}$. Then
\[U+W=\{(x,y,0)\mid x,y\in\FF\}.\]

Suppose that $U=\{(x,x,y,y)\in\FF^4\mid x,y\in\FF\}$ and $W=\{(x,x,x,y)\in\FF^4\mid x,y\in\FF\}$. Then
\[U+W=\{(x,x,y,z)\in\FF^4\mid x,y,z\in\FF\}.\]
\end{example}

The next result states that the sum of subspaces is a subspace, and is in fact the smallest subspace containing all the summands.

\begin{proposition}
Suppose $U_1,\dots,U_n\le V$. Then $U_1+\cdots+U_n$ is the smallest subspace of $V$ containing $U_1,\dots,U_n$.
\end{proposition}

\begin{proof}
It is easy to see that $\vb{0}\in U_1+\cdots+U_n$ and that $U_1+\cdots+U_n$ is closed under addition and scalar multiplication. Hence by the subspace test, $U_1+\cdots+U_n\le V$.

Let $M$ be the smallest subspace of $V$ containing $U_1,\dots,U_n$. We want to show that $U_1+\cdots+U_n=M$. To do so, we show double inclusion: $U_1+\cdots+U_n\subset M$ and $M\subset U_1+\cdots+U_n$.
\begin{enumerate}[label=(\roman*)]
\item For all $u_i\in U_i$ ($1\le i\le n$),
\[u_i=\vb{0}+\cdots+\vb{0}+u_i+\vb{0}+\cdots+\vb{0}\in U_1+\cdots+U_n,\]
where all except one of the $u$'s are $\vb{0}$. Thus $U_i\subset U_1+\cdots+U_n$ for $1\le i\le n$. Hence $M\subset U_1+\cdots+U_n$.
\item Conversely, every subspace of $V$ containing $U_1,\dots,U_n$ contains $U_1+\cdots+U_n$ (because subspaces must contain all finite sums of their elements). Hence $U_1+\cdots+U_n\subset M$.
\end{enumerate}
\end{proof}

\begin{definition}[Direct sum]
Suppose $U_1,\dots,U_n\le V$. If each element of $U_1+\cdots+U_n$ can be written in only one way as a sum $u_1+\cdots+u_n$, $u_i\in U_i$, then $U_1+\cdots+U_n$ is called a \vocab{direct sum}\index{direct sum}. In this case, we denote the sum as
\[U_1\oplus\cdots\oplus U_n.\]
\end{definition}

\begin{example}
Suppose that $U=\{(x,y,0)\in\FF^3\mid x,y\in\FF\}$ and $W=\{(0,0,z)\in\FF^3\mid z\in\FF\}$. Then $\FF^3=U\oplus W$.

Suppose $U_i$ is the subspace of $\FF^n$ of those vectors whose coordinates are all 0 except for the $i$-th coordinate; that is, $U_i=\{(0,\dots,0,x,0,\dots,0)\in\FF^n\mid x\in\FF\}$. Then $\FF^n=U_1\oplus\cdots\oplus U_n$.
\end{example}

\begin{lemma}[Condition for direct sum]\label{lemma:condition-direct-sum}
Suppose $V_1,\dots,V_n\le V$, let $W=V_1+\cdots+V_n$. Then the following are equivalent:
\begin{enumerate}[label=(\roman*)]
\item Any element in $W$ can be uniquely expressed as the sum of vectors in $V_1,\dots,V_n$.
\item If $v_i\in V_i$ satisfies $v_1+\cdots+v_n=\vb{0}$, then $v_1=\cdots=v_n=\vb{0}$.
\item For $k=2,\dots,n$, $(V_1+\cdots+V_{k-1})\cap V_k=\{\vb{0}\}$.
\end{enumerate}
\end{lemma}

\begin{proof} \

(i)$\iff$(ii) First suppose $W$ is a direct sum. Then by the definition of direct sum, the only way to write $\vb{0}$ as a sum $u_1+\cdots+u_n$ is by taking $u_i=\vb{0}$.

Now suppose that the only way to write $\vb{0}$ as a sum $v_1+\cdots+v_n$ by taking $v_1=\cdots=v_n=\vb{0}$. For $v\in V_1+\cdots+V_n$, suppose that there is more than one way to represent $v$:
\begin{align*}
v&=v_1+\cdots+v_n\\
v&=v_1^\prime+\cdots+v_n^\prime
\end{align*}
for some $v_i,v_i^\prime\in V_i$. Substracting the above two equations gives
\[\vb{0}=(v_1-v_1^\prime)+\cdots+(v_n-v_n^\prime).\]
Since $v_i-v_i^\prime\in V_i$, we have $v_i-v_i^\prime=\vb{0}$ so $v_i=v_i^\prime$. Hence there is only one unique way to represent $v_1+\cdots+v_n$, thus $W$ is a direct sum.

(ii)$\iff$(iii) First suppose if $v_i\in V_i$ satisfies $v_1+\cdots+v_n=\vb{0}$, then $v_1=\cdots=v_n=\vb{0}$. Let $v_k\in(V_1+\cdots+V_{k-1})\cap V_k$. Then $v_k=v_1+\cdots+v_{k-1}$ where $v_i\in V_i$ ($1\le i\le k-1$). Thus
\begin{align*}
v_1+\cdots+v_{k-1}-v_k&=\vb{0}\\
v_1+\cdots+v_{k-1}+(-v_k)+\vb{0}+\cdots+\vb{0}&=\vb{0}
\end{align*}
by taking $v_{k+1}=\cdots=v_n=\vb{0}$. Then $v_1=\cdots=v_k=\vb{0}$.

Now suppose that for $k=2,\dots,n$, $(V_1+\cdots+V_{k-1})\cap V_k=\{\vb{0}\}$.
\begin{align*}
v_1+\cdots+v_n&=\vb{0}\\
v_1+\cdots+v_{n-1}&=-v_n
\end{align*}
where $v_1+\cdots+v_{n-1}\in V_1+\cdots+V_{n-1}$, $-v_n\in V_n$. Thus
\[v_1+\cdots+v_{n-1}=-v_n\in(V_1+\cdots+V_{n-1})\cap V_n=\{\vb{0}\}\]
so $v_1+\cdots+v_{n-1}=\vb{0}$, $v_n=\vb{0}$. Induction on $n$ gives $v_1=\cdots=v_{n-1}=v_n=\vb{0}$.
\end{proof}

\begin{proposition}
Suppose $U,W\le V$. Then $U+W$ is a direct sum if and only if $U\cap W=\{\vb{0}\}$.
\end{proposition}

\begin{proof} \

\fbox{$\implies$} Suppose that $U+W$ is a direct sum. If $v\in U\cap W$, then $\vb{0}=v+(-v)$, where $v\in U$, $-v\in W$. By the unique representation of $\vb{0}$ as the sum of a vector in $U$ and a vector in $W$, we have $v=\vb{0}$. Thus $U\cap W=\{\vb{0}\}$.

\fbox{$\impliedby$} Suppose $U\cap W=\{\vb{0}\}$. Suppose $u\in U$, $w\in W$, and $0=u+w$. $u=-w\in W$, thus $u\in U\cap W$, so $u=w=\vb{0}$. By \cref{lemma:condition-direct-sum}, $U+W$ is a direct sum.
\end{proof}
\pagebreak

\section{Span and Linear Independence}
\begin{definition}[Linear combination]
$v$ is a \vocab{linear combination}\index{linear combination} of vectors $v_1,\dots,v_n\in V$ if there exists $a_1,\dots,a_n\in\FF$ such that
\[v=a_1v_1+\cdots+a_nv_n.\]
\end{definition}

\begin{definition}[Span]
The \vocab{span}\index{span} of $\{v_1,\dots,v_n\}$ is the set of all linear combinations of $v_1,\dots,v_n$:
\[\spn(v_1,\dots,v_n)\coloneqq\{a_1v_1+\cdots+a_nv_n\mid a_i\in\FF\}.\]
The span of the empty set $\{\:\}$ is defined to be $\{\vb{0}\}$.

We say that $v_1,\dots,v_n$ \emph{spans} $V$ if $\spn(v_1,\dots,v_n)=V$.

If $S\subset V$ is such that $\spn(S)=V$, then we say that $S$ \emph{spans} $V$, and that $S$ is a \emph{spanning set} for $V$:
\[\spn(S)\coloneqq\{a_1v_1+\cdots+a_nv_n\mid v_i\in S, a_i\in\FF\}.\]
\end{definition}

\begin{proposition}
$\spn(v_1,\dots,v_n)$ in $V$ is the smallest subspace of $V$ containing $v_1,\dots,v_n$.
\end{proposition}

\begin{proof}
First we show that $\spn(v_1,\dots,v_n)\le V$, using the subspace test.
\begin{enumerate}[label=(\roman*)]
\item $\vb{0}=0v_1+\cdots+0v_n\in\spn(v_1,\dots,v_n)$
\item $(a_1v_1+\cdots+a_nv_n)+(c_1v_1+\cdots+c_nv_n)=(a_1+c_1)v_1+\cdots+(a_n+c_n)v_n\in\spn(v_1,\dots,v_n)$, so $\spn(v_1,\dots,v_n)$ is closed under addition.
\item $\lambda(a_1v_1+a_nv_n)=(\lambda a_1)v_1+\cdots+(\lambda a_n)v_n\in\spn(v_1,\dots,v_n)$, so $\spn(v_1,\dots,v_n)$ is closed under scalar multiplication.
\end{enumerate}

Let $M$ be the smallest vector subspace of $V$ containing $v_1,\dots,v_n$. We claim that $M=\spn(v_1,\dots,v_n)$. To show this, we show that (i) $M\subset\spn(v_1,\dots,v_n)$ and (ii) $M\supset\spn(v_1,\dots,v_n)$.
\begin{enumerate}[label=(\roman*)]
\item Each $v_i$ is a linear combination of $v_1,\dots,v_n$, as
\[v_i=0\cdot v_1+\cdots+0\cdot v_{i-1}+1\cdot v_i+0\cdot v_{i+1}+\cdots+0\cdot v_n,\]
so by the definition of the span as the collection of all linear combinations of $v_1,\dots,v_n$, we have that $v_i\in\spn(v_1,\dots,v_n)$. But $M$ is the smallest vector subspace containing $v_1,\dots,v_n$, so
\[M\subset\spn(v_1,\dots,v_n).\]
\item Since $v_i\in M$ ($1\le i\le n$) and $M$ is a vector subspace (closed under addition and scalar multiplication), it follows that
\[a_1v_1+\dots+a_nv_n\in M\]
for all $a_i\in\FF$ (i.e. $M$ contains all linear combinations of $v_1,\dots,v_n$). So
\[\spn(v_1,\dots,v_n)\subset M.\]
\end{enumerate}
\end{proof}

\begin{definition}[Finite-dimensional vector space]
$V$ is \vocab{finite-dimensional}\index{finite-dimensional} if there exists some list of vector $\{v_1,\dots,v_n\}$ that spans $V$; otherwise, it is \emph{infinite-dimensional}.
\end{definition}

\begin{remark}
Recall that by defnition every list of vectors has finite length.
\end{remark}

\begin{remark}
From this definition, infinite-dimensionality is the negation of finite-dimensionality (i.e. \emph{not} finite-dimensional). Hence to prove that a vector space is infinite-dimensional, we prove by contradiction; that is, first assume that the vector space is finite-dimensional, then try to come to a contradiction.
\end{remark}

\begin{exercise}
For positive integer $n$, $\FF^n$ is finite-dimensional.
\end{exercise}

\begin{proof}
Suppose $(x_1,x_2,\dots,x_n)\in\FF^n$, then
\[(x_1,x_2,\dots,x_n)=x_1(1,0,\dots,0)+x_2(0,1,\dots,0)+\cdots+x_n(0,0,\dots,1)\]
so
\[(x_1,\dots,x_n)\in\spn\brac{(1,0,\dots,0),(0,1,\dots,0),\dots,(0,\dots,0,1)}.\]
The vectors $(1,0,\dots,0),(0,1,\dots,0),\dots,(0,\dots,0,1)$ spans $\FF^n$, so $\FF^n$ is finite-dimensional.
\end{proof}

\begin{definition}[Linear independence]
A list of vectors $v_1,\dots,v_n$ is \vocab{linearly independent}\index{linear independence} in $V$ if the only choice of $a_1,\dots,a_n\in\FF$ that makes
\[a_1v_1+\cdots+a_nv_n=\vb{0}\]
is $a_1=\cdots=a_n=0$; otherwise, it is \emph{linearly dependent}.

We say that $S\subset V$ is linearly independent if every finite subset of $S$ is linearly independent.
\end{definition}

\begin{proposition}[Compare coefficients]
Let $v_1,\dots,v_n$ be linearly independent in $V$. Then
\[a_1v_1+\cdots+a_nv_n=b_1v_1+\cdots+b_nv_n\]
if and only if $a_i=b_i$ ($1\le i\le n$).
\end{proposition}

\begin{proof}
Exercise.
\end{proof}

The following result will often be useful; it states that given a linearly dependent set of vectors, one of the vectors is in the span of the previous ones; furthermore we can throw out that vector without changing the span of the original set.

\begin{lemma}[Linear dependence lemma]\label{lemma:linear-dependence}
Suppose $v_1,\dots,v_n$ are linearly dependent in $V$. Then there exists $v_k$ such that the following hold:
\begin{enumerate}[label=(\roman*)]
\item $v_k\in\spn\brac{v_1,\dots,v_{k-1}}$
\item $\spn(v_1,\dots,v_{k-1},v_{k+1},\dots,v_n)=\spn(v_1,\dots,v_n)$
\end{enumerate}
\end{lemma}

\begin{proof} \
\begin{enumerate}[label=(\roman*)]
\item Since $v_1,\dots,v_n$ are linearly dependent, there exists $a_1,\dots,a_n\in\FF$, not all $0$, such that
\[a_1v_1+\cdots+a_nv_n=0.\]
Take $k=\max\{1,\dots,n\}$ such that $a_k\neq0$. Then
\[v_k=-\frac{a_1}{a_k}v_1-\cdots-\frac{a_{k-1}}{a_k}v_{k-1},\]
which means that $v_k$ can be written as a linear combination of $v_1,\dots,v_{k-1}$, so $v_k\in\spn\brac{v_1,\dots,v_{k-1}}$ by definition of span.

\item Now suppose $k$ is such that $v_k\in\spn(v_1,\dots,v_{k-1})$. Then there exists $b_1,\dots,b_{k-1}\in\FF$ be such that
\begin{equation*}\tag{1}
v_k=b_1v_1+\cdots+b_{k-1}v_{k-1}.
\end{equation*}
Suppose $u\in\spn(v_1,\dots,v_n)$. Then there exists $c_1,\dots,c_n\in\FF$ such that
\begin{equation*}\tag{2}
u=c_1v_1+\cdots+c_nv_n.
\end{equation*}
In (2), we can replace $v_k$ with the RHS of (1), which gives
\begin{align*}
u&=c_1v_1+\cdots+c_{k-1}v_{k-1}+c_kv_k+c_{k+1}v_{k+1}+\cdots+c_nv_n\\
&=c_1v_1+\cdots+c_{k-1}v_{k-1}+c_k(b_1v_1+\cdots+b_{k-1}v_{k-1})+c_{k+1}v_{k+1}+\cdots+c_nv_n\\
&=c_1v_1+\cdots+c_{k-1}v_{k-1}+c_kb_1v_1+\cdots+c_kb_{k-1}v_{k-1}+c_{k+1}v_{k+1}+\cdots+c_nv_n\\
&=(c_1+bc_k)v_1+\cdots+(c_{k-1}+b_{k-1}c_k)v_{k-1}+c_{k+1}v_{k+1}+\cdots+c_nv_n.
\end{align*}
Thus $u\in\spn(v_1,\dots,v_{k-1},v_{k+1},\dots,v_n)$. This shows that removing $v_k$ from $v_1,\dots,v_n$ does not change the span of the list.
\end{enumerate}
\end{proof}

The following result says that no linearly independent set in $V$ is longer than a spanning set in $V$.

\begin{proposition}\label{prop:length-linind-span}
In a finite-dimensional vector space, the length of every linearly independent set of vectors is less than or equal to the length of every spanning set of vectors.
\end{proposition}

\begin{proof}
Suppose $A=\{u_1,\dots,u_m\}$ is linearly independent in $V$, $B=\{w_1,\dots,w_n\}$ spans $V$. We want to prove that $m\le n$.

Since $B$ spans $V$, if we add any other vector from $V$ to the list $B$, we will get a linearly dependent list, since this newly added vector can, by the definition of a span, be expressed as a linear combination of the vectors in $B$. In particular, if we add $u_1\in A$ to $B$, then the new list
\[\{u_1,w_1,\dots,w_n\}\]
is linearly dependent. By the linear independence lemma, we can remove one of the $w_i$'s from $B$, so that the remaining list of $n$ vectors still spans $V$. For the sake of argument, let's say we remove $w_n$ (we can always order the $w_i$'s in the list so that the element we remove is at the end). Then we are left with the revised list
\[B_1=\{u_1,w_1,\dots,w_{n-1}\}.\]
We can repeat this process $m$ times, each time adding the next element $u_i$ from list $A$ and removing the last $w_i$. Because of the linear dependence lemma, we know that there must always be a $w_i$ that can be removed each time we add a $u_i$, so there must be at least as many $w_i$'s as $u_i$'s. In other words, $m\le n$ which is what we wanted to prove.
\end{proof}

\begin{remark}
We can use this result to show, without any computations, that certain lists are not linearly independent and that certain lists do not span a given vector space.
\end{remark}

Our intuition suggests that every subspace of a finite-dimensional vector space should also be finite-dimensional. We now prove that this intuition is correct.

\begin{proposition}\label{prop:subspace-finite-dim}
Every subspace of a finite-dimensional vector space is finite-dimensional.
\end{proposition}

\begin{proof}
Suppose $V$ is finite-dimensional, $U\le V$. To show that $U$ is finite-dimensional, we need to find a spanning set of vectors in $U$. We prove by construction of this spanning set.

\begin{enumerate}
\item[Step 1] If $U=\{\vb{0}\}$, then $U$ is finite-dimensional and we are done. Otherwise, choose $v_1\in U$, $v_1\neq\vb{0}$ and add it to our list of vectors.
\item[Step $k$] Our list so far is $\{v_1,\dots,v_{k-1}\}$. If $U=\spn(v_1,\dots,v_{k-1})$, then $U$ is finite-dimensional and we are done. Otherwise, choose $v_k\in U$ such that $v_k\notin\spn(v_1,\dots,v_{k-1})$ and add it to our list.
\end{enumerate}

After each step, we have constructed a list of vectors such that no vector in this list is in the span of the previous vectors; by the linear dependence lemma, our constructed list is a linearly independent set.

By \cref{prop:length-linind-span}, this linearly independent set cannot be longer than any spanning set of $V$. Thus the process must terminate after a finite number of steps, and we have constructed a spanning set of $U$. Hence $U$ is finite-dimensional.
\end{proof}
\pagebreak

\section{Bases}
\begin{definition}[Basis]
$B=\{v_1\dots,v_n\}$ is a \vocab{basis}\index{basis} of $V$ if
\begin{enumerate}[label=(\roman*)]
\item $B$ is linearly independent in $V$;
\item $B$ is a spanning set of $V$.
\end{enumerate}
\end{definition}

\begin{example}[Standard basis]
Let $\vb{e}_i=(0,\dots,0,1,0,\dots,0)$ where the $i$-th coordinate is $1$. $\{\vb{e}_1,\dots,\vb{e}_n\}$ is a basis of $\FF^n$, known as the \emph{standard basis} of $\FF^n$.
\end{example}

\begin{lemma}[Criterion for basis]\label{lemma:basis-criterion}
Let $B=\{v_1,\dots,v_n\}$ be a list of vectors in $V$. Then $B$ is a basis of $V$ if and only if every $v\in V$ can uniquely expressed as a linear combination of $v_1,\dots,v_n$.
\end{lemma}

\begin{proof} \

\fbox{$\implies$} Let $v\in V$. Since $B$ is a basis of $V$, there exists $a_1,\dots,a_n\in\FF$ such that
\begin{equation*}\tag{1}
v=a_1v_1+\cdots+a_nv_n.
\end{equation*}
To show that the representation is unique, suppose that $c_1,\dots,c_n\in\FF$ also satisfy
\begin{equation*}\tag{2}
v=c_1v_1+\cdots+c_nv_n.
\end{equation*}
Subtracting (2) from (1) gives
\[\vb{0}=(a_1-c_1)v_1+\cdots+(a_n-c_n)v_n.\]
Since $v_1,\dots,v_n$ are linearly independent, we have $a_i-c_i=0$, or $a_i=c_i$ for all $i$ ($1\le i\le n$). Thus the representation of $v$ as a linear combination of $v_1,\dots,v_n$ is unique.

\fbox{$\impliedby$} Suppose that every $v\in V$ can be uniquely expressed as a linear combination of $v_1,\dots,v_n$. This implies that $B$ spans $V$. To show that $B$ is linearly independent, suppose that $a_1,\dots,a_n\in\FF$ satisfy
\[a_1v_1+\cdots+a_nv_n=\vb{0}.\]
Since $\vb{0}$ can be uniquely expressed as a linear combination of $v_1,\dots,v_n$, we have $a_1=\cdots=a_n=0$, thus $B$ is linearly independent. Since $B$ is linearly independent and spans $V$, $B$ is a basis of $V$.
\end{proof}

A spanning set in a vector space may not be a basis because it is not linearly independent. Our next result says that given any spanning set, some (possibly none) of the vectors in it can be discarded so that the remaining list is linearly independent and still spans the vector space.

\begin{lemma}\label{lemma:reduce-spanninglist-basis}
Every spanning set in a vector space can be reduced to a basis of the vector space.
\end{lemma}

\begin{proof}
Suppose $B=\{v_1,\dots,v_n\}$ spans $V$. We want to remove some vectors from $B$ so that the remaining vectors form a basis of $V$. We do this through the multistep process described below.

\begin{enumerate}
\item[Step 1] If $v_1=\vb{0}$, delete $v_1$ from $B$. If $v_1\neq\vb{0}$, leave $B$ unchanged.
\item[Step $k$] If $v_k\in\spn(v_1,\dots,v_{k-1})$, delete $v_k$ from $B$. If $v_k\notin\spn(v_1,\dots,v_{k-1})$, leave $B$ unchanged.
\end{enumerate}

Stop the process after step $n$, getting a list $B$. Since we only delete vectors from $B$ that are in the span of the previous vectors, by the linear dependence lemma, the list $B$ still spans $V$.

The process ensures that no vector in $B$ is in the span of the previous ones. By the linear dependence lemma, $B$ is linearly independent.

Since $B$ is linearly independent and spans $V$, $B$ is a basis of $V$.
\end{proof}

\begin{corollary}\label{cor:basis-fin-dim}
Every finite-dimensional vector space has a basis.
\end{corollary}

\begin{proof}
We prove by construction. Suppose $V$ is finite-dimensional. By definition, there exists a spanning set of vectors in $V$. By \cref{lemma:reduce-spanninglist-basis}, the spanning set can be reduced to a basis.
\end{proof}

Now we show that given any linearly independent set, we can adjoin some additional vectors so that the extended list is still linearly independent but also spans the space.

\begin{lemma}\label{lemma:extend-linind-basis}
Every linearly independent set of vectors in a finite-dimensional vector space can be extended to a basis of the vector space.
\end{lemma}

\begin{proof}
Suppose $u_1,\dots,u_m$ are linearly independent in $V$, $w_1,\dots,w_n$ span $V$. Then the list
\[\{u_1,\dots,u_m,w_1,\dots,w_n\}\]
spans $V$. By \cref{lemma:reduce-spanninglist-basis}, we can reduce this list to a basis of $V$ consisting $u_1,\dots,u_m$ (since $u_1,\dots,u_m$ are linearly independent, $u_i\notin\spn(u_1,\dots,u_{i-1})$ for all $i$, so none of the $u_i$'s are deleted in the process), and some of the $w_i$'s.
\end{proof}

We now show that every subspace of a finite-dimensional vector space can be paired with another subspace to form a direct sum of the whole space.

\begin{corollary}
Suppose $V$ is finite-dimensional, $U\le V$. Then there exists $W\le V$ such that $V=U\oplus W$.
\end{corollary}

\begin{proof}
Since $V$ is finite-dimensional and $U\le V$, by \cref{prop:subspace-finite-dim}, $U$ is finite-dimensional, so $U$ has a basis $B$, by \cref{cor:basis-fin-dim}; let $B=\{u_1,\dots,u_n\}$. Since $B$ is linearly independent, by \cref{lemma:extend-linind-basis}, $B$ can be extended to a basis of $V$, say
\[\{u_1,\dots,u_n,w_1,\dots,w_n\}.\]
Take $W=\spn(w_1,\dots,w_n)$. We claim that $V=U\oplus W$. To show this, by \cref{lemma:condition-direct-sum}, we need to show that (i) $V=U+W$, and (ii) $U\cap W=\{\vb{0}\}$.
\begin{enumerate}[label=(\roman*)]
\item Suppose $v\in V$. Since $\{u_1,\dots,u_n,w_1,\dots,w_n\}$ spans $V$, there exists $a_1,\dots,a_n,b_1,\dots,b_n\in\FF$ such that
\[v=a_1u_1+\cdots+a_nu_n+b_1w_1+\cdots+b_nw_n.\]
Take $u=a_1u_1+\cdots+a_nu_n\in U$, $w=b_1w_1+\cdots+b_nw_n\in W$. Then $v=u+w\in U+W$, so $V=U+W$.
\item Suppose $v\in U\cap W$. Since $v\in U$, $v$ can be written as a linear combination of $u_1,\dots,u_n$:
\begin{equation*}\tag{1}
v=a_1u_1+\cdots+a_nu_n.
\end{equation*}
Since $v\in W$, $v$ can be written as a linear combination of $w_1,\dots,w_n$:
\begin{equation*}\tag{2}
v=b_1w_1+\cdots+b_nw_n.
\end{equation*}
Subtracting (2) from (1) gives
\[\vb{0}=a_1u_1+\cdots+a_nu_n-b_1w_1-\cdots-b_nw_n.\]
Since $u_1,\dots,u_n,w_1,\dots,w_n$ are linearly independent, we have $a_i=b_i=0$ for all $i$ ($1\le i\le n$). Thus $v=\vb{0}$, so $U\cap W=\{\vb{0}\}$.
\end{enumerate}
\end{proof}
\pagebreak

\section{Dimension}
\begin{lemma}
Any two bases of a finite-dimensional vector space have the same length.
\end{lemma}

\begin{proof}
Suppose $V$ is finite-dimensional, let $B_1$ and $B_2$ be two bases of $V$. By definition, $B_1$ is linearly independent in $V$, and $B_2$ spans $V$, so by \cref{prop:length-linind-span}, $|B_1|\le|B_2|$.

Similarly, by definition, $B_2$ is linearly independent in $V$ and $B_1$ spans $V$, so $|B_2|\le|B_1|$.

Since $|B_1|\le|B_2|$ and $|B_2|\le|B_1|$, we have $|B_1|=|B_2|$, as desired.
\end{proof}

Since any two bases of a finite-dimensional vector space have the same length, we can formally define the dimension of such spaces.

\begin{definition}[Dimension]
The \vocab{dimension}\index{dimension} of $V$ is the length of any basis of $V$, denoted by $\dim V$.
\end{definition}

\begin{proposition}\label{prop:dim-subspace}
Suppose $V$ is finite-dimensional, $U\le V$. Then $\dim U\le\dim V$.
\end{proposition}

\begin{proof}
Since $V$ is finite-dimensional and $U\le V$, $U$ is finite-dimensional. Let $B_U$ be a basis of $U$, and $B_V$ be a basis of $V$.

By definition, $B_U$ is linearly independent in $V$, and $B_V$ spans $V$. By \cref{prop:length-linind-span}, $|B_U|\le|B_V|$, so
\[\dim U=|B_U|\le|B_V|=\dim V,\]
since $|B_U|=\dim U$ and $|B_V|=\dim V$ by definition.
\end{proof}

To check that a list of vectors is a basis, we must show that it is linearly independent and that it spans the vector space. The next result shows that if the list in question has the right length, then we only need to check that it satisfies one of the two required properties.

\begin{proposition}\label{prop:basis-length-dim}
Suppose $V$ is finite-dimensional. Then
\begin{enumerate}[label=(\roman*)]
\item every linearly independent set of vectors in $V$ with length $\dim V$ is a basis of $V$;
\item every spanning set of vectors in $V$ with length $\dim V$ is a basis of $V$.
\end{enumerate}
\end{proposition}

\begin{proof} \
\begin{enumerate}[label=(\roman*)]
\item Suppose $\dim V=n$, $\{v_1,\dots,v_n\}$ is linearly independent in $V$. By \cref{lemma:extend-linind-basis}, $\{v_1,\dots,v_n\}$ can be extended to a basis of $V$. However, every basis of $V$ has length $n$ (by definition of dimension), which means that no elements are adjoined to $\{v_1,\dots,v_n\}$. Hence $\{v_1,\dots,v_n\}$ is a basis of $V$, as desired.

\item Suppose $\dim V=n$, $\{v_1,\dots,v_n\}$ spans $V$. By \cref{lemma:reduce-spanninglist-basis}, $\{v_1,\dots,v_n\}$ can be reduced to a basis of $V$. However, every basis of $V$ has length $n$, which means that no elements are deleted from $\{v_1,\dots,v_n\}$. Hence $\{v_1,\dots,v_n\}$ is a basis of $V$, as desired.
\end{enumerate}
\end{proof}

\begin{corollary}
Suppose $V$ is finite-dimensional, $U\le V$. If $\dim U=\dim V$, then $U=V$.
\end{corollary}

\begin{proof}
Let $\dim U=\dim V=n$, let $\{u_1,\dots,u_n\}$ be a basis of $U$. Then $\{u_1,\dots,u_n\}$ is linearly independent in $V$ (because it is a basis of $U$) of length $\dim V$. From \cref{prop:basis-length-dim}, $\{u_1,\dots,u_n\}$ is a basis of $V$. In particular every vector in $V$ is a linear combination of $u_1,\dots,u_n$. Thus $U=V$.
\end{proof}

\begin{lemma}[Dimension of sum]
Suppose $V$ is finite-dimensional, $U_1,U_2\le V$. Then
\[\dim(U_1+U_2)=\dim U_1+\dim U_2-\dim(U_1\cap U_2).\]
\end{lemma}

\begin{proof}
Let $\{u_1,\dots,u_m\}$ be a basis of $U_1\cap U_2$; thus $\dim(U_1\cap U_2)=m$. Since $\{u_1,\dots,u_m\}$ is a basis of $U_1\cap U_2$, it is linearly independent in $U_1$. By \cref{lemma:extend-linind-basis}, $\{u_1,\dots,u_m\}$ can be extended to a basis $\{u_1,\dots,u_m,v_1,\dots,v_j\}$ of $U_1$; thus $\dim U_1=m+j$. Similarly, extend $\{u_1,\dots,u_m\}$ to a basis $\{u_1,\dots,u_m,v_1,\dots,v_k\}$ of $U_2$; thus $\dim U_2=m+k$.

We will show that
\[\{u_1,\dots,u_m,v_1,\dots,v_j,w_1,\dots,w_k\}\]
is a basis of $U_1+U_2$. This will complete the proof because then we will have
\begin{align*}
\dim(U_1+U_2)&=m+j+k\\
&=(m+j)+(m+k)-m\\
&=\dim U_1+\dim U_2-\dim(U_1\cap U_2).
\end{align*}

We just need to show that $\{u_1,\dots,u_m,v_1,\dots,v_j,w_1,\dots,w_k\}$ is linearly independent. To prove this, suppose
\begin{equation*}\tag{1}
a_1u_1+\cdots+a_mu_m+b_1v_1+\cdots+b_jv_j+c_1w_1+\cdots+c_kw_k=\vb{0},
\end{equation*}
where $a_i,b_i,c_i\in\FF$. We need to show that $a_i=b_i=c_i=0$ for all $i$. (1) can be rewritten as
\[c_1w_1+\cdots+c_kw_k=-a_1u_1-\cdots-a_mu_m-b_1v_1-\cdots-b_jv_j,\]
which shows that $c_1w_1+\cdots+c_kw_k\in U_1$. But actually all the $w_i$'s are in $U_2$, so $c_1w_1+\cdots+c_kw_k\in U_2$, thus $c_1w_1+\cdots+c_kw_k\in U_1\cap U_2$. Then we can write
\[c_1w_1+\cdots+c_kw_k=d_1u_1+\cdots+d_mu_m\]
for some $d_i\in\FF$. But $u_1,\dots,u_m,w_1,\dots,w_k$ are linearly independent, so $c_i=d_i=0$ for all $i$. Thus our original equation (1) becomes
\[a_1u_1+\cdots+a_mu_m+b_1v_1+\cdots+b_jv_j=\vb{0}.\]
Since $u_1,\dots,u_m,v_1,\dots,v_j$ are linearly independent, we have $a_i=b_i=0$ for all $i$, as desired.
\end{proof}
\pagebreak

\section*{Exercises}
\begin{exercise}[\cite{axler} 1C Q12]
Suppose $W$ is a vector space over $\FF$, $V_1$ and $V_2$ are subspaces of $W$. Show that $V_1\cup V_2$ is a vector space over $\FF$ if and only if $V_1\subset V_2$ or $V_2\subset V_1$.
\end{exercise}

\begin{solution}
The backward direction is trivial. We focus on proving the forward direction.

Supppse otherwise, then $V_1\setminus V_2\neq\emptyset$ and $V_2\setminus V_1\neq\emptyset$. Pick $v_1\in V_1\setminus V_2$ and $v_2\in V_2\setminus V_1$. Then
\begin{align*}
v_1,v_2\in V_1\cup V_2&\implies v_1+v_2\in V_1\cup V_2\\
&\implies v_2,v_1+v_2\in V_2\\
&\implies v_1=(v_1+v_2)-v_2\in V_2
\end{align*}
which is a contradiction.
\end{solution}

\begin{exercise}[\cite{axler} 1C Q13]
Suppose $W$ is a vector space over $\FF$, $V_1,V_2,V_3$ are subspaces of $W$. Then $V_1\cup V_2\cup V_3$ is a vector space over $\FF$ if and only if one of the $V_i$ contains the other two.
\end{exercise}

\begin{solution}
We prove the forward direction. Suppose otherwise, then $v_1\in V_1\setminus(V_2+V_3)$, $v_2\in V_2\setminus(V_1+V_3)$, $v_3\in V_3\setminus(V_1+V_2)$. Consider
\[\{v_1+v_2+v_3,v_1+v_2+2v_3,v_1+2v_2+v_3,v_1+2v_2+2v_3\}\subset V_1\cup V_2\cup V_3\]
Then
\begin{align*}
&(v_1+v_2+2v_3)-(v_1+v_2+v_3)=v_3\notin V_1+V_2\\
&\implies v_1+v_2+v_3\notin V_1+V_2\quad\text{or}\quad v_1+v_2+2v_3\notin V_1+V_2\\
&\implies v_1+v_2+v_3\in V_3\quad\text{or}\quad v_1+v_2+2v_3\in V_3\\
&\implies v_1+v_2\in V_3
\end{align*}
Similarly,
\begin{align*}
&(v_1+2v_2+2v_3)-(v_1+2v_2+v_3)=v_3\notin V_1+V_2\\
&\implies v_1+2v_2+v_3\notin V_1+V_2\quad\text{or}\quad v_1+2v_2+2v_3\notin V_1+V_2\\
&\implies v_1+2v_2+v_3\in V_3\quad\text{or}\quad v_1+2v_2+2v_3\in V_3\\
&\implies v_1+2v_2\in V_3
\end{align*}
This implies $(v_1+2v_2)-(v_1+v_2)=v_2\in V_3$, a contradiction.
\end{solution}

\begin{exercise}[\cite{axler} 2A Q12]
Suppose $\{v_1,\dots,v_n\}$ is linearly independent in $V$, $w\in V$. Prove that if $\{v_1+w,\dots,v_n+w\}$ is linearly dependent, then $w\in\spn(v_1,\dots,v_n)$.
\end{exercise}

\begin{solution}
If $\{v_1+w,\dots,v_n+w\}$ is linearly dependent, then there exists $a_1,\dots,a_n\in\FF$, not all zero, such that
\[a_1(v_1+w)+\cdots+a_n(v_n+w)=0,\]
or
\[a_1v_1+\cdots+a_nv_n=-(a_1+\cdots+a_n)w.\]
Suppose otherwise, that $a_1+\cdots+a_n=0$. Then
\[a_1v_1+\cdots+a_nv_n=\vb{0},\]
but the linear independence of $\{v_1,\dots,v_n\}$ implies that $a_1=\cdots=a_n=0$, which is a contradiction. Hence we must have $a_1+\cdots+a_n\neq0$, so we can write
\[w=-\frac{a_1}{a_1+\cdots+a_n}v_1-\cdots-\frac{a_n}{a_1+\cdots+a_n}v_n,\]
which is a linear combination of $v_1,\dots,v_n$. Thus by definition of span, $w\in\spn(v_1,\dots,v_n)$.
\end{solution}

\begin{exercise}[\cite{axler} 2A Q14]
Suppose $\{v_1,\dots,v_n\}\subset V$. Let
\[w_i=v_1+\cdots+v_i\quad(i=1,\dots,n)\]
Show that $\{v_1,\dots,v_n\}$ is linearly independent if and only if $\{w_1,\dots,w_n\}$ is linearly independent.
\end{exercise}

\begin{solution}
Write
\begin{align*}
v_1&=w_1\\
v_2&=w_2-w_1\\
v_3&=w_3-w_2\\
&\vdots\\
v_n&=w_n-w_{n-1}.
\end{align*}

\fbox{$\implies$}
\[a_1w_1+\cdots+a_nw_n=\vb{0}\]
for some $a_i\in\FF$. Expressing $w_i$'s as $v_i$'s,
\[a_1v_1+a_2(v_1+v_2)+\cdots+a_n(v_1+\cdots+v_n)=0,\]
or
\[(a_1+\cdots+a_n)v_1+(a_2+\cdots+a_n)v_2+\cdots+a_nv_n=\vb{0}.\]
Since $v_1,\dots,v_n$ are linearly independent,
\begin{align*}
a_1+a_2+\cdots+a_n&=0\\
a_2+\cdots+a_n&=0\\
&\vdots\\
a_n&=0
\end{align*}
on solving simultaneously gives $a_1=\cdots=a_n=0$.

\fbox{$\impliedby$} Similar to the above.
\end{solution}

\begin{exercise}[\cite{axler} 2A Q18]
Prove that $\FF^\infty$ is infinite-dimensional.
\end{exercise}

\begin{solution}
To prove that $\FF^\infty$ has no finite spanning sets, we prove by contradiction. Suppose otherwise, that there exists a finite spanning set of $\FF^\infty$, say $\{v_1,\dots,v_n\}$.

Let
\begin{align*}
e_1&=(1,0,\dots)\\
e_2&=(0,1,0,\dots)\\
e_3&=(0,0,1,0,\dots)\\
&\vdots\\
e_{n+1}&=(0,\dots,0,1,0,\dots)
\end{align*}
where $e_i$ has a $1$ at the $i$-th coordinate, and $0$'s for the remaining coordinates. Let
\[a_1e_1+\cdots+a_{n+1}e_{n+1}=\vb{0}\]
for some $a_i\in\FF$. Then
\[(a_1,a_2,\dots,a_{n+1},0,0,\dots)=\vb{0}\]
so $a_1=a_2=\cdots=a_{n+1}=0$. Thus $\{e_1,\dots,e_{n+1}\}$ is a linearly independent set, of length $n+1$. However, $\{v_1,\dots,v_n\}$ is a spanning set of length $n$. By \cref{prop:length-linind-span}, we have reached a contradiction.
\end{solution}

\begin{exercise}[\cite{axler} 2B Q5]
Suppose $V$ is finite-dimensional, $U,W\le V$ such that $V=U+W$. Prove that $V$ has a basis in $U\cup W$.
\end{exercise}

\begin{solution}
Let $\{v_i\}_{i=1}^n$ denote the basis for $V$. By definition we have $v_i=u_i+w_i$ for some $u_i\in U$, $w_i\in W$. Then we have the spanning set of the vector space $V$ $\sum_{i=1}^{n}a_i(u_i+w_i)$, which can be reduced to a basis by the lemma.
\end{solution}

\begin{exercise}[\cite{axler} 2B Q7]
Suppose $\{v_1,v_2,v_3,v_4\}$ is a basis of $V$. Prove that
\[\{v_1+v_2,v_2+v_3,v_3+v_4,v_4\}\]
is also a basis of $V$.
\end{exercise}

\begin{solution}
We know that $\{v_1,v_2,v_3,v_4\}$ is linearly independent and spans $V$. Then there exist $a_i\in\FF$ such that
\[a_1(v_1+v_2)+a_2(v_2+v_3)+a_3(v_3+v_4)+a_4v_4=0\implies a_1=a_2=a_3=a_4=0.\]

Write
\begin{align*}
&a_1(v_1+v_2)+a_2(v_2+v_3)+a_3(v_3+v_4)+a_4v_4\\
&=a_1v_1+(a_1+a_2)v_2+(a_2+a_3)v_3+(a_3+a_4)v_4,
\end{align*}
this shows the linear independence. To prove spanning, let $v\in V$, then
\begin{align*}
v&=a_1v_1+a_2v_2+a_3v_3+a_4v_4\\
&=a_1(v_1+v_2)+(a_2-a_1)(v_2+v_3)+(a_3-a_2)(v_3+v_4)+(a_4-a_3)v_4,
\end{align*}
which is a linear combination of $v_1+v_2,v_2+v_3,v_3+v_4,v_4$.
\end{solution}

\begin{exercise}[\cite{axler} 2B Q10]
Suppose $U,W\le V$ such that $V=U\oplus W$. Suppose also that $\{u_1,\dots,u_m\}$ is a basis of $U$, $\{w_1,\dots,w_n\}$ is a basis of $W$. Prove that
\[\{u_1,\dots,u_m,w_1,\dots,w_n\}\]
is a basis of $V$.
\end{exercise}

\begin{solution}
We know that this set is linearly independent (otherwise violating the direct sum assumption) so it suffices to prove the spanning. Let $v\in V$, then
\[v=u+w=\sum_{i=1}^{m}a_iu_i+\sum_{i=1}^{n}b_jw_j.\]
\end{solution}

\begin{exercise}[\cite{axler} 2C Q8]

\end{exercise}

\begin{exercise}[\cite{axler} 2C Q16]

\end{exercise}

\begin{exercise}[\cite{axler} 2C Q17]
Suppose that $V_1,\dots,V_n\le V$ are finite-dimensional. Prove that $V_1+\cdots+V_n$ is finite-dimensional, and
\[\dim(V_1+\cdots+V_n)\le\dim V_1+\cdots+\dim V_n.\]
\end{exercise}

\begin{solution}
We prove by induction on $n$. The base case is trivial. Assume the statement holds for $k$. Then for $k+1$, denoting $V_1+\cdots+V_k=M_k$, we have that
\[\dim(M_k+V_{k+1})\le\dim V_1+\cdots+\dim V_{k+1},\]
which is finite.
\end{solution}