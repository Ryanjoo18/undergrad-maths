\chapter{Vector Spaces}\label{chap:vector-spaces}
\section{Definition of Vector Space}
\begin{notation}
A field is denoted by $\FF$, which can mean either $\RR$ or $\CC$. $\FF^n$ is the set of $n$-tuples whose elements belong to $\FF$:
\[\FF^n\coloneqq\{(x_1,\dots,x_n)\mid x_i\in\FF\}\]
For $(x_1,\dots,x_n)\in\FF^n$ and $i=1,\dots,n$, we say that $x_i$ is the $i$-th coordinate of $(x_1,\dots,x_n)$.
\end{notation}

\begin{definition}[Vector space]
$V$ is a \vocab{vector space}\index{vector space} over $\FF$ if the following properties hold:
\begin{enumerate}[label=(\roman*)]
\item Addition is commutative: $u+v=v+u$ for all $u,v\in V$
\item Addition is associative: $(u+v)+w=u+(v+w)$ for all $u,v,w\in V$

Multiplication is associative: $(ab)v=a(bv)$ for all $v\in V$, $a,b\in\FF$
\item Additive identity: there exists $\vb{0}\in V$ such that $v+\vb{0}=v$ for all $v\in V$
\item Additive inverse: for every $v\in V$, there exists $w\in V$ such that $v+w=\vb{0}$
\item Multiplicative identity: $1v=v$ for all $v\in V$
\item Distributive properties: $a(u+v)=au+av$ and $(a+b)v=av+bv$ for all $a,b,\in\FF$ and $u,v\in V$
\end{enumerate}
\end{definition}

\begin{notation}
For the rest of this text, $V$ denotes a vector space over $\FF$.
\end{notation}

\begin{example}
$\RR^n$ is a vector space over $\RR$, $\CC^n$ is a vector space over $\CC$.
\end{example}

Elements of a vector space are called \emph{vectors} or \emph{points}.

The scalar multiplication in a vector space depends on $\FF$. Thus when we
need to be precise, we will say that $V$ is a vector space over $\FF$ instead of saying simply that $V$ is a vector space. For example, $\RR^n$ is a vector space over $\RR$, and $\CC^n$ is a vector space over $\CC$. A vector space over $\RR$ is called a \emph{real vector space}\index{vector space!real vector space}; a vector space over $\CC$ is called a \emph{complex vector space}\index{vector space!complex vector space}.

\begin{proposition}[Uniqueness of additive identity]
A vector space has a unique additive identity.
\end{proposition}

\begin{proof}
Suppose otherwise, then $\vb{0}$ and $\vb{0}^\prime$ are additive identities of $V$. Then
\[\vb{0}^\prime=\vb{0}^\prime+\vb{0}=\vb{0}+\vb{0}^\prime=\vb{0}\]
where the first equality holds because $\vb{0}$ is an additive identity, the second equality comes from commutativity, and the third equality holds because $\vb{0}^\prime$ is an additive identity. Thus $\vb{0}^\prime=\vb{0}$.
\end{proof}

\begin{proposition}[Uniqueness of additive inverse]
Every element in a vector space has a unique additive inverse.
\end{proposition}

\begin{proof}
Suppose otherwise, then for $v\in V$, $w$ and $w^\prime$ are additive inverses of $v$. Then
\[w=w+\vb{0}=w+(v+w^\prime)=(w+v)+w^\prime=\vb{0}+w^\prime=w^\prime.\]
Thus $w=w^\prime$.
\end{proof}

Because additive inverses are unique, the following notation now makes sense.

\begin{notation}
Let $v,w\in V$. Then $-v$ denotes the additive inverse of $v$; $w-v$ is defined to be $w+(-v)$.
\end{notation}

We now prove some seemingly trivial facts.

\begin{proposition} \
\begin{enumerate}[label=(\roman*)]
\item For every $v\in V$, $0v=\vb{0}$.
\item For every $a\in\FF$, $a\vb{0}=\vb{0}$.
\item For every $v\in V$, $(-1)v=-v$.
\end{enumerate}
\end{proposition}

\begin{proof} \
\begin{enumerate}[label=(\roman*)]
\item For $v\in V$, we have
\[0v=(0+0)v=0v+0v.\]
Adding the additive inverse of $0v$ to both sides of the equation gives $\vb{0}=0v$.

\item For $a\in\FF$, we have
\[a\vb{0}=a(\vb{0}+\vb{0})=a\vb{0}+a\vb{0}.\]
Adding the additive inverse of $a\vb{0}$ to both sides of the equation gives $\vb{0}=a\vb{0}$.

\item For $v\in V$, we have
\[v+(-1)v=1v+(-1)v=(1+(-1))v=0v=\vb{0}.\]
Since $v+(-1)v=\vb{0}$, $(-1)v$ is the additive inverse of $v$.
\end{enumerate}
\end{proof}

\begin{example}
$\FF^\infty$ is defined to be the set of all sequences of elements of $\FF$:
\[\FF^\infty\coloneqq\{(x_1,x_2,\dots)\mid x_i\in\FF\}\]
\begin{itemize}
\item Addition on $\FF^\infty$ is defined by
\[(x_1,x_2,\dots)+(y_1,y_2,\dots)=(x_1+y_1,x_2+y_2,\dots)\]
\item Scalar multiplication on $\FF^\infty$ is defined by
\[\lambda(x_1,x_2,\dots)=(\lambda x_1,\lambda x_2,\dots)\]
\end{itemize}

Verify that $\FF^\infty$ becomes a vector space over $\FF$. Also verify that the additive identity in $\FF^\infty$ is $\vb{0}=(0,0,\dots)$.
\end{example}

Our next example of a vector space involves a set of functions.

\begin{example}
If $S$ is a set, $\FF^S\coloneqq\{f\mid f:S\to\FF\}$.
\begin{itemize}
\item Addition on $\FF^S$ is defined by 
\[(f+g)(x)=f(x)+g(x)\quad(\forall x\in S)\]
for all $f,g\in\FF^S$.
\item Multiplication on $\FF^S$ is defined by
\[(\lambda f)(x)=\lambda f(x)\quad(\forall x\in S)\]
for all $\lambda\in\FF$, $f\in\FF^S$.
\end{itemize}

Verify that if $S$ is a non-empty set, then $\FF^S$ is a vector space over $\FF$.

Also verify that the additive identity of $\FF^S$ is the function $0:S\to\FF$ defined by
\[0(x)=0\quad(\forall x\in S)\]
and for $f\in\FF^S$, additive inverse of $f$ is the function $-f:S\to\FF$ defined by
\[(-f)(x)=-f(x)\quad(\forall x\in S)\]
\end{example}

\begin{remark}
$\FF^n$ and $\FF^\infty$ are special cases of the vector space $\FF^S$; think of $\FF^n$ as $\FF^{\{1,2,\dots,n\}}$, and $\FF^\infty$ as $\FF^{\{1,2,\dots\}}$.
\end{remark}

\begin{example}[Complexification]
Suppose $V$ is a real vector space. The \emph{complexifcation} of $V$, denoted by $V_\CC$, equals $V\times V$. An element of $V_\CC$ is an ordered pair $(u,v)$, where $u,v\in V$, which we write as $u+iv$.
\begin{itemize}
\item Addition on $V_\CC$ is defined by
\[(u_1+iv_1)+(u_2+iv_2)=(u_1+u_2)+i(v_1+v_2)\]
for all $u_1,v_2,u_2,v_2\in V$.
\item Complex scalar multiplication on $V_\CC$ is defined by
\[(a+bi)(u+iv)=(au-bv)+i(av+bu)\]
for all $a,b\in\RR$ and all $u,v\in V$.
\end{itemize}
You should verify that with the defnitions of addition and scalar multiplication as above, $V_\CC$ is a (complex) vector space.
\end{example}

\section{Subspaces}
Whenever we have a mathematical object with some structure, we want to consider subsets that also have the same structure.

\begin{definition}[Subspace]
$U\subset V$ is a \vocab{subspace}\index{vector space!subspace} of $V$ if $U$ is also a vector space (with the same addition and scalar multiplication as on $V$). We denote this as $U\le V$.
\end{definition}

The sets $\{\vb{0}\}$ and $V$ are always subspaces of $V$. The subspace $\{\vb{0}\}$ is called the \emph{zero subspace} or \emph{trivial subspace}. Subspaces other than $V$ are called \emph{proper subspaces}.

The following result is useful in determining whether a given subset of $V$ is a subspace of $V$.

\begin{lemma}[Subspace test]\label{lemma:subspace-conditions}
Suppose $U\subset V$. Then $U\le V$ if and only if $U$ satisfies the following conditions:
\begin{enumerate}[label=(\roman*)]
\item Additive identity: $\vb{0}\in U$
\item Closed under addition: $u+w\in U$ for all $u,w\in U$
\item Closed under scalar multiplication: $\lambda u\in U$ for all $\lambda\in\FF$, $u\in U$
\end{enumerate}
\end{lemma}

\begin{proof} \

\fbox{$\implies$} If $U\le V$, then $U$ satisfies the three conditions above by the definition of vector space.

\fbox{$\impliedby$} Suppose $U$ satisfies the three conditions above. (i) ensures that the additive identity of $V$ is in $U$. (ii) ensures that addition makes sense on $U$. (iii) ensures that scalar multiplication makes sense on $U$.

If $u\in U$, then $-u=(-1)u\in U$ by (iii). Hence every element of $U$ has an additive inverse in $U$.

The other parts of the definition of a vector space, such as associativity and commutativity, are automatically satisfied for $U$ because they hold on the larger space $V$. Thus $U$ is a vector space and hence is a subspace of $V$.
\end{proof}

\begin{proposition}
Suppose $U\le V$. Then
\begin{enumerate}[label=(\roman*)]
\item $U$ is a vector space over $\FF$. In fact, the only subsets of $V$ that are vector spaces over $\FF$ are the subspaces of $V$;
\item if $W\le U$, then $W\le V$ (``a subspace of a subspace is a subspace'').
\end{enumerate}
\end{proposition}

\begin{proof} \
\begin{enumerate}[label=(\roman*)]
\item We first check that we have legitimate operations. Since $U$ is closed under addition, the operation $+$ restricted to $U$ gives a map $U\times U\to U$. Likewise since $U$ is closed under scalar multiplication, that operation restricted to $U$ gives a map $\FF\times U\to U$.

We now check that $U$ satisfies the vector space axioms.
\begin{enumerate}[label=(\roman*)]
\item Commutativity and associativity of addition are inherited from $V$.
\item There is an additive identity (by the subspace test).
\item There are additive inverses: if $u\in U$ then multiplying by $-1\in\FF$ and shows that $-u=(-1)u\in U$.
\item The remaining four properties are all inherited from $V$. That is, they apply to general vectors of $V$ and vectors in $U$ are vectors in $V$.
\end{enumerate}

\item This is immediate from the definition of a subspace.
\end{enumerate}
\end{proof}

\begin{definition}[Sum of subsets]
Suppose $U_1,\dots,U_n\subset V$. The sum of $U_1,\dots,U_n$ is the set of all possible sums of elements of $U_1,\dots,U_n$:
\[U_1+\cdots+U_n\coloneqq\{u_1+\cdots+u_n\mid u_i\in U_i\}.\]
\end{definition}

\begin{example}
Suppose that $U=\{(x,0,0)\in\FF^3\mid x\in F\}$ and $W=\{(0,y,0)\in\FF^3\mid y\in\FF\}$. Then
\[U+W=\{(x,y,0)\mid x,y\in\FF\}.\]

Suppose that $U=\{(x,x,y,y)\in\FF^4\mid x,y\in\FF\}$ and $W=\{(x,x,x,y)\in\FF^4\mid x,y\in\FF\}$. Then
\[U+W=\{(x,x,y,z)\in\FF^4\mid x,y,z\in\FF\}.\]
\end{example}

The next result states that the sum of subspaces is a subspace, and is in fact the smallest subspace containing all the summands.

\begin{proposition}
Suppose $U_1,\dots,U_n\le V$. Then $U_1+\cdots+U_n$ is the smallest subspace of $V$ containing $U_1,\dots,U_n$.
\end{proposition}

\begin{proof}
It is easy to see that $\vb{0}\in U_1+\cdots+U_n$ and that $U_1+\cdots+U_n$ is closed under addition and scalar multiplication. Hence by the subspace test, $U_1+\cdots+U_n\le V$.

Let $M$ be the smallest subspace of $V$ containing $U_1,\dots,U_n$. We want to show that $U_1+\cdots+U_n=M$. To do so, we show double inclusion: $U_1+\cdots+U_n\subset M$ and $M\subset U_1+\cdots+U_n$.
\begin{enumerate}[label=(\roman*)]
\item For all $u_i\in U_i$ ($1\le i\le n$),
\[u_i=\vb{0}+\cdots+\vb{0}+u_i+\vb{0}+\cdots+\vb{0}\in U_1+\cdots+U_n,\]
where all except one of the $u$'s are $\vb{0}$. Thus $U_i\subset U_1+\cdots+U_n$ for $1\le i\le n$. Hence $M\subset U_1+\cdots+U_n$.
\item Conversely, every subspace of $V$ containing $U_1,\dots,U_n$ contains $U_1+\cdots+U_n$ (because subspaces must contain all finite sums of their elements). Hence $U_1+\cdots+U_n\subset M$.
\end{enumerate}
\end{proof}

\begin{definition}[Direct sum]\label{def:direct-sum}
Suppose $U_1,\dots,U_n\le V$. If each element of $U_1+\cdots+U_n$ can be written in only one way as a sum $u_1+\cdots+u_n$, $u_i\in U_i$, then $U_1+\cdots+U_n$ is called a \vocab{direct sum}\index{direct sum}. In this case, we denote the sum as
\[U_1\oplus\cdots\oplus U_n.\]
\end{definition}

\begin{example}
Suppose that $U=\{(x,y,0)\in\FF^3\mid x,y\in\FF\}$ and $W=\{(0,0,z)\in\FF^3\mid z\in\FF\}$. Then $\FF^3=U\oplus W$.

Suppose $U_i$ is the subspace of $\FF^n$ of those vectors whose coordinates are all 0 except for the $i$-th coordinate; that is, $U_i=\{(0,\dots,0,x,0,\dots,0)\in\FF^n\mid x\in\FF\}$. Then $\FF^n=U_1\oplus\cdots\oplus U_n$.
\end{example}

\begin{lemma}[Condition for direct sum]\label{lemma:condition-direct-sum}
Suppose $V_1,\dots,V_n\le V$, let $W=V_1+\cdots+V_n$. Then the following are equivalent:
\begin{enumerate}[label=(\roman*)]
\item Any element in $W$ can be uniquely expressed as the sum of vectors in $V_1,\dots,V_n$.
\item If $v_i\in V_i$ satisfies $v_1+\cdots+v_n=\vb{0}$, then $v_1=\cdots=v_n=\vb{0}$.
\item For $k=2,\dots,n$, $(V_1+\cdots+V_{k-1})\cap V_k=\{\vb{0}\}$.
\end{enumerate}
\end{lemma}

\begin{proof} \

(i)$\iff$(ii) First suppose $W$ is a direct sum. Then by the definition of direct sum, the only way to write $\vb{0}$ as a sum $u_1+\cdots+u_n$ is by taking $u_i=\vb{0}$.

Now suppose that the only way to write $\vb{0}$ as a sum $v_1+\cdots+v_n$ by taking $v_1=\cdots=v_n=\vb{0}$. For $v\in V_1+\cdots+V_n$, suppose that there is more than one way to represent $v$:
\begin{align*}
v&=v_1+\cdots+v_n\\
v&=v_1^\prime+\cdots+v_n^\prime
\end{align*}
for some $v_i,v_i^\prime\in V_i$. Substracting the above two equations gives
\[\vb{0}=(v_1-v_1^\prime)+\cdots+(v_n-v_n^\prime).\]
Since $v_i-v_i^\prime\in V_i$, we have $v_i-v_i^\prime=\vb{0}$ so $v_i=v_i^\prime$. Hence there is only one unique way to represent $v_1+\cdots+v_n$, thus $W$ is a direct sum.

(ii)$\iff$(iii) First suppose if $v_i\in V_i$ satisfies $v_1+\cdots+v_n=\vb{0}$, then $v_1=\cdots=v_n=\vb{0}$. Let $v_k\in(V_1+\cdots+V_{k-1})\cap V_k$. Then $v_k=v_1+\cdots+v_{k-1}$ where $v_i\in V_i$ ($1\le i\le k-1$). Thus
\begin{align*}
v_1+\cdots+v_{k-1}-v_k&=\vb{0}\\
v_1+\cdots+v_{k-1}+(-v_k)+\vb{0}+\cdots+\vb{0}&=\vb{0}
\end{align*}
by taking $v_{k+1}=\cdots=v_n=\vb{0}$. Then $v_1=\cdots=v_k=\vb{0}$.

Now suppose that for $k=2,\dots,n$, $(V_1+\cdots+V_{k-1})\cap V_k=\{\vb{0}\}$.
\begin{align*}
v_1+\cdots+v_n&=\vb{0}\\
v_1+\cdots+v_{n-1}&=-v_n
\end{align*}
where $v_1+\cdots+v_{n-1}\in V_1+\cdots+V_{n-1}$, $-v_n\in V_n$. Thus
\[v_1+\cdots+v_{n-1}=-v_n\in(V_1+\cdots+V_{n-1})\cap V_n=\{\vb{0}\}\]
so $v_1+\cdots+v_{n-1}=\vb{0}$, $v_n=\vb{0}$. Induction on $n$ gives $v_1=\cdots=v_{n-1}=v_n=\vb{0}$.
\end{proof}

\begin{proposition}
Suppose $U,W\le V$. Then $U+W$ is a direct sum if and only if $U\cap W=\{\vb{0}\}$.
\end{proposition}

\begin{proof} \

\fbox{$\implies$} Suppose that $U+W$ is a direct sum. If $v\in U\cap W$, then $\vb{0}=v+(-v)$, where $v\in U$, $-v\in W$. By the unique representation of $\vb{0}$ as the sum of a vector in $U$ and a vector in $W$, we have $v=\vb{0}$. Thus $U\cap W=\{\vb{0}\}$.

\fbox{$\impliedby$} Suppose $U\cap W=\{\vb{0}\}$. Suppose $u\in U$, $w\in W$, and $0=u+w$. $u=-w\in W$, thus $u\in U\cap W$, so $u=w=\vb{0}$. By \cref{lemma:condition-direct-sum}, $U+W$ is a direct sum.
\end{proof}

\section{Span and Linear Independence}
\begin{definition}[Linear combination]\label{def:linear-combination}
$v$ is a \vocab{linear combination}\index{linear combination} of vectors $v_1,\dots,v_n\in V$ if there exists $a_1,\dots,a_n\in\FF$ such that
\[v=a_1v_1+\cdots+a_nv_n.\]
\end{definition}

\begin{definition}[Span]\label{def:span}
The \vocab{span}\index{span} of $\{v_1,\dots,v_n\}$ is the set of all linear combinations of $v_1,\dots,v_n$:
\[\spn(v_1,\dots,v_n)\coloneqq\{a_1v_1+\cdots+a_nv_n\mid a_i\in\FF\}.\]
The span of the empty list $(\:)$ is defined to be $\{\vb{0}\}$.

We say that $v_1,\dots,v_n$ \emph{spans} $V$ if $\spn(v_1,\dots,v_n)=V$.

If $S\subset V$ is such that $\spn(S)=V$, then we say that $S$ \emph{spans} $V$, and that $S$ is a \emph{spanning set} for $V$:
\[\spn(S)\coloneqq\{a_1v_1+\cdots+a_nv_n\mid v_i\in S, a_i\in\FF\}.\]
\end{definition}

\begin{proposition}
$\spn(v_1,\dots,v_n)$ in $V$ is the smallest subspace of $V$ containing $v_1,\dots,v_n$.
\end{proposition}

\begin{proof}
First we show that $\spn(v_1,\dots,v_n)\le V$, using the subspace test.
\begin{enumerate}[label=(\roman*)]
\item $\vb{0}=0v_1+\cdots+0v_n\in\spn(v_1,\dots,v_n)$
\item $(a_1v_1+\cdots+a_nv_n)+(c_1v_1+\cdots+c_nv_n)=(a_1+c_1)v_1+\cdots+(a_n+c_n)v_n\in\spn(v_1,\dots,v_n)$, so $\spn(v_1,\dots,v_n)$ is closed under addition.
\item $\lambda(a_1v_1+a_nv_n)=(\lambda a_1)v_1+\cdots+(\lambda a_n)v_n\in\spn(v_1,\dots,v_n)$, so $\spn(v_1,\dots,v_n)$ is closed under scalar multiplication.
\end{enumerate}

Let $M$ be the smallest vector subspace of $V$ containing $v_1,\dots,v_n$. We claim that $M=\spn(v_1,\dots,v_n)$. To show this, we show that (i) $M\subset\spn(v_1,\dots,v_n)$ and (ii) $M\supset\spn(v_1,\dots,v_n)$.
\begin{enumerate}[label=(\roman*)]
\item Each $v_i$ is a linear combination of $v_1,\dots,v_n$, as
\[v_i=0\cdot v_1+\cdots+0\cdot v_{i-1}+1\cdot v_i+0\cdot v_{i+1}+\cdots+0\cdot v_n,\]
so by the definition of the span as the collection of all linear combinations of $v_1,\dots,v_n$, we have that $v_i\in\spn(v_1,\dots,v_n)$. But $M$ is the smallest vector subspace containing $v_1,\dots,v_n$, so
\[M\subset\spn(v_1,\dots,v_n).\]
\item Since $v_i\in M$ ($1\le i\le n$) and $M$ is a vector subspace (closed under addition and scalar multiplication), it follows that
\[a_1v_1+\dots+a_nv_n\in M\]
for all $a_i\in\FF$ (i.e. $M$ contains all linear combinations of $v_1,\dots,v_n$). So
\[\spn(v_1,\dots,v_n)\subset M.\]
\end{enumerate}
\end{proof}

\begin{definition}[Finite-dimensional vector space]
$V$ is \vocab{finite-dimensional}\index{finite-dimensional} if there exists some list of vector $(v_1,\dots,v_n)$ that spans $V$; otherwise, it is \emph{infinite-dimensional}.
\end{definition}

\begin{remark}
Recall that by defnition every list of vectors has finite length.
\end{remark}

\begin{remark}
From this definition, infinite-dimensionality is the negation of finite-dimensionality (i.e. \emph{not} finite-dimensional). Hence to prove that a vector space is infinite-dimensional, we prove by contradiction; that is, first assume that the vector space is finite-dimensional, then try to come to a contradiction.
\end{remark}

\begin{exercise}
For positive integer $n$, $\FF^n$ is finite-dimensional.
\end{exercise}

\begin{proof}
Suppose $(x_1,x_2,\dots,x_n)\in\FF^n$, then
\[(x_1,x_2,\dots,x_n)=x_1(1,0,\dots,0)+x_2(0,1,\dots,0)+\cdots+x_n(0,0,\dots,1)\]
so
\[(x_1,\dots,x_n)\in\spn\brac{(1,0,\dots,0),(0,1,\dots,0),\dots,(0,\dots,0,1)}.\]
The vectors $(1,0,\dots,0),(0,1,\dots,0),\dots,(0,\dots,0,1)$ spans $\FF^n$, so $\FF^n$ is finite-dimensional.
\end{proof}

\begin{definition}[Linear independence]\label{def:linear-independence}
A list of vectors $v_1,\dots,v_n$ is \vocab{linearly independent}\index{linear independence} in $V$ if the only choice of $a_1,\dots,a_n\in\FF$ that makes
\[a_1v_1+\cdots+a_nv_n=\vb{0}\]
is $a_1=\cdots=a_n=0$; otherwise, it is \emph{linearly dependent}.

We say that $S\subset V$ is linearly independent if every finite subset of $S$ is linearly independent.
\end{definition}

\begin{proposition}[Compare coefficients]
Let $v_1,\dots,v_n$ be linearly independent in $V$. Then
\[a_1v_1+\cdots+a_nv_n=b_1v_1+\cdots+b_nv_n\]
if and only if $a_i=b_i$ ($1\le i\le n$).
\end{proposition}

\begin{proof}
Exercise.
\end{proof}

\cref{lemma:linear-dependence} will often be useful; it states that given a linearly dependent list of vectors, one of the vectors is in the span of the previous ones and furthermore we can throw out that vector without changing the span of the original list.

\begin{lemma}[Linear dependence lemma]\label{lemma:linear-dependence}
Suppose $v_1,\dots,v_n$ are linearly dependent in $V$. Then there exists $v_k$ such that the following hold:
\begin{enumerate}[label=(\roman*)]
\item $v_k\in\spn\brac{v_1,\dots,v_{k-1}}$
\item $\spn(v_1,\dots,v_{k-1},v_{k+1},\dots,v_n)=\spn(v_1,\dots,v_n)$
\end{enumerate}
\end{lemma}

\begin{proof} \
\begin{enumerate}[label=(\roman*)]
\item Since $v_1,\dots,v_n$ are linearly dependent, there exists $a_1,\dots,a_n\in\FF$, not all $0$, such that
\[a_1v_1+\cdots+a_nv_n=0.\]
Let $k=\max\{1,\dots,n\}$ such that $a_k\neq0$. Then
\[v_k=-\frac{a_1}{a_k}v_1-\cdots-\frac{a_{k-1}}{a_k}v_{k-1},\]
which means that $v_k$ can be written as a linear combination of $v_1,\dots,v_{k-1}$, so $v_k\in\spn\brac{v_1,\dots,v_{k-1}}$ by definition of span.

\item Now suppose $k$ is such that $v_k\in\spn(v_1,\dots,v_{k-1})$. Then there exists $b_1,\dots,b_{k-1}\in\FF$ be such that
\begin{equation*}\tag{1}
v_k=b_1v_1+\cdots+b_{k-1}v_{k-1}.
\end{equation*}
Suppose $u\in\spn(v_1,\dots,v_n)$. Then there exists $c_1,\dots,c_n\in\FF$ such that
\begin{equation*}\tag{2}
u=c_1v_1+\cdots+c_nv_n.
\end{equation*}
In (2), we can replace $v_k$ with the RHS of (1), which gives
\begin{align*}
u&=c_1v_1+\cdots+c_{k-1}v_{k-1}+c_kv_k+c_{k+1}v_{k+1}+\cdots+c_nv_n\\
&=c_1v_1+\cdots+c_{k-1}v_{k-1}+c_k(b_1v_1+\cdots+b_{k-1}v_{k-1})+c_{k+1}v_{k+1}+\cdots+c_nv_n\\
&=c_1v_1+\cdots+c_{k-1}v_{k-1}+c_kb_1v_1+\cdots+c_kb_{k-1}v_{k-1}+c_{k+1}v_{k+1}+\cdots+c_nv_n\\
&=(c_1+bc_k)v_1+\cdots+(c_{k-1}+b_{k-1}c_k)v_{k-1}+c_{k+1}v_{k+1}+\cdots+c_nv_n.
\end{align*}
Thus $u\in\spn(v_1,\dots,v_{k-1},v_{k+1},\dots,v_n)$. This shows that removing $v_k$ from $v_1,\dots,v_n$ does not change the span of the list.
\end{enumerate}
\end{proof}

The following result says that no linearly independent list in $V$ is longer than a spanning list in $V$.

\begin{proposition}\label{prop:length-linind-span}
In a finite-dimensional vector space, the length of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors.
\end{proposition}

\begin{proof}
Suppose $u_1,\dots,u_m$ are linearly independent in $V$, $\{w_1,\dots,w_n\}$ is a spanning set of $V$. We want to show $m\le n$. We do so through the following steps:
\begin{itemize}
\item[Step 1] Adjoin $u_1$ at the beginning of $\{w_1,\dots,w_n\}$. Then $\{u_1,w_1,\dots,w_n\}$ is linearly dependent.

By linear dependence lemma, one of the vectors in $\{u_1,w_1,\dots,w_n\}$ is a linear combination of the previous vectors. Since $\{u_1,\dots,u_m\}$ is linearly independent, $u_1\neq\vb{0}$, so 
\end{itemize}
\end{proof}

\section{Bases}
\begin{definition}[Basis]\label{def:basis}
$B=\{v_1\dots,v_n\}$ is a \vocab{basis}\index{basis} of $V$ if
\begin{enumerate}[label=(\roman*)]
\item $B$ is linearly independent in $V$;
\item $B$ is a spanning set of $V$.
\end{enumerate}
\end{definition}

\begin{example}[Standard basis]
Let $\vb{e}_i=(0,\dots,0,1,0,\dots,0)$ where the $i$-th coordinate is $1$. $\{\vb{e}_1,\dots,\vb{e}_n\}$ is a basis of $\FF^n$, known as the \emph{standard basis} of $\FF^n$.
\end{example}

\begin{lemma}[Criterion for basis]\label{lemma:basis-criterion}
The following are equivalent:
\begin{enumerate}[label=(\roman*)]
\item $\{v_1,\dots,v_n\}$ is a basis of $V$.
\item Every $v\in V$ is uniquely expressed as a linear combination of $v_1,\dots,v_n$.
\item $v_i\neq0$, $V=Fv_1\oplus\cdots\oplus Fv_n$.
\end{enumerate}
\end{lemma}

\begin{proof}

\end{proof}

\begin{lemma}\label{lemma:reduce-spanninglist-basis}
Every spanning list in a vector space can be reduced to a basis of the vector space.
\end{lemma}

\begin{proof}
Suppose $B=\{v_1,\dots,v_n\}$ spans $V$. We want to remove some vectors from $B$ so that the remaining vectors form a basis of $V$. We do this through the multistep process described below.

\begin{enumerate}
\item[Step 1] If $v_1=\vb{0}$, delete $v_1$ from $B$. If $v_1\neq\vb{0}$, leave $B$ unchanged.
\item[Step $k$] If $v_k\in\spn(v_1,\dots,v_{k-1})$, delete $v_k$ from $B$. If $v_k\notin\spn(v_1,\dots,v_{k-1})$, leave $B$ unchanged.
\end{enumerate}

Stop the process after step $n$.

Since our original list spanned $V$ and we have discarded only vectors that were already in the span of the previous vectors, the resulting list $B$ spans $V$ because

The process ensures that no vector in $B$ is in the span of the previous ones. By linear dependence lemma, $B$ is linearly independent.

Since $B$ is linearly independent and spans $V$, $B$ is a basis of $V$.
\end{proof}

\begin{proposition}
Every finite-dimensional vector space has a basis.
\end{proposition}

\begin{proof}
By definition, a finite-dimensional vector space has a spanning list. By \cref{lemma:reduce-spanninglist-basis}, the spanning list can be reduced to a basis.
\end{proof}

\begin{lemma}\label{lemma:extend-linind-basis}
Every linearly independent list of vectors in a finite-dimensional vector space can be extended to a basis of the vector space.
\end{lemma}

\begin{proof}
Suppose $\{u_1,\dots,u_m\}$ linearly independent in $V$, $\{w_1,\dots,w_n\}$ spans $V$. Thus
\[\{u_1,\dots,u_m,w_1,\dots,w_n\}\]
spans $V$. By \cref{lemma:reduce-spanninglist-basis}, we can reduce this list to a basis of $V$ consisting $u_1,\dots,u_m$ (since $\{u_1,\dots,u_m\}$ linearly independent) and some of the $w$'s.
\end{proof}

\begin{proposition}
Suppose $U\le V$. Then there exists a subspace $W$ of $V$ such that $V=U\oplus W$.
\end{proposition}

\begin{proof}

\end{proof}

\begin{proposition}\label{prop:bases-same-length}
Any two bases of a finite-dimensional vector space have the same length.
\end{proposition}

\begin{proof}
Suppose $V$ is finite-dimensional. Let $B_1$ and $B_2$ be two bases of $V$. Then $B_1$ is linearly independent in $V$ and $B_2$ spans $V$, so by \cref{prop:length-linind-span}, the length of $B_1$ is at most the length of $B_2$.

Similarly, $B_2$ is linearly independent in $V$ and $B_1$ spans $V$, so the length of $B_2$ is at most the length of $B_1$.

Hence the length of $B_1$ equals the length of $B_2$, as desired.
\end{proof}

\section{Dimension}
By \cref{prop:bases-same-length}, since any two bases of a fnite-dimensional vector space have the same length, we can formally define the dimension of such spaces.

\begin{definition}[Dimension]
The \vocab{dimension}\index{dimension} of $V$ is the length of any basis of $V$, denoted by $\dim V$.
\end{definition}

\begin{proposition}
Suppose $V$ is finite-dimensional, $U\le V$. Then $\dim U\le\dim V$.
\end{proposition}

\begin{proof}
Think of a basis of $U$ as a linearly independent list in $V$, and think of a basis of $V$ as a spanning list in $U$. Now use 2.22 to conclude that $\dim U\le\dim V$.
\end{proof}

\begin{proposition}
Suppose $V$ is finite-dimensional. Then every linearly independent list of vectors in $V$ with length $\dim V$ is a basis of $V$.
\end{proposition}

\begin{proof}
Suppose $\dim V=n$ and $(v_1,\dots,v_n)$ is linearly independent in $V$. By 2.32, the list $(v_1,\dots,v_n)$ can be extended to a basis of $V$. However, every basis of $V$ has length $n$, which means that no elements are adjoined to $(v_1,\dots,v_n)$. Hence $(v_1,\dots,v_n)$ is a basis of $V$, as desired.
\end{proof}

\begin{proposition}
Suppose $V$ is finite-dimensional, $U\le V$ and $\dim U=\dim V$. Then $U=V$.
\end{proposition}

\begin{proof}
Let $(u_1,\dots,u_n)$ be a basis of $U$. Then $\dim U=n$ so $\dim V=n$. Thus $(u_1,\dots,u_n)$ is a linearly indepdent list of vectors in $V$ (because it is a basis of $U$) of length $\dim V$. From 2.38, we see that $(u_1,\dots,u_n)$ is a basis of $V$. In particular every vector in $V$ is a linear combination of $u_1,\dots,u_n$. Thus $U=V$.
\end{proof}

\begin{proposition}
Suppose $V$ is finite-dimensional. Then every spanning list of vectors in $V$ with length $\dim V$ is a basis of $V$.
\end{proposition}

\begin{proof}
Suppose $\dim V=n$, and $\{v_1,\dots,v_n\}$ spans $V$. By 2.30, $\{v_1,\dots,v_n\}$ can be reduced to a basis of $V$. However, every basis of $V$ has length $n$, which means that no elements are deleted from $\{v_1,\dots,v_n\}$. Hence $\{v_1,\dots,v_n\}$ is a basis of $V$, as desired.
\end{proof}

\begin{lemma}[Dimension of sum]
Suppose $U_1,U_2\le V$. Then
\[\dim(U_1+U_2)=\dim U_1+\dim U_2-\dim(U_1\cap U_2).\]
\end{lemma}
\pagebreak

\subsection*{Exercises}
\begin{prbm}
Suppose $W$ is a vector space over $\FF$, $V_1$ and $V_2$ are subspaces of $W$. Show that $V_1\cap V_2$ is a vector space over $\FF$ if and only if $V_1\subset V_2$ or $V_2\subset V_1$.
\end{prbm}

\begin{solution}
The backward direction is trivial. We focus on proving the forward direction.

Supppse otherwise, then $V_1\setminus V_2\neq\emptyset$ and $V_2\setminus V_1\neq\emptyset$. Pick $v_1\in V_1\setminus V_2$ and $v_2\in V_2\setminus V_1$. Then
\begin{align*}
v_1,v_2\in V_1\cup V_2&\implies v_1+v_2\in V_1\cup V_2\\
&\implies v_2,v_1+v_2\in V_2\\
&\implies v_1=(v_1+v_2)-v_2\in V_2
\end{align*}
which is a contradiction.
\end{solution}

\begin{prbm}
Suppose $W$ is a vector space over $\FF$, $V_1,V_2,V_3$ are subspaces of $W$. Then $V_1\cup V_2\cup V_3$ is a vector space over $\FF$ if and only if one of the $V_i$ contains the other two.
\end{prbm}

\begin{solution}
We prove the forward direction. Suppose otherwise, then $v_1\in V_1\setminus(V_2+V_3)$, $v_2\in V_2\setminus(V_1+V_3)$, $v_3\in V_3\setminus(V_1+V_2)$. Consider
\[\{v_1+v_2+v_3,v_1+v_2+2v_3,v_1+2v_2+v_3,v_1+2v_2+2v_3\}\subset V_1\cup V_2\cup V_3\]
Then
\begin{align*}
&(v_1+v_2+2v_3)-(v_1+v_2+v_3)=v_3\notin V_1+V_2\\
&\implies v_1+v_2+v_3\notin V_1+V_2\quad\text{or}\quad v_1+v_2+2v_3\notin V_1+V_2\\
&\implies v_1+v_2+v_3\in V_3\quad\text{or}\quad v_1+v_2+2v_3\in V_3\\
&\implies v_1+v_2\in V_3
\end{align*}
Similarly,
\begin{align*}
&(v_1+2v_2+2v_3)-(v_1+2v_2+v_3)=v_3\notin V_1+V_2\\
&\implies v_1+2v_2+v_3\notin V_1+V_2\quad\text{or}\quad v_1+2v_2+2v_3\notin V_1+V_2\\
&\implies v_1+2v_2+v_3\in V_3\quad\text{or}\quad v_1+2v_2+2v_3\in V_3\\
&\implies v_1+2v_2\in V_3
\end{align*}
This implies $(v_1+2v_2)-(v_1+v_2)=v_2\in V_3$, a contradiction.
\end{solution}