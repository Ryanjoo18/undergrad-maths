\chapter{Vector Spaces}\label{chap:vector-spaces}
\section{Definition of Vector Space}
Let $\FF$ denote a field, which can mean either $\RR$ or $\CC$.

\begin{definition}[Vector space]
$V$ is a \vocab{vector space}\index{vector space} over $\FF$ if the following properties hold:
\begin{enumerate}[label=(\roman*)]
\item Addition is commutative: $u+v=v+u$ for all $u,v\in V$
\item Addition is associative: $(u+v)+w=u+(v+w)$ for all $u,v,w\in V$

Multiplication is associative: $(ab)v=a(bv)$ for all $v\in V$, $a,b\in\FF$
\item Additive identity: there exists $\vb{0}\in V$ such that $v+\vb{0}=v$ for all $v\in V$
\item Additive inverse: for every $v\in V$, there exists $w\in V$ such that $v+w=\vb{0}$
\item Multiplicative identity: $1v=v$ for all $v\in V$
\item Distributive properties: $a(u+v)=au+av$ and $(a+b)v=av+bv$ for all $a,b,\in\FF$ and $u,v\in V$
\end{enumerate}
\end{definition}

\begin{notation}
For the rest of this text, $V$ denotes a vector space over $\FF$.
\end{notation}

Elements of a vector space are called \emph{vectors} or \emph{points}.

\begin{remark}
The scalar multiplication in a vector space depends on $\FF$. Thus when we need to be precise, we will say that $V$ is a vector space \emph{over $\FF$}. 

A vector space over $\RR$ is called a \emph{real vector space}\index{vector space!real vector space}; a vector space over $\CC$ is called a \emph{complex vector space}\index{vector space!complex vector space}.
\end{remark}

\begin{lemma}[Uniqueness of additive identity]
A vector space has a unique additive identity.
\end{lemma}

\begin{proof}
Suppose that $\vb{0}$ and $\vb{0}^\prime$ are additive identities of $V$. Then
\[\vb{0}^\prime=\vb{0}^\prime+\vb{0}=\vb{0}+\vb{0}^\prime=\vb{0}\]
where the first equality holds because $\vb{0}$ is an additive identity, the second equality comes from commutativity, and the third equality holds because $\vb{0}^\prime$ is an additive identity.
\end{proof}

\begin{lemma}[Uniqueness of additive inverse]
Every element in a vector space has a unique additive inverse.
\end{lemma}

\begin{proof}
Let $v\in V$. Suppose $w$ and $w^\prime$ are additive inverses of $v$. Then
\[w=w+\vb{0}=w+(v+w^\prime)=(w+v)+w^\prime=\vb{0}+w^\prime=w^\prime.\]
\end{proof}

Because additive inverses are unique, the following notation now makes sense.

\begin{notation}
Let $v,w\in V$. Then $-v$ denotes the additive inverse of $v$, and define $w-v$ to be $w+(-v)$.
\end{notation}

We now prove some seemingly trivial facts.

\begin{lemma} \
\begin{enumerate}[label=(\roman*)]
\item For every $v\in V$, $0v=\vb{0}$.
\item For every $a\in\FF$, $a\vb{0}=\vb{0}$.
\item For every $v\in V$, $(-1)v=-v$.
\end{enumerate}
\end{lemma}

\begin{proof} \
\begin{enumerate}[label=(\roman*)]
\item Let $v\in V$,
\[0v=(0+0)v=0v+0v.\]
Adding the additive inverse of $0v$ to both sides of the equation gives $\vb{0}=0v$.

\item Let $a\in\FF$,
\[a\vb{0}=a(\vb{0}+\vb{0})=a\vb{0}+a\vb{0}.\]
Adding the additive inverse of $a\vb{0}$ to both sides of the equation gives $\vb{0}=a\vb{0}$.

\item Let $v\in V$,
\[v+(-1)v=1v+(-1)v=(1+(-1))v=0v=\vb{0}.\]
Since $v+(-1)v=\vb{0}$, $(-1)v$ is the additive inverse of $v$.
\end{enumerate}
\end{proof}

\begin{example}[$n$-tuple space]
Let $\FF^n$ be the set of $n$-tuples whose elements belong to $\FF$:
\[\FF^n\colonequals\{(x_1,\dots,x_n)\mid x_i\in\FF\}\]
For $x=(x_1,\dots,x_n)\in\FF^n$ and $i=1,\dots,n$, we say that $x_i$ is the $i$-th \emph{coordinate} of $x$.

Define addition and scalar multiplication on $\FF^n$ as
\begin{align*}
(x_1,\dots,x_n)+(y_1,\dots,y_n)&=(x_1+y_1,\dots,x_n+y_n)\\
\lambda(x_1,\dots,x_n)&=(\lambda x_1,\dots,\lambda x_n)
\end{align*}

Then $\FF^n$ is a vector space over $\FF$.
\end{example}

\begin{example}
Let $\FF^\infty$ be the set of all sequences of elements of $\FF$:
\[\FF^\infty\colonequals\{(x_1,x_2,\dots)\mid x_i\in\FF\}\]
Define addition and scalar multiplication on $\FF^\infty$ as
\begin{align*}
(x_1,x_2,\dots)+(y_1,y_2,\dots)&=(x_1+y_1,x_2+y_2,\dots)\\
\lambda(x_1,x_2,\dots)&=(\lambda x_1,\lambda x_2,\dots)
\end{align*}
Then $\FF^\infty$ is a vector space over $\FF$, where the additive identity is $\vb{0}=(0,0,\dots)$.
\end{example}

\begin{example}[Space of functions from a set to a field]
If $S$ is a set, $\FF^S\colonequals\{f\mid f\colon S\to\FF\}$. Define addition and scalar multiplication on $\FF^S$ as
\begin{align*}
(f+g)(x)&=f(x)+g(x)\quad(x\in S)\\
(\lambda f)(x)&=\lambda f(x)\quad(x\in S)
\end{align*}
for all $f,g\in\FF^S$, $\lambda\in\FF$.
Then $\FF^S$ is a vector space over $\FF$ (if $S$ is a non-empty set), where the additive identity of $\FF^S$ is the function $0:S\to\FF$ defined as
\[0(x)=0\quad(\forall x\in S)\]
and for $f\in\FF^S$, additive inverse of $f$ is the function $-f:S\to\FF$ defined as
\[(-f)(x)=-f(x)\quad(\forall x\in S)\]
\end{example}

\begin{remark}
$\FF^n$ and $\FF^\infty$ are special cases of the vector space $\FF^S$; think of $\FF^n$ as $\FF^{\{1,2,\dots,n\}}$, and $\FF^\infty$ as $\FF^{\{1,2,\dots\}}$.
\end{remark}

\begin{example}[Complexification]
Suppose $V$ is a real vector space. The \emph{complexifcation} of $V$, denoted by $V_\CC$, equals $V\times V$. An element of $V_\CC$ is an ordered pair $(u,v)$, where $u,v\in V$, which we write as $u+iv$.
\begin{itemize}
\item Addition on $V_\CC$ is defined as
\[(u_1+iv_1)+(u_2+iv_2)=(u_1+u_2)+i(v_1+v_2)\]
for all $u_1,v_2,u_2,v_2\in V$.
\item Complex scalar multiplication on $V_\CC$ is defined as
\[(a+bi)(u+iv)=(au-bv)+i(av+bu)\]
for all $a,b\in\RR$ and all $u,v\in V$.
\end{itemize}
Then $V_\CC$ is a (complex) vector space.
\end{example}
\pagebreak

\section{Subspaces}
Whenever we have a mathematical object with some structure, we want to consider subsets that also have the same structure.

\begin{definition}[Subspace]
We say $U\subset V$ is a \vocab{subspace}\index{vector space!subspace} of $V$, denoted as $U\le V$, if $U$ is also a vector space (with the same addition and scalar multiplication as on $V$).
\end{definition}

The sets $\{\vb{0}\}$ and $V$ are always subspaces of $V$. The subspace $\{\vb{0}\}$ is called the \emph{zero subspace} or \emph{trivial subspace}. Subspaces other than $V$ are called \emph{proper subspaces}.

The following result is useful in determining whether a given subset of $V$ is a subspace of $V$.

\begin{lemma}[Subspace test]\label{lemma:subspace-test}
Suppose $U\subset V$. Then $U\le V$ if and only if $U$ satisfies the following conditions:
\begin{enumerate}[label=(\roman*)]
\item $\vb{0}\in U$;\hfill(additive identity)
\item $u+w\in U$ for all $u,w\in U$;\hfill(closed under addition)
\item $\lambda u\in U$ for all $\lambda\in\FF$, $u\in U$.\hfill(closed under scalar multiplication)
\end{enumerate}
\end{lemma}

\begin{proof} \

\fbox{$\implies$} If $U\le V$, then $U$ satisfies the three conditions above by the definition of vector space.

\fbox{$\impliedby$} Suppose $U$ satisfies the three conditions above. (i) ensures that the additive identity of $V$ is in $U$. (ii) ensures that addition makes sense on $U$. (iii) ensures that scalar multiplication makes sense on $U$.

If $u\in U$, then $-u=(-1)u\in U$ by (iii). Hence every element of $U$ has an additive inverse in $U$.

The other parts of the definition of a vector space, such as associativity and commutativity, are automatically satisfied for $U$ because they hold on the larger space $V$. Thus $U$ is a vector space and hence is a subspace of $V$.
\end{proof}

\begin{lemma}[A subspace of a subspace is a subspace]
If $U\le V$ and $W\le U$, then $W\le V$.
\end{lemma}

\begin{proof}
This is immediate from the definition of a subspace.
\end{proof}

\begin{lemma}[Intersection of subspaces is a subspace]
Let $\{U_i\mid i\in I\}$ be a collection of subspaces of $V$. Then $\bigcap_{i\in I}U_i\le V$.
\end{lemma}

\begin{proof}
Let $U=\bigcap_{i\in I}U_i$. 
\begin{enumerate}[label=(\roman*)]
\item Since each $U_i\le V$, $\vb{0}\in U_i$ so $\vb{0}\in U$.
\item Let $u,w\in U$, then $u,w\in U_i$. Since $U_i\le V$, $u+w\in U_i$ so $u+w\in U$.
\item Let $\lambda\in\FF$, $u\in U$, then $u\in U_i$. Since $U_i\le V$, $\lambda u\in U_i$ so $\lambda u\in U$.
\end{enumerate}
\end{proof}

\begin{definition}[Sum of subsets]
Suppose $U_1,\dots,U_n\subset V$. The \vocab{sum} of $U_1,\dots,U_n$ is the set of all possible sums of elements of $U_1,\dots,U_n$:
\[U_1+\cdots+U_n\colonequals\{u_1+\cdots+u_n\mid u_i\in U_i\}.\]
\end{definition}

\begin{example} \
\begin{itemize}
\item Let $U=\{(x,0,0)\in\FF^3\mid x\in F\}$ and $W=\{(0,y,0)\in\FF^3\mid y\in\FF\}$. Then
\[U+W=\{(x,y,0)\mid x,y\in\FF\}.\]
\item Let $U=\{(x,x,y,y)\in\FF^4\mid x,y\in\FF\}$ and $W=\{(x,x,x,y)\in\FF^4\mid x,y\in\FF\}$. Then
\[U+W=\{(x,x,y,z)\in\FF^4\mid x,y,z\in\FF\}.\]
\end{itemize}
\end{example}

The next result states that the sum of subspaces is a subspace, and is in fact the smallest subspace containing all the summands.

\begin{proposition}
Suppose $U_1,\dots,U_n\le V$. Then $U_1+\cdots+U_n$ is the smallest subspace of $V$ containing $U_1,\dots,U_n$.
\end{proposition}

\begin{proof}
It is easy to see that $\vb{0}\in U_1+\cdots+U_n$ and that $U_1+\cdots+U_n$ is closed under addition and scalar multiplication. Hence by the subspace test, $U_1+\cdots+U_n\le V$.

Let $M$ be the smallest subspace of $V$ containing $U_1,\dots,U_n$. We want to show that $U_1+\cdots+U_n=M$. To do so, we show double inclusion.

\fbox{$\supset$} For all $u_i\in U_i$ ($1\le i\le n$),
\[u_i=\vb{0}+\cdots+\vb{0}+u_i+\vb{0}+\cdots+\vb{0}\in U_1+\cdots+U_n,\]
where all except one of the $u$'s are $\vb{0}$. Thus $U_i\subset U_1+\cdots+U_n$ for $1\le i\le n$. Hence $M\subset U_1+\cdots+U_n$.

\fbox{$\subset$} Conversely, every subspace of $V$ containing $U_1,\dots,U_n$ contains $U_1+\cdots+U_n$ (because subspaces must contain all finite sums of their elements). Hence $U_1+\cdots+U_n\subset M$.
\end{proof}

\begin{definition}[Direct sum]
Suppose $U_1,\dots,U_n\le V$. We say $U_1+\cdots+U_n$ is a \vocab{direct sum}\index{direct sum} if each element of $U_1+\cdots+U_n$ can be written in only one way as a sum $u_1+\cdots+u_n$, $u_i\in U_i$. In this case, we denote the sum as
\[U_1\oplus\cdots\oplus U_n.\]
\end{definition}

\begin{example} \
\begin{itemize}
\item Suppose that $U=\{(x,y,0)\in\FF^3\mid x,y\in\FF\}$ and $W=\{(0,0,z)\in\FF^3\mid z\in\FF\}$. Then $\FF^3=U\oplus W$.

\item Suppose $U_i$ is the subspace of $\FF^n$ of those vectors whose coordinates are all 0 except for the $i$-th coordinate; that is, $U_i=\{(0,\dots,0,x,0,\dots,0)\in\FF^n\mid x\in\FF\}$. Then $\FF^n=U_1\oplus\cdots\oplus U_n$.
\end{itemize}
\end{example}

The definition of direct sum requires
every vector in the sum to have a unique representation as an appropriate sum.
The next result shows that when deciding whether a sum of subspaces is a direct sum, we only need to consider whether $\vb{0}$ can be uniquely written as an appropriate sum.

\begin{lemma}[Condition for direct sum]\label{lemma:condition-direct-sum}
Suppose $V_1,\dots,V_n\le V$. Then $V_1\oplus\cdots\oplus V_n$ if and only if $v_1+\cdots+v_n=\vb{0}$ implies $v_1=\cdots=v_n=\vb{0}$.
\end{lemma}

\begin{proof} \

\fbox{(i)$\implies$(ii)} Suppose $V_1+\cdots+V_n$ is a direct sum. Then by the definition of direct sum, the only way to write $\vb{0}$ as a sum $u_1+\cdots+u_n$ is by taking $u_i=\vb{0}$.

\fbox{(ii)$\implies$(i)} Suppose that the only way to write $\vb{0}$ as a sum $v_1+\cdots+v_n$ by taking $v_1=\cdots=v_n=\vb{0}$. 

To show that $v\in V_1+\cdots+V_n$ is a direct sum, let $v\in V_1+\cdots+V_n$. Then
\begin{equation*}\tag{I}
v=v_1+\cdots+v_n
\end{equation*}
for some $v_i\in V_i$.
To show that this representation is unique, suppose
\begin{equation*}\tag{II}
v=v_1^\prime+\cdots+v_n^\prime
\end{equation*}
for some $v_i^\prime\in V_i$.
Substracting (II) from (I) gives
\[\vb{0}=(v_1-v_1^\prime)+\cdots+(v_n-v_n^\prime).\]
Since $v_i-v_i^\prime\in V_i$, the equation above implies $v_i-v_i^\prime=\vb{0}$, so $v_i=v_i^\prime$. Hence there is only one unique way to represent $v_1+\cdots+v_n$.
\end{proof}

The next result provides a characterisation for direct sum.

\begin{lemma}\label{lemma:direct-sum-intersection-zero}
Suppose $U,W\le V$. Then $U+W$ is a direct sum if and only if $U\cap W=\{\vb{0}\}$.
\end{lemma}

\begin{proof} \

\fbox{$\implies$} Suppose that $U+W$ is a direct sum. Let $v\in U\cap W$, we will show that $v=\vb{0}$.

Note that $\vb{0}=v+(-v)$, where $v\in U$, $-v\in W$. By the unique representation of $\vb{0}$ as the sum of a vector in $U$ and a vector in $W$, we must have $v=\vb{0}$. Hence $U\cap W=\{\vb{0}\}$.

\fbox{$\impliedby$} Suppose $U\cap W=\{\vb{0}\}$. Suppose $u\in U$, $w\in W$, and $0=u+w$. $u=-w\in W$, thus $u\in U\cap W$, so $u=w=\vb{0}$. By \ref{lemma:condition-direct-sum}, $U+W$ is a direct sum.
\end{proof}
\pagebreak

\section{Span and Linear Independence}
\begin{definition}[Linear combination]
We say $v\in V$ is a \vocab{linear combination}\index{linear combination} of $v_1,\dots,v_n\in V$ if there exists $a_1,\dots,a_n\in\FF$ such that
\begin{align*}
v&=a_1v_1+\cdots+a_nv_n\\
&=\sum_{i=1}^{n}a_iv_i.
\end{align*}
\end{definition}

\begin{definition}[Span]
The \vocab{span}\index{span} of $\{v_1,\dots,v_n\}$ is the set of all linear combinations of $v_1,\dots,v_n$:
\[\spn(v_1,\dots,v_n)\colonequals\{a_1v_1+\cdots+a_nv_n\mid a_i\in\FF\}.\]
We say $v_1,\dots,v_n$ \emph{spans} $V$ if $\spn(v_1,\dots,v_n)=V$.

If $S\subset V$ is such that $\spn(S)=V$, we say $S$ \emph{spans} $V$, and $S$ is a \emph{spanning set} for $V$:
\[\spn(S)\colonequals\{a_1v_1+\cdots+a_nv_n\mid v_i\in S, a_i\in\FF\}.\]
\end{definition}

\begin{proposition}
$\spn(v_1,\dots,v_n)$ in $V$ is the smallest subspace of $V$ containing $v_1,\dots,v_n$.
\end{proposition}

\begin{proof}
First we show that $\spn(v_1,\dots,v_n)\le V$, using the subspace test.
\begin{enumerate}[label=(\roman*)]
\item $\vb{0}=0v_1+\cdots+0v_n\in\spn(v_1,\dots,v_n)$
\item $(a_1v_1+\cdots+a_nv_n)+(c_1v_1+\cdots+c_nv_n)=(a_1+c_1)v_1+\cdots+(a_n+c_n)v_n\in\spn(v_1,\dots,v_n)$, so $\spn(v_1,\dots,v_n)$ is closed under addition.
\item $\lambda(a_1v_1+a_nv_n)=(\lambda a_1)v_1+\cdots+(\lambda a_n)v_n\in\spn(v_1,\dots,v_n)$, so $\spn(v_1,\dots,v_n)$ is closed under scalar multiplication.
\end{enumerate}

Let $M$ be the smallest vector subspace of $V$ containing $v_1,\dots,v_n$. We claim that $M=\spn(v_1,\dots,v_n)$.

\fbox{$\subset$} Each $v_i$ is a linear combination of $v_1,\dots,v_n$, as
\[v_i=0\cdot v_1+\cdots+0\cdot v_{i-1}+1\cdot v_i+0\cdot v_{i+1}+\cdots+0\cdot v_n,\]
so by the definition of the span as the collection of all linear combinations of $v_1,\dots,v_n$, we have that $v_i\in\spn(v_1,\dots,v_n)$. But $M$ is the smallest vector subspace containing $v_1,\dots,v_n$, so
\[M\subset\spn(v_1,\dots,v_n).\]

\fbox{$\supset$} Since $v_i\in M$ ($1\le i\le n$) and $M$ is a vector subspace (closed under addition and scalar multiplication), it follows that
\[a_1v_1+\cdots+a_nv_n\in M\]
for all $a_i\in\FF$ (i.e., $M$ contains all linear combinations of $v_1,\dots,v_n$). Thus
\[\spn(v_1,\dots,v_n)\subset M.\]
\end{proof}

\begin{definition}[Finite-dimensional]
We say $V$ is \vocab{finite-dimensional}\index{finite-dimensional} if it has a finite spanning set; otherwise, it is \emph{infinite-dimensional}.
\end{definition}

\begin{example}
For positive integer $n$, $\FF^n$ is finite-dimensional.
\end{example}

\begin{proof}
Suppose $(x_1,x_2,\dots,x_n)\in\FF^n$, then
\[(x_1,x_2,\dots,x_n)=x_1(1,0,\dots,0)+x_2(0,1,\dots,0)+\cdots+x_n(0,0,\dots,1)\]
so
\[(x_1,\dots,x_n)\in\spn\brac{(1,0,\dots,0),(0,1,\dots,0),\dots,(0,\dots,0,1)}.\]
The vectors $(1,0,\dots,0),(0,1,\dots,0),\dots,(0,\dots,0,1)$ spans $\FF^n$, so $\FF^n$ is finite-dimensional.
\end{proof}

\begin{definition}[Linear independence]
We say $v_1,\dots,v_n$ are \vocab{linearly independent}\index{linear independence} in $V$ if
\[a_1v_1+\cdots+a_nv_n=\vb{0}\implies a_1=\cdots=a_n=0.\]
Otherwise, the vectors are \emph{linearly dependent}.

We say $S\subset V$ is linearly independent if every finite subset of $S$ is linearly independent.
\end{definition}

\begin{lemma}[Compare coefficients]
Let $v_1,\dots,v_n$ be linearly independent in $V$. Then
\[a_1v_1+\cdots+a_nv_n=b_1v_1+\cdots+b_nv_n\]
if and only if $a_i=b_i$ ($1\le i\le n$).
\end{lemma}

\begin{proof}
Exercise.
\end{proof}

The following are easy consequences of the definition.
\begin{enumerate}
\item Any set which contains a linearly dependent set is linearly dependent.
\item Any subset of a linearly independent set is linearly independent.
\item Any set which contains $\vb{0}$ is linearly dependent.
\item A set $S$ of vectors is linearly independent if and only if each finite subset of $S$ is linearly independent.
\end{enumerate}

The following result will often be useful; (i) states that given a linearly dependent set of vectors, one of the vectors is in the span of the previous ones; furthermore (ii) states that we can throw out that vector without changing the span of the original set.

\begin{lemma}[Linear dependence lemma]\label{lemma:linear-dependence}
Suppose $v_1,\dots,v_n$ are linearly dependent in $V$. Then there exists $v_i$ such that the following hold:
\begin{enumerate}[label=(\roman*)]
\item $v_i\in\spn\brac{v_1,\dots,v_{i-1}}$
\item $\spn(v_1,\dots,v_{i-1},v_{i+1},\dots,v_n)=\spn(v_1,\dots,v_n)$
\end{enumerate}
\end{lemma}

\begin{proof} \
\begin{enumerate}[label=(\roman*)]
\item Since $v_1,\dots,v_n$ are linearly dependent, there exists $a_1,\dots,a_n\in\FF$, not all $0$, such that
\[a_1v_1+\cdots+a_nv_n=0.\]
Take $i=\max\{1,\dots,n\}$ such that $a_i\neq0$. Then
\[v_i=-\frac{a_1}{a_i}v_1-\cdots-\frac{a_{i-1}}{a_i}v_{i-1},\]
since $a_{i-1},\dots,a_n=0$.
Thus $v_i$ can be written as a linear combination of $v_1,\dots,v_{i-1}$, so $v_i\in\spn\brac{v_1,\dots,v_{i-1}}$.

\item Now suppose $i$ is such that $v_i\in\spn(v_1,\dots,v_{i-1})$. Then there exists $b_1,\dots,b_{i-1}\in\FF$ be such that
\begin{equation*}\tag{I}
v_i=b_1v_1+\cdots+b_{i-1}v_{i-1}.
\end{equation*}
Suppose $u\in\spn(v_1,\dots,v_n)$. Then there exists $c_1,\dots,c_n\in\FF$ such that
\begin{equation*}\tag{II}
u=c_1v_1+\cdots+c_nv_n.
\end{equation*}
Substituting (I) into (II) gives
\begin{align*}
u&=c_1v_1+\cdots+c_{i-1}v_{i-1}+c_iv_i+c_{i+1}v_{i+1}+\cdots+c_nv_n\\
&=c_1v_1+\cdots+c_{i-1}v_{i-1}+c_i(b_1v_1+\cdots+b_{i-1}v_{i-1})+c_{i+1}v_{i+1}+\cdots+c_nv_n\\
&=c_1v_1+\cdots+c_{i-1}v_{i-1}+c_ib_1v_1+\cdots+c_ib_{i-1}v_{i-1}+c_{i+1}v_{i+1}+\cdots+c_nv_n\\
&=(c_1+bc_i)v_1+\cdots+(c_{i-1}+b_{i-1}c_i)v_{i-1}+c_{i+1}v_{i+1}+\cdots+c_nv_n.
\end{align*}
Thus $u\in\spn(v_1,\dots,v_{i-1},v_{i+1},\dots,v_n)$. This shows that removing $v_i$ from $v_1,\dots,v_n$ does not change the span of the set.
\end{enumerate}
\end{proof}

The next result says that no linearly independent set in $V$ is longer than a spanning set in $V$.

\begin{proposition}\label{prop:length-linind-span}
In a finite-dimensional vector space, the length of every linearly independent set of vectors is less than or equal to the length of every spanning set of vectors.
\end{proposition}

That is,
\begin{equation}
\{\text{linearly independent}\}\le\{\text{spanning set}\}.
\end{equation}

\begin{proof}
Suppose $A=\{u_1,\dots,u_m\}$ is linearly independent in $V$, $B=\{w_1,\dots,w_n\}$ spans $V$. We want to show that $m\le n$.

We do so through the process described below with $m$ steps; in each step, we add one of the $u$'s and remove one of the $w$'s.

\begin{description}
\item[Step 1] Since $B$ spans $V$, if we add any other vector to $B$, we will get a linearly dependent set, since this newly added vector can, by the definition of a span, be expressed as a linear combination of the vectors in $B$. In particular, if we add $u_1$ to $B$, then the new set
\[\{u_1,w_1,\dots,w_n\}\]
is linearly dependent.

By the linear independence lemma, one of the vectors in the above set is a linear combination of the previous vectors. 
Since $\{u_1,\dots,u_m\}$ is linearly independent, $u_1\neq\vb{0}$ so $u_1\notin\spn\{\:\}=\{\vb{0}\}$.
Hence the linear dependence lemma implies we can remove one of the $w$'s, so that the new set $B$ (of length $n$) consisting of $u_1$ and the remaining $w$'s spans $V$.

\item[Step $i$ ($2\le i\le m$)] 
The set $B$ (of length $n$) from step $i-1$ spans $V$. In particular, $u_i$ is in the span of $B$. If we add $u_i$ to $B$, placing it just after $u_1,\dots,u_{i-1}$, then the new set (of length $n+1$)
\[\{u_1,\dots,u_{i-1},u_i,w\text{'s}\}\]
is linearly dependent. 

By the linear dependence lemma, one of the vectors in this set is in the span of the previous ones. Since $u_1,\dots,u_i$ are linearly independent, this vector cannot be one of the $u$'s.
Hence there still must be at least one remaining $w$ at this step. We can remove from our new set (after adjoining $u_i$ in the proper place) a $w$ that is a linear combination of the previous vectors in the set, so that the new set $B$ (of length $n$) consisting of $u_1,\dots,u_i$ and the remaining $w$'s spans $V$.
\end{description}

After step $m$, we have added \emph{all} the $u$'s and the process stops. At each step as we add a $u$ to $B$, the linear dependence lemma implies that there is some $w$ to remove. 
Hence there must be at least as many $w$'s as $u$'s.
\end{proof}

We can use this result to show, without any computations, that certain sets are not linearly independent and that certain sets do not span a given vector space.

\begin{example} \
\begin{itemize}
\item $(1,0,0), (0,1,0), (0,0,1)$ spans $\RR^3$. Thus no set of length larger than three is linearly independent in $\RR^3$.
\item $(1,0,0,0), (0,1,0,0), (0,0,1,0), (0,0,0,1)$ is linearly independent in $\RR^4$. Thus no set of length less than four spans $\RR^4$.
\end{itemize}
\end{example}

Our intuition suggests that every subspace of a finite-dimensional vector space should also be finite-dimensional. We now prove that this intuition is correct.

\begin{proposition}\label{prop:subspace-finite-dim}
Every subspace of a finite-dimensional vector space is finite-dimensional.
\end{proposition}

\begin{proof}
Suppose $V$ is finite-dimensional, $U\le V$. To show that $U$ is finite-dimensional, we shall construct a spanning set of vectors in $U$, via the following steps.

\begin{description}
\item[Step 1] If $U=\{\vb{0}\}$, then $U$ is finite-dimensional and we are done. 

Otherwise, choose $v_1\in U$, $v_1\neq\vb{0}$ and add it to our set of vectors.

\item[Step $i$] Our set so far is $\{v_1,\dots,v_{i-1}\}$. 

If $U=\spn(v_1,\dots,v_{i-1})$, then $U$ is finite-dimensional and we are done. 

Otherwise, choose $v_i\in U$ such that $v_i\notin\spn(v_1,\dots,v_{i-1})$ and add it to our set.
\end{description}

After each step, we have constructed a set of vectors such that no vector in this set is in the span of the previous vectors; by the linear dependence lemma, our constructed set is linearly independent.

By \ref{prop:length-linind-span}, this linearly independent set cannot be longer than any spanning set of $V$. Thus the process must terminate after a finite number of steps, and we have constructed a spanning set of $U$. Hence $U$ is finite-dimensional.
\end{proof}
\pagebreak

\section{Bases}
\begin{definition}[Basis]
We say $B=\{v_1\dots,v_n\}$ is a \vocab{basis}\index{basis} of $V$ if
\begin{enumerate}[label=(\roman*)]
\item $B$ is linearly independent in $V$, and
\item $B$ is a spanning set of $V$.
\end{enumerate}
\end{definition}

\begin{example}[Standard basis]
Let $\vb{e}_i=(0,\dots,0,1,0,\dots,0)$, where the $i$-th coordinate is $1$. Then $\{\vb{e}_1,\dots,\vb{e}_n\}$ is a basis of $\FF^n$, known as the \emph{standard basis} of $\FF^n$.
\end{example}

The next result helps explain why bases are useful.

\begin{lemma}[Criterion for basis]\label{lemma:basis-criterion}
Let $B=\{v_1,\dots,v_n\}$ be a set of vectors in $V$. Then $B$ is a basis of $V$ if and only if every $v\in V$ can be uniquely expressed as a linear combination of $v_1,\dots,v_n$.
\end{lemma}

\begin{proof} \

\fbox{$\implies$} Let $v\in V$. Since $B$ is a basis of $V$, there exist $a_1,\dots,a_n\in\FF$ such that
\begin{equation*}\tag{I}
v=a_1v_1+\cdots+a_nv_n.
\end{equation*}
To show that the representation is unique, suppose that $c_1,\dots,c_n\in\FF$ also satisfy
\begin{equation*}\tag{II}
v=c_1v_1+\cdots+c_nv_n.
\end{equation*}
Subtracting (II) from (I) gives
\[\vb{0}=(a_1-c_1)v_1+\cdots+(a_n-c_n)v_n.\]
Since $v_1,\dots,v_n$ are linearly independent, we have $a_i-c_i=0$, or $a_i=c_i$ for all $i$.

\fbox{$\impliedby$} Suppose every $v\in V$ can be uniquely expressed as a linear combination of $v_1,\dots,v_n$. This implies that $B$ spans $V$. 

To show that $B$ is linearly independent, suppose that $a_1,\dots,a_n\in\FF$ are such that
\[a_1v_1+\cdots+a_nv_n=\vb{0}.\]
Since $\vb{0}$ can be uniquely expressed as a linear combination of $v_1,\dots,v_n$, we have $a_1=\cdots=a_n=0$, thus $B$ is linearly independent. 

Since $B$ is linearly independent and spans $V$, we conclude that $B$ is a basis of $V$.
\end{proof}

A spanning set in a vector space may not be a basis because it is not linearly independent. The next result says that given any spanning set, we can \emph{remove} some vectors so that the remaining set is linearly independent and still spans the vector space.

\begin{lemma}\label{lemma:reduce-spanningset-basis}
In a vector space, every spanning set can be reduced to a basis.
\end{lemma}

\begin{proof}
Suppose $B=\{v_1,\dots,v_n\}$ spans $V$. We want to remove some vectors from $B$ so that the remaining vectors form a basis of $V$. We do this through the multistep process described below.

\begin{description}
\item[Step 1] If $v_1=\vb{0}$, delete $v_1$ from $B$. If $v_1\neq\vb{0}$, leave $B$ unchanged.
\item[Step $i$ ($2\le i\le n$)] If $v_i\in\spn(v_1,\dots,v_{i-1})$, delete $v_i$ from $B$. If $v_i\notin\spn(v_1,\dots,v_{i-1})$, leave $B$ unchanged.
\end{description}

Since we only delete vectors from $B$ that are in the span of the previous vectors, by the linear dependence lemma, the set $B$ still spans $V$.

The process ensures that no vector in $B$ is in the span of the previous ones. By the linear dependence lemma, $B$ is linearly independent.

Since $B$ is linearly independent and spans $V$, we conclude that $B$ is a basis of $V$.
\end{proof}

\begin{corollary}\label{cor:basis-fin-dim}
Every finite-dimensional vector space has a basis.
\end{corollary}

\begin{proof}
We prove by construction. Suppose $V$ is finite-dimensional. By definition, there exists a spanning set of vectors in $V$. By \ref{lemma:reduce-spanningset-basis}, the spanning set can be reduced to a basis.
\end{proof}

We now show that given any linearly independent set, we can \emph{add} some vectors so that the extended set is still linearly independent but also spans the space.

\begin{lemma}\label{lemma:extend-linind-basis}
In a finite-dimensional vector space, every linearly independent set can be extended to a basis.
\end{lemma}

\begin{proof}
Suppose $\{u_1,\dots,u_m\}$ is linearly independent in $V$, and $\{w_1,\dots,w_n\}$ spans $V$. Then the set
\[\{u_1,\dots,u_m,w_1,\dots,w_n\}\]
spans $V$. By \ref{lemma:reduce-spanningset-basis}, we can reduce this set to a basis of $V$ consisting $u_1,\dots,u_m$ (since $u_1,\dots,u_m$ are linearly independent, $u_i\notin\spn(u_1,\dots,u_{i-1})$ for all $i$, so none of the $u_i$'s are deleted in the process), and some of the $w_i$'s.
\end{proof}

We now show that every subspace of a finite-dimensional vector space can be paired with another subspace to form a direct sum of the whole space.

\begin{corollary}
Suppose $V$ is finite-dimensional, $U\le V$. Then there exists $W\le V$ such that $V=U\oplus W$.
\end{corollary}

\begin{proof}
Since $V$ is finite-dimensional and $U\le V$, by \ref{prop:subspace-finite-dim}, $U$ is finite-dimensional. By \ref{cor:basis-fin-dim}, $U$ has a basis, say $B=\{u_1,\dots,u_n\}$.

Since $B$ is linearly independent, by \ref{lemma:extend-linind-basis}, $B$ can be extended to a basis of $V$, say
\[\{u_1,\dots,u_n,w_1,\dots,w_n\}.\]
\begin{claim}
$W=\spn(w_1,\dots,w_n)$.
\end{claim}
We need to show that $V=U\oplus W$; by \ref{lemma:condition-direct-sum}, we need to show (i) $V=U+W$, and (ii) $U\cap W=\{\vb{0}\}$.
\begin{enumerate}[label=(\roman*)]
\item Let $v\in V$. Since $\{u_1,\dots,u_n,w_1,\dots,w_n\}$ spans $V$, there exists $a_1,\dots,a_n,b_1,\dots,b_n\in\FF$ such that
\[v=a_1u_1+\cdots+a_nu_n+b_1w_1+\cdots+b_nw_n.\]
Take $u=a_1u_1+\cdots+a_nu_n\in U$, $w=b_1w_1+\cdots+b_nw_n\in W$. Then $v=u+w\in U+W$, so $V=U+W$.
\item Let $v\in U\cap W$. Since $v\in U$, $v$ can be written as a linear combination of $u_1,\dots,u_n$:
\begin{equation*}\tag{I}
v=a_1u_1+\cdots+a_nu_n.
\end{equation*}
Since $v\in W$, $v$ can be written as a linear combination of $w_1,\dots,w_n$:
\begin{equation*}\tag{II}
v=b_1w_1+\cdots+b_nw_n.
\end{equation*}
Subtracting (II) from (I) gives
\[\vb{0}=a_1u_1+\cdots+a_nu_n-b_1w_1-\cdots-b_nw_n.\]
Since $u_1,\dots,u_n,w_1,\dots,w_n$ are linearly independent, we have $a_i=b_i=0$ for all $i$. Thus $v=\vb{0}$, so $U\cap W=\{\vb{0}\}$.
\end{enumerate}
\end{proof}
\pagebreak

\section{Dimension}
\begin{lemma}
Any two bases of a finite-dimensional vector space have the same length.
\end{lemma}

\begin{proof}
Suppose $V$ is finite-dimensional, let $B_1$ and $B_2$ be two bases of $V$. By definition, $B_1$ is linearly independent in $V$, and $B_2$ spans $V$, so by \ref{prop:length-linind-span}, $|B_1|\le|B_2|$.

Similarly, by definition, $B_2$ is linearly independent in $V$ and $B_1$ spans $V$, so $|B_2|\le|B_1|$.

Since $|B_1|\le|B_2|$ and $|B_2|\le|B_1|$, we have $|B_1|=|B_2|$, as desired.
\end{proof}

Since any two bases of a finite-dimensional vector space have the same length, we can formally define the dimension of such spaces.

\begin{definition}[Dimension]
The \vocab{dimension}\index{dimension} of $V$ is the length of any basis of $V$, denoted by $\dim V$.
\end{definition}

\begin{lemma}[Dimension of subspace]\label{lemma:dim-subspace}
Suppose $V$ is finite-dimensional, $U\le V$. Then $\dim U\le\dim V$.
\end{lemma}

\begin{proof}
Since $V$ is finite-dimensional and $U\le V$, $U$ is finite-dimensional. Let $B_U$ be a basis of $U$, and $B_V$ be a basis of $V$.

By definition, $B_U$ is linearly independent in $V$, and $B_V$ spans $V$. By \ref{prop:length-linind-span}, $|B_U|\le|B_V|$, so
\[\dim U=|B_U|\le|B_V|=\dim V.\]
\end{proof}

To check that a set of vectors is a basis, we must show that it is linearly independent and that it spans the vector space. The next result shows that if the set has the \emph{right length}, then we only need to check that it satisfies one of the two required properties.

\begin{proposition}\label{prop:basis-length-dim}
Suppose $V$ is finite-dimensional. Then
\begin{enumerate}[label=(\roman*)]
\item every linearly independent set of vectors in $V$ with length $\dim V$ is a basis of $V$;
\item every spanning set of vectors in $V$ with length $\dim V$ is a basis of $V$.
\end{enumerate}
\end{proposition}

\begin{proof} \
\begin{enumerate}[label=(\roman*)]
\item Suppose $\dim V=n$, and $\{v_1,\dots,v_n\}$ is linearly independent in $V$. 

By \ref{lemma:extend-linind-basis}, $\{v_1,\dots,v_n\}$ can be extended to a basis of $V$. However, every basis of $V$ has length $n$, which means no elements are added to $\{v_1,\dots,v_n\}$. Hence $\{v_1,\dots,v_n\}$ is a basis of $V$.

\item Suppose $\dim V=n$, and $\{v_1,\dots,v_n\}$ spans $V$. 

By \ref{lemma:reduce-spanningset-basis}, $\{v_1,\dots,v_n\}$ can be reduced to a basis of $V$. However, every basis of $V$ has length $n$, which means no elements are removed from $\{v_1,\dots,v_n\}$. Hence $\{v_1,\dots,v_n\}$ is a basis of $V$.
\end{enumerate}
\end{proof}

\begin{corollary}\label{cor:subspace-full-dimension-equals-whole-space}
Suppose $V$ is finite-dimensional, $U\le V$. If $\dim U=\dim V$, then $U=V$.
\end{corollary}

\begin{proof}
Let $\dim U=\dim V=n$, let $\{u_1,\dots,u_n\}$ be a basis of $U$. 

Then $\{u_1,\dots,u_n\}$ is linearly independent in $V$ (because it is a basis of $U$) of length $\dim V$. By \ref{prop:basis-length-dim}, $\{u_1,\dots,u_n\}$ is a basis of $V$. In particular every vector in $V$ is a linear combination of $u_1,\dots,u_n$. Thus $U=V$.
\end{proof}

The next result gives a formula for the dimension of the sum of two subspaces of a finite-dimensional vector space. 

\begin{lemma}[Dimension of sum]
Suppose $V$ is finite-dimensional, $U_1,U_2\le V$. Then
\begin{equation}
\dim(U_1+U_2)=\dim U_1+\dim U_2-\dim(U_1\cap U_2).
\end{equation}
\end{lemma}

\begin{proof}
Let $\{u_1,\dots,u_m\}$ be a basis of $U_1\cap U_2$; thus $\dim(U_1\cap U_2)=m$. 

Since $\{u_1,\dots,u_m\}$ is a basis of $U_1\cap U_2$, it is linearly independent in $U_1$. By \ref{lemma:extend-linind-basis}, $\{u_1,\dots,u_m\}$ can be extended to a basis $\{u_1,\dots,u_m,v_1,\dots,v_j\}$ of $U_1$; thus $\dim U_1=m+j$. Similarly, extend $\{u_1,\dots,u_m\}$ to a basis $\{u_1,\dots,u_m,v_1,\dots,v_k\}$ of $U_2$; thus $\dim U_2=m+k$.

We will show that
\[\{u_1,\dots,u_m,v_1,\dots,v_j,w_1,\dots,w_k\}\]
is a basis of $U_1+U_2$. This will complete the proof because then we will have
\begin{align*}
\dim(U_1+U_2)&=m+j+k\\
&=(m+j)+(m+k)-m\\
&=\dim U_1+\dim U_2-\dim(U_1\cap U_2).
\end{align*}

We just need to show that $\{u_1,\dots,u_m,v_1,\dots,v_j,w_1,\dots,w_k\}$ is linearly independent. To prove this, suppose
\begin{equation*}\tag{I}
a_1u_1+\cdots+a_mu_m+b_1v_1+\cdots+b_jv_j+c_1w_1+\cdots+c_kw_k=\vb{0},
\end{equation*}
where $a_i,b_i,c_i\in\FF$. We need to show that $a_i=b_i=c_i=0$ for all $i$. (I) can be rewritten as
\[c_1w_1+\cdots+c_kw_k=-a_1u_1-\cdots-a_mu_m-b_1v_1-\cdots-b_jv_j,\]
which shows that $c_1w_1+\cdots+c_kw_k\in U_1$. But actually all the $w_i$'s are in $U_2$, so $c_1w_1+\cdots+c_kw_k\in U_2$. Thus $c_1w_1+\cdots+c_kw_k\in U_1\cap U_2$. Then we can write
\[c_1w_1+\cdots+c_kw_k=d_1u_1+\cdots+d_mu_m\]
for some $d_i\in\FF$. But $u_1,\dots,u_m,w_1,\dots,w_k$ are linearly independent, so $c_i=d_i=0$ for all $i$. Thus our original equation (I) becomes
\[a_1u_1+\cdots+a_mu_m+b_1v_1+\cdots+b_jv_j=\vb{0}.\]
Since $u_1,\dots,u_m,v_1,\dots,v_j$ are linearly independent, $a_i=b_i=0$ for all $i$, as desired.
\end{proof}
\pagebreak

\section*{Exercises}
\begin{exercise}[\cite{ladr} 1C Q12]
Suppose $W$ is a vector space over $\FF$, $V_1$ and $V_2$ are subspaces of $W$. Show that $V_1\cup V_2$ is a vector space over $\FF$ if and only if $V_1\subset V_2$ or $V_2\subset V_1$.
\end{exercise}

\begin{solution}
The backward direction is trivial. We focus on proving the forward direction.

Supppse otherwise, then $V_1\setminus V_2\neq\emptyset$ and $V_2\setminus V_1\neq\emptyset$. Pick $v_1\in V_1\setminus V_2$ and $v_2\in V_2\setminus V_1$. Then
\begin{align*}
v_1,v_2\in V_1\cup V_2&\implies v_1+v_2\in V_1\cup V_2\\
&\implies v_2,v_1+v_2\in V_2\\
&\implies v_1=(v_1+v_2)-v_2\in V_2
\end{align*}
which is a contradiction.
\end{solution}

\begin{exercise}[\cite{ladr} 1C Q13]
Suppose $W$ is a vector space over $\FF$, $V_1,V_2,V_3$ are subspaces of $W$. Then $V_1\cup V_2\cup V_3$ is a vector space over $\FF$ if and only if one of the $V_i$ contains the other two.
\end{exercise}

\begin{solution}
We prove the forward direction. Suppose otherwise, then $v_1\in V_1\setminus(V_2+V_3)$, $v_2\in V_2\setminus(V_1+V_3)$, $v_3\in V_3\setminus(V_1+V_2)$. Consider
\[\{v_1+v_2+v_3,v_1+v_2+2v_3,v_1+2v_2+v_3,v_1+2v_2+2v_3\}\subset V_1\cup V_2\cup V_3\]
Then
\begin{align*}
&(v_1+v_2+2v_3)-(v_1+v_2+v_3)=v_3\notin V_1+V_2\\
&\implies v_1+v_2+v_3\notin V_1+V_2\quad\text{or}\quad v_1+v_2+2v_3\notin V_1+V_2\\
&\implies v_1+v_2+v_3\in V_3\quad\text{or}\quad v_1+v_2+2v_3\in V_3\\
&\implies v_1+v_2\in V_3
\end{align*}
Similarly,
\begin{align*}
&(v_1+2v_2+2v_3)-(v_1+2v_2+v_3)=v_3\notin V_1+V_2\\
&\implies v_1+2v_2+v_3\notin V_1+V_2\quad\text{or}\quad v_1+2v_2+2v_3\notin V_1+V_2\\
&\implies v_1+2v_2+v_3\in V_3\quad\text{or}\quad v_1+2v_2+2v_3\in V_3\\
&\implies v_1+2v_2\in V_3
\end{align*}
This implies $(v_1+2v_2)-(v_1+v_2)=v_2\in V_3$, a contradiction.
\end{solution}

\begin{exercise}[\cite{ladr} 2A Q12]
Suppose $\{v_1,\dots,v_n\}$ is linearly independent in $V$, $w\in V$. Prove that if $\{v_1+w,\dots,v_n+w\}$ is linearly dependent, then $w\in\spn(v_1,\dots,v_n)$.
\end{exercise}

\begin{solution}
If $\{v_1+w,\dots,v_n+w\}$ is linearly dependent, then there exists $a_1,\dots,a_n\in\FF$, not all zero, such that
\[a_1(v_1+w)+\cdots+a_n(v_n+w)=0,\]
or
\[a_1v_1+\cdots+a_nv_n=-(a_1+\cdots+a_n)w.\]
Suppose otherwise, that $a_1+\cdots+a_n=0$. Then
\[a_1v_1+\cdots+a_nv_n=\vb{0},\]
but the linear independence of $\{v_1,\dots,v_n\}$ implies that $a_1=\cdots=a_n=0$, which is a contradiction. Hence we must have $a_1+\cdots+a_n\neq0$, so we can write
\[w=-\frac{a_1}{a_1+\cdots+a_n}v_1-\cdots-\frac{a_n}{a_1+\cdots+a_n}v_n,\]
which is a linear combination of $v_1,\dots,v_n$. Thus by definition of span, $w\in\spn(v_1,\dots,v_n)$.
\end{solution}

\begin{exercise}[\cite{ladr} 2A Q14]
Suppose $\{v_1,\dots,v_n\}\subset V$. Let
\[w_i=v_1+\cdots+v_i\quad(i=1,\dots,n)\]
Show that $\{v_1,\dots,v_n\}$ is linearly independent if and only if $\{w_1,\dots,w_n\}$ is linearly independent.
\end{exercise}

\begin{solution}
Write
\begin{align*}
v_1&=w_1\\
v_2&=w_2-w_1\\
v_3&=w_3-w_2\\
&\vdots\\
v_n&=w_n-w_{n-1}.
\end{align*}

\fbox{$\implies$}
\[a_1w_1+\cdots+a_nw_n=\vb{0}\]
for some $a_i\in\FF$. Expressing $w_i$'s as $v_i$'s,
\[a_1v_1+a_2(v_1+v_2)+\cdots+a_n(v_1+\cdots+v_n)=0,\]
or
\[(a_1+\cdots+a_n)v_1+(a_2+\cdots+a_n)v_2+\cdots+a_nv_n=\vb{0}.\]
Since $v_1,\dots,v_n$ are linearly independent,
\begin{align*}
a_1+a_2+\cdots+a_n&=0\\
a_2+\cdots+a_n&=0\\
&\vdots\\
a_n&=0
\end{align*}
on solving simultaneously gives $a_1=\cdots=a_n=0$.

\fbox{$\impliedby$} Similar to the above.
\end{solution}

\begin{exercise}[\cite{ladr} 2A Q18]
Prove that $\FF^\infty$ is infinite-dimensional.
\end{exercise}

\begin{solution}
Suppose, for a contradiction, that $\FF^\infty$ is finite-dimensional, i.e., there exists a finite spanning set $\{v_1,\dots,v_n\}$.
Let
\begin{align*}
e_1&=(1,0,\dots)\\
e_2&=(0,1,0,\dots)\\
e_3&=(0,0,1,0,\dots)\\
&\vdots\\
e_{n+1}&=(0,\dots,0,1,0,\dots)
\end{align*}
where $e_i$ has a $1$ at the $i$-th coordinate, and $0$'s for the remaining coordinates. Let
\[a_1e_1+\cdots+a_{n+1}e_{n+1}=\vb{0}\]
for some $a_i\in\FF$. Then
\[(a_1,a_2,\dots,a_{n+1},0,0,\dots)=\vb{0}\]
so $a_1=a_2=\cdots=a_{n+1}=0$. Thus $\{e_1,\dots,e_{n+1}\}$ is a linearly independent set, of length $n+1$. However, $\{v_1,\dots,v_n\}$ is a spanning set of length $n$. By \ref{prop:length-linind-span}, we have reached a contradiction.
\end{solution}

\begin{exercise}[\cite{ladr} 2B Q5]
Suppose $V$ is finite-dimensional, $U,W\le V$ such that $V=U+W$. Prove that $V$ has a basis in $U\cup W$.
\end{exercise}

\begin{solution}
Let $\{v_i\}_{i=1}^n$ denote the basis for $V$. By definition we have $v_i=u_i+w_i$ for some $u_i\in U$, $w_i\in W$. Then we have the spanning set of the vector space $V$ $\sum_{i=1}^{n}a_i(u_i+w_i)$, which can be reduced to a basis by the lemma.
\end{solution}

\begin{exercise}[\cite{ladr} 2B Q7]
Suppose $\{v_1,v_2,v_3,v_4\}$ is a basis of $V$. Prove that
\[\{v_1+v_2,v_2+v_3,v_3+v_4,v_4\}\]
is also a basis of $V$.
\end{exercise}

\begin{solution}
We know that $\{v_1,v_2,v_3,v_4\}$ is linearly independent and spans $V$. Then there exist $a_i\in\FF$ such that
\[a_1(v_1+v_2)+a_2(v_2+v_3)+a_3(v_3+v_4)+a_4v_4=0\implies a_1=a_2=a_3=a_4=0.\]

Write
\begin{align*}
&a_1(v_1+v_2)+a_2(v_2+v_3)+a_3(v_3+v_4)+a_4v_4\\
&=a_1v_1+(a_1+a_2)v_2+(a_2+a_3)v_3+(a_3+a_4)v_4,
\end{align*}
this shows the linear independence. To prove spanning, let $v\in V$, then
\begin{align*}
v&=a_1v_1+a_2v_2+a_3v_3+a_4v_4\\
&=a_1(v_1+v_2)+(a_2-a_1)(v_2+v_3)+(a_3-a_2)(v_3+v_4)+(a_4-a_3)v_4,
\end{align*}
which is a linear combination of $v_1+v_2,v_2+v_3,v_3+v_4,v_4$.
\end{solution}

\begin{exercise}[\cite{ladr} 2B Q10]
Suppose $U,W\le V$ such that $V=U\oplus W$. Suppose also that $\{u_1,\dots,u_m\}$ is a basis of $U$, $\{w_1,\dots,w_n\}$ is a basis of $W$. Prove that
\[\{u_1,\dots,u_m,w_1,\dots,w_n\}\]
is a basis of $V$.
\end{exercise}

\begin{solution}
We know that this set is linearly independent (otherwise violating the direct sum assumption) so it suffices to prove the spanning. Let $v\in V$, then
\[v=u+w=\sum_{i=1}^{m}a_iu_i+\sum_{i=1}^{n}b_jw_j.\]
\end{solution}

\begin{exercise}[\cite{ladr} 2C Q8]

\end{exercise}

\begin{exercise}[\cite{ladr} 2C Q16]

\end{exercise}

\begin{exercise}[\cite{ladr} 2C Q17]
Suppose that $V_1,\dots,V_n\le V$ are finite-dimensional. Prove that $V_1+\cdots+V_n$ is finite-dimensional, and
\[\dim(V_1+\cdots+V_n)\le\dim V_1+\cdots+\dim V_n.\]
\end{exercise}

\begin{solution}
We prove by induction on $n$. The base case is trivial. Assume the statement holds for $k$. Then for $k+1$, denoting $V_1+\cdots+V_k=M_k$, we have that
\[\dim(M_k+V_{k+1})\le\dim V_1+\cdots+\dim V_{k+1},\]
which is finite.
\end{solution}