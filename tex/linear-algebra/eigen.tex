\chapter{Eigenvalues and Eigenvectors}
\section{Invariant Subspaces}
\subsection{Eigenvalues}
\begin{definition}[Operator]
A linear map from a vector space to itself is called an \vocab{operator}\index{operator}.
\end{definition}

\begin{definition}[Invariant subspace]
Suppose $T\in\mathcal{L}(V)$. $U\le V$ is called \vocab{invariant}\index{invariant subspace} under $T$ if $Tu\in U$ for all $u\in U$.
\end{definition}

\begin{example}
Suppose $T\in\mathcal{L}(V)$. Then the following subspaces of $V$ are all invariant under $T$.
\begin{enumerate}[label=(\roman*)]
\item The subspace $\{\vb{0}\}$ is invariant under $T$ because if $u\in\{\vb{0}\}$, then $u=\vb{0}$ and hence $Tu=\vb{0}\in\{\vb{0}\}$.
\item The subspace $V$ is invariant under $T$ because if $u\in V$, then $Tu\in V$. 
\item The subspace $\ker T$ is invariant under $T$ because if $u\in\ker T$, then $Tu=\vb{0}$, and hence $Tu\in\ker T$, since a subspace must contain $\vb{0}$.
\item The subspace $\im T$ is invariant under $T$ because if $u\in\im T$, then $Tu\in\im T$ by definition.
\end{enumerate}
\end{example}

\begin{definition}[Eigenvalue and eigenvector]
Suppose $T\in\mathcal{L}(V)$. $\lambda\in\FF$ is called an \vocab{eigenvalue}\index{eigenvalue} of $T$ if there exists $v\in V\setminus\{\vb{0}\}$ such that $Tv=\lambda v$; $v$ is called an \vocab{eigenvector}\index{eigenvector} of $T$ corresponding to $\lambda$.
\end{definition}

\begin{lemma}[Equivalent conditions to be an eigenvalue]
Suppose $V$ is finite-dimensional, $T\in\mathcal{L}(V)$, $\lambda\in\FF$. Then the following are equivalent:
\begin{enumerate}[label=(\roman*)]
\item $\lambda$ is an eigenvalue of $T$.
\item $T-\lambda I$ is not injective.
\item $T-\lambda I$ is not surjective.
\item $T-\lambda I$ is not invertible.
\end{enumerate}
\end{lemma}

\begin{proof} \

\fbox{(i)$\iff$(ii)} $Tv=\lambda v$ is equivalent to the equation $(T-\lambda I)v=\vb{0}$, so $T-\lambda I$ is not injective.

\fbox{(ii)$\iff$(iii)$\iff$(iv)} This directly follows from \cref{lemma:invertibility-criterion}.
\end{proof}

\begin{proposition}[Linearly independent eigenvectors]\label{prop:eigenvectors-linind}
Suppose $T\in\mathcal{L}(V)$. Then every list of eigenvectors of $T$ corresponding to distinct eigenvalues of $T$ is linearly independent.
\end{proposition}

\begin{proof}
We prove by contradiction. Suppose, for a contradiction, that the desired result is false. Then there exists a smallest positive integer $m$ such that $v_1,\dots,v_m$ are linearly dependent eigenvectors of $T$ corresponding to distinct eigenvalues $\lambda_1,\dots,\lambda_m$ of $T$. The linear dependence implies there exists $a_1,\dots,a_m\in\FF$, none of which are $0$ (because of the minimality of $m$) such that
\[a_1v_1+\cdots+a_mv_m=\vb{0}.\]
Applying $T-\lambda_mI$ to both sides of the equation,
\begin{align*}
a_1(T-\lambda_mI)v_1+\cdots+a_{m-1}(T-\lambda_mI)v_{m-1}+a_m(T-\lambda_mI)v_m&=\vb{0}\\
a_1(Tv_1-\lambda_mv_1)+\cdots+a_{m-1}(Tv_{m-1}-\lambda_mv_{m-1})+a_m(Tv_m-\lambda_mv_m)&=\vb{0}\\
a_1(\lambda_1-\lambda_m)v_1+\cdots+a_{m-1}(\lambda_{m-1}-\lambda_m)v_{m-1}&=\vb{0}
\end{align*}
Since the eigenvalues $\lambda_1,\dots,\lambda_m$ are distinct, none of the coefficients $a_i(\lambda_i-\lambda_m)$ equal $0$. Thus $v_1,\dots,v_{m-1}$ are $m-1$ linearly dependent eigenvectors of $T$ corresponding to distinct eigenvalues, contradicting the minimality of $m$.
\end{proof}

\begin{corollary}
Suppose $V$ is finite-dimensional. Then each operator on $V$ has at most $\dim V$ distinct eigenvalues.
\end{corollary}

\begin{proof}
Let $T\in\mathcal{L}(V)$. Suppose $\lambda_1,\dots,\lambda_m$ are distinct eigenvalues of $T$ with corresponding eigenvectors $v_1,\dots,v_m$.

By \cref{prop:eigenvectors-linind}, the eigenvectors $v_1,\dots,v_m$ are linearly independent. Since the length of a linearly independent set is less than or equal to the length of a spanning set, we have that $m\le\dim V$, as desired.
\end{proof}

\subsection{Polynomials Applied to Operators}
\begin{notation}
Suppose $T\in\mathcal{L}(V)$, $n\in\ZZ^+$. $T^n\in\mathcal{L}(V)$ is defined by $T^n=\underbrace{T\cdots T}_\text{$m$ times}$. $T^0$ is defined to be the identity operator $I$ on $V$. If $T$ is invertible with inverse $T^{-1}$, then $T^{-n}\in\mathcal{L}(V)$ is defined by $T^{-n}=\brac{T^{-1}}^n$.
\end{notation}

Having defined powers of an operator, we can now define what it means to apply a polynomial to an operator.

\begin{definition}
Suppose $T\in\mathcal{L}(V)$, $p\in\FF[z]$ is a polynomial given by
\[p(z)=a_nz^n+\cdots+a_1z+a_0\quad(z\in\FF)\]
Then $p(T)$ is the operator on $V$ defined by
\[p(T)\coloneqq a_nT^n+\cdots+a_1T+a_0.\]
\end{definition}

\begin{remark}
If we fix an operator $T\in\mathcal{L}(V)$, then the function $\FF[z]\to\mathcal{L}(V)$ given by $p\mapsto p(T)$ is linear.
\end{remark}

\begin{definition}[Product of polynomials]
Suppose $p,q\in\FF[z]$. Then $pq\in\FF[z]$ is the polynomial defined by
\[(pq)(z)=p(z)q(z)\quad(z\in\FF)\]
\end{definition}

\begin{proposition}
Suppose $p,q\in\FF[z]$, $T\in\mathcal{L}(V)$. Then
\begin{enumerate}[label=(\roman*)]
\item $(pq)(T)=p(T)q(T)$;
\item $p(T)q(T)=q(T)p(T)$.
\end{enumerate}
\end{proposition}

\begin{remark}
This means when a product of polynomials is expanded using the distributive property, it does not matter whether the symbol is $z$ or $T$.
\end{remark}

\begin{proof} \
\begin{enumerate}[label=(\roman*)]
\item Suppose
\[p(z)=\sum_{i=0}^{m}a_iz^i,\quad q(z)=\sum_{j=0}^{n}b_jz^j\quad(z\in\FF)\]
Then
\begin{align*}
(pq)(z)&=p(z)q(z)\\
&=\brac{\sum_{i=0}^{m}a_iz^i}\brac{\sum_{j=0}^{n}b_jz^j}\\
&=\sum_{i=0}^{m}\sum_{j=0}^{n}a_ib_jz^{i+j}.
\end{align*}
Thus
\begin{align*}
(pq)(T)&=\sum_{i=0}^{m}\sum_{j=0}^{n}a_ib_jT^{i+j}\\
&=\brac{\sum_{i=0}^{m}a_iT^i}\brac{\sum_{j=0}^{n}b_jT^j}\\
&=p(T)q(T).
\end{align*}

\item Using (i) twice, we have
\[p(T)q(T)=(pq)(T)=(qp)(T)=q(T)p(T).\]
\end{enumerate}
\end{proof}

\begin{proposition}
Suppose $T\in\mathcal{L}(V)$, $p\in\FF[z]$. Then
\begin{enumerate}[label=(\roman*)]
\item $\ker p(T)$ is invariant under $T$;
\item $\im p(T)$ is invariant under $T$.
\end{enumerate}
\end{proposition}

\begin{proof} \

\begin{enumerate}[label=(\roman*)]
\item Suppose $u\in\ker p(T)$. Then $p(T)u=\vb{0}$. Thus
\[\brac{p(T)}(Tu)=\brac{p(T)T}(u)=\brac{Tp(T)}(u)=T\brac{p(T)u}=T(\vb{0})=\vb{0}.\]
Hence $Tu\in\ker p(T)$, so $\ker p(T)$ is invariant under $T$.

\item Suppose $u\in\im p(T)$. Then there exists $v\in V$ such that $u=p(T)v$. Thus
\[Tu=T\brac{p(T)v}=p(T)(Tv).\]
Hence $Tu\in\im p(T)$, so $\im p(T)$ is invariant under $T$.
\end{enumerate}
\end{proof}
\pagebreak

\section{The Minimal Polynomial}
\subsection{Existence of Eigenvalues on Complex Vector Spaces}
\begin{theorem}[Existence of eigenvalues]
Every operator on a finite-dimensional non-zero complex vector space has an eigenvalue.
\end{theorem}

\begin{proof}
Suppose $V$ is a finite-dimensional complex vector space, $\dim V=n>0$, $T\in\mathcal{L}(V)$. Let $v\in V\setminus\{\vb{0}\}$. Consider the set
\[\{v,Tv,T^2v,\dots,T^nv\}.\]
Since $\dim V=n$ and this set has length $n+1$, this set is not linearly independent. Thus there exist $a_i\in\CC$ such that
\[a_0v+\cdots+a_1Tv+a_2T^2v+\cdots+a_nT^nv=\vb{0},\]
which we can write as
\[p(T)v=\vb{0},\]
where $p(z)=a_0+a_1z+\cdots+a_nz^n$, where we pick $p$ such that $\deg p$ is minimal.

By the fundamental theorem of algebra, there exists a root of $p$ in $\CC$; let $\lambda\in\CC$ be a root of $p$. Then by the factor theorem,
\[p(z)=(z-\lambda)q(z)\quad(z\in\CC).\]
Thus
\begin{align*}
p(T)&=(T-\lambda I)q(T)\\
\vb{0}=p(T)v&=(T-\lambda I)q(T)v\\
Tq(T)v&=\lambda q(T)v
\end{align*}
Since $p$ is the minimal polynomial and $\deg q<\deg p$, we must have that $q(T)v\neq\vb{0}$. Therefore $\lambda$ is an eigenvalue of $T$, with corresponding eigenvector $q(T)v$.
\end{proof}

\begin{example}
Note that the hypothesis in the result above that $\FF=\CC$ cannot be replaced with the hypothesis that $\FF=\RR$.

Consider $T\in\mathcal{L}(\RR^2)$ defined by
\begin{equation*}\tag{$\ast$}
Tv=\begin{pmatrix}
0&1\\
-1&0
\end{pmatrix}v.
\end{equation*}
Then
\[\begin{pmatrix}
0&1\\
-1&0
\end{pmatrix}\begin{pmatrix}
1\\0
\end{pmatrix}=\begin{pmatrix}
0\\1
\end{pmatrix},\quad
\begin{pmatrix}
0&1\\
-1&0
\end{pmatrix}\begin{pmatrix}
0\\1
\end{pmatrix}=\begin{pmatrix}
-1\\0
\end{pmatrix}.\]
Notice that $T$ is a rotation, so there is no vector that is fixed in its original direction. Hence $T$ does not have an eigenvalue.

In contrast, consider $T\in\mathcal{L}(\CC^2)$ defined by ($\ast$). Then 
\[\begin{pmatrix}
0&-1\\
1&0
\end{pmatrix}\begin{pmatrix}
i\\1
\end{pmatrix}=\begin{pmatrix}
-1\\i
\end{pmatrix}=i\begin{pmatrix}
i\\1
\end{pmatrix},\]
so $i$ is an eigenvalue with corresponding eigenvector $\begin{pmatrix}i\\1\end{pmatrix}$.
\end{example}

\subsection{Eigenvalues and the Minimal Polynomial}
A \emph{monic polynomial} is a polynomial whose highest-degree coefficient equals $1$.

The following result shows the existence, uniqueness and degree of the \emph{minimal polynomial}.

\begin{lemma}
Suppose $V$ is finite-dimensional, $T\in\mathcal{L}(V)$. Then there exists a unique monic polynomial $p\in\FF[z]$ of smallest degree such that $p(T)=0$. Furthermore, $\deg p\le\dim V$.
\end{lemma}

\begin{proof} \

\fbox{Existence} Let $\dim V=n$. We use strong induction on $n$.

If $n=0$, then $I$ is the zero operator on $V$; thus take $p$ to be the constant polynomial $1$.

Now assume that $n>0$ and that the desired result holds for all operators on all vector spaces of smaller dimension. We want to construct a monic polynomial of smallest degree such that when applied to $T$ gives the $0$ operator.

Let $u\in V\setminus\{\vb{0}\}$, consider the set
\[\{u,Tu,T^2u,\dots,T^nu\}.\]
This set has length $n+1$, so it is linearly dependent. By the linear dependence lemma, there exists a smallest positive integer $m\le n$ such that $T^mu$ is a linear combination of $u,Tu,\dots,T^{m-1}u$; thus there exist $c_i\in\FF$ such that
\[c_0u+c_1Tu+\cdots+c_{m-1}T^{m-1}u+T^mu=\vb{0}.\]
Define a monic polynomial $q\in\FF[z]$ by $q(z)=c_0+c_1z+\cdots+c_{m-1}z^{m-1}+z^m$. Then $q(T)u=\vb{0}$. Thus for non-negative integer $k$,
\[q(T)\brac{T^ku}=T^k\brac{q(T)u}=T^k(\vb{0})=\vb{0}.\]
By the linear dependence lemma, $\{u,Tu,\dots,T^{m-1}u\}$ is linearly independent. Thus the above equation implies that $\dim\ker q(T)\ge m$. Hence by the fundamental theorem of linear maps,
\begin{align*}
\dim\im q(T)&=\dim V-\dim\ker q(T)\\
&\le\dim V-m.
\end{align*}
Since $\im q(T)$ is invariant under $T$, we can apply the induction hypothesis to the restriction $T|_{\im q(T)}$. Thus there exists a monic polynomial $s\in\FF[z]$ with $\deg s\le \dim V-m$ such that
\[s\brac{T|_{\im q(T)}}=0.\]
Hence for all $v\in V$ we have
\[((sq)(T))v=s(T)(q(T)v)=\vb{0}\]
because $q(T)v\in\im q(T)$ and $s(T)|_{\im q(T)}=s(T|_{\im q(T)})=0$. Thus $sq$ is a monic polynomial such that $\deg sq\le\dim V$ and $(sq)(T)=0$, as desired.

\fbox{Uniqueness} Let $p\in\FF[z]$ be a monic polynomial of smallest degree such that $p(T)=0$; let $r\in\FF[z]$ be a monic polynomial of same degree and $r(T)=0$. Then $(p-r)(T)=0$ and also $\deg(p-r)<\deg p$.

We claim that $p-r=0$. Suppose otherwise, for a contradiction, that $p-r\neq0$. Then divide $p-r$ by the coefficient of the highest-order term in $p-r$ to get a monic polynomial $s\in\FF[z]$, which satisfies $s(T)=0$ and also $\deg s=\deg(p-r)<\deg p$, a contradiction.
\end{proof}

\begin{definition}[Minimal polynomial]
Suppose $V$ is finite-dimensional, $T\in\mathcal{L}(V)$. The \vocab{minimal polynomial}\index{minimal polynomial} of $T$ is the unique monic polynomial $p\in\FF[z]$ of smallest degree such that $p(T)=0$.
\end{definition}

\begin{theorem}
Suppose $V$ is finite-dimensional, $T\in\mathcal{L}(V)$.
\begin{enumerate}[label=(\roman*)]
\item The zeros of the minimal polynomial of $T$ are eigenvalues of $T$.
\item If $V$ is a complex vector space, then the minimal polynomial of $T$ has the form
\[(z-\lambda_1)\cdots(z-\lambda_m),\]
where $\lambda_i$ are eigenvalues of $T$.
\end{enumerate}
\end{theorem}

\begin{proof}
Let $p$ be the minimal polynomial of $T$.
\begin{enumerate}[label=(\roman*)]
\item First suppose $\lambda\in\FF$ is a zero of $p$. Then $p$ can be written in the form
\[p(z)=(z-\lambda)q(z)\]
where $q$ is a monic polynomial with coefficients in $\FF$. Since $p(T)=0$, we have
\[\vb{0}=(T-\lambda I)(q(T)v)\quad(v\in V).\]
Since $\deg p<\deg p$ and $p$ is the minimal polynomial of $T$, there exists at least one $v\in V$ such that $q(T)v\neq\vb{0}$. The equation above thus implies that $\lambda$ is an eigenvalue of $T$, as desired.

To prove that every eigenvalue of $T$ is a zero of $p$, now suppose $\lambda\in\FF$ is an eigenvalue of $T$. Thus there exists $v\in V\setminus\{\vb{0}\}$ such that $Tv=\lambda v$. Repeated applications of $T$ to both sides of this equation show that $T^k v=\lambda^k v$ for every nonnegative integer $k$. Thus
\[p(T)v=p(\lambda)v.\]
Since $p$ is the minimal polynomial of $T$, we have $p(T)v=\vb{0}$. Hence the equation above implies that $p(\lambda)=0$. Thus $\lambda$ is a zero of $p$, as desired.

\item To get the desired result, use (a) and the second version of the fundamental theorem of algebra (see 4.13).
\end{enumerate}
\end{proof}

\begin{proposition}
Suppose $V$ is finite-dimensional, $T\in\mathcal{L}(V)$, $q\in\FF[z]$. Then $q(T)=0$ if and only if $q$ is a polynomial multiple of the minimal polynomial of $T$.
\end{proposition}

\begin{proposition}
Suppose $V$ is finite-dimensional, $T\in\mathcal{L}(V)$, $U\le V$ is invariant under $T$. Then the minimal polynomial of $T$ is a polynomial multiple of the minimal polynomial of $T|_U$.
\end{proposition}

\begin{corollary}
Suppose $V$ is finite-dimensional, $T\in\mathcal{L}(V)$. Then $T$ is not invertible if and only if the constant term of the minimal polynomial of $T$ is $0$.
\end{corollary}

\subsection{Eigenvalues on Odd-Dimensional Real Vector Spaces}
The next result will be the key tool that we use to show that every operator on an odd-dimensional real vector space has an eigenvalue.

\begin{proposition}
Suppose $V$ is finite-dimensional, over $\RR$. Suppose also that $T\in\mathcal{L}(V)$ and $b,c\in\RR$ with $b^2<4c$. Then $\dim\ker(T^2+bT+cI)$ is an even number.
\end{proposition}

\begin{proposition}
Every operator on an odd-dimensional vector space has an eigenvalue.
\end{proposition}
\pagebreak

\section{Upper-Triangular Matrices}
\begin{definition}[Matrix of operator]
Suppose $T\in\mathcal{L}(V)$. The matrix of $T$ with respect to a basis $\mathcal{V}=\{v_1,\dots,v_n\}$ of $V$ is the $n\times n$ matrix, whose entries $a_{ij}$ are defined by
\[Tv_j=\sum_{i=1}^{n}a_{ij}v_i.\]
\end{definition}

\begin{notation}
The notation $\mathcal{M}_\mathcal{V}(T)$ is used if the basis is not clear from the context.
\end{notation}

\begin{remark}
Operators have square matrices.
\end{remark}

\begin{definition}[Diagonal of matrix]
The \vocab{diagonal} of a square matrix consists of the entries on the line from the upper left corner to the bottom right corner.
\end{definition}

\begin{definition}[Upper-triangular matrix]
A square matrix is called \vocab{upper triangular} if all the entries below the diagonal are $0$.
\end{definition}

\begin{lemma}[Conditions for upper-triangular matrix]
Suppose $T\in\mathcal{L}(V)$, $\{v_1,\dots,v_n\}$ is a basis of $V$. Then the following are equivalent:
\begin{enumerate}[label=(\roman*)]
\item The matrix with respect to $\{v_1,\dots,v_n\}$ is upper triangular.
\item $\spn(v_1,\dots,v_k)$ is invariant under $T$ for each $k=1,\dots,n$.
\item $Tv_k\in\spn(v_1,\dots,v_k)$ for each $k=1,\dots,n$.
\end{enumerate}
\end{lemma}

\begin{lemma}
Suppose $T\in\mathcal{L}(V)$, $V$ has a basis with respect to which $T$ has an upper-triangular matrix with diagonal matrix with diagonal entries $\lambda_1,\dots,\lambda_m$. Then
\[(T-\lambda_1I)\cdots(T-\lambda_mI)=0.\]
\end{lemma}

\begin{proposition}
Suppose $T\in\mathcal{L}(V)$ has an upper-triangular matrix with respect to some basis of $V$. Then the eigenvalues of $T$ are precisely the entries on the diagonal of that upper-triangular matrix.
\end{proposition}

The following result gives a necessary and sufficient condition to have an upper-triangular matrix.

\begin{lemma}
Suppose $V$ is finite-dimensional, $T\in\mathcal{L}(V)$. Then $T$ has an upper-triangular matrix with respect to some basis of $V$ if and only if the minimal polynomial equals $(z-\lambda_1)\cdots(z-\lambda_m)$ for some $\lambda_i\in\FF$.
\end{lemma}

\begin{theorem}
Suppose $V$ is finite-dimensional complex vector space, $T\in\mathcal{L}(V)$. Then $T$ has an upper-triangular matrix with respect to some basis of $V$.
\end{theorem}
\pagebreak

\section{Diagonalisable Operators}
\begin{definition}[Diagonal matrix]
A \vocab{diagonal matrix}\index{diagonal matrix} is a square matrix that is $0$ everywhere except possibly on the diagonal.
\end{definition}

\begin{remark}
The entries on the diagonal are precisely the eigenvalue of the operator.
\end{remark}

\begin{definition}[Diagonalisable]
An operator on $V$ is called \vocab{diagonalisable} if the operator has a diagonal matrix with respect to some basis of $V$.
\end{definition}

\begin{remark}
Diagonalisation may require a different basis.
\end{remark}

\begin{definition}[Eigenspace]
Suppose $T\in\mathcal{L}(V)$, $\lambda\in\FF$. The \vocab{eigenspace}\index{eigenspace} of $T$ corresponding to $\lambda$ is the subspace of $V$ defined by
\[E(\lambda,T)\coloneqq\ker(T-\lambda I)=\{v\in V\mid Tv=\lambda v\}.\]
\end{definition}

\begin{remark}
Hence $E(\lambda,T)$ is the set of all eigenvectors of $T$ corresponding to $\lambda$, along with the $\vb{0}$ vector.
\end{remark}

\begin{proposition}
Suppose $T\in\mathcal{L}(V)$, $\lambda_1,\dots,\lambda_m$ are distinct eigenvalues of $T$. Then
\[E(\lambda_1,T)+\cdots+E(\lambda_m,T)\]
is a direct sum. Furthermore, if $V$ is finite-dimensional, then
\[\dim E(\lambda_1,T)+\cdots+\dim E(\lambda_m,T)\le\dim V.\]
\end{proposition}

\begin{lemma}[Conditions equivalent to diagonalisability]
Suppose $V$ is finite-dimensional, $T\in\mathcal{L}(V)$, $\lambda_1,\dots,\lambda_m$ are distinct eigenvalues of $T$. Then the following are equivalent:
\begin{enumerate}[label=(\roman*)]
\item $T$ is diagonalisable.
\item $V$ has a basis consisting of eigenvectors of $T$.
\item $V=E(\lambda_1,T)\oplus\cdots\oplus E(\lambda_m,T)$.
\item $\dim V=\dim E(\lambda_1,T)+\cdots+\dim E(\lambda_m,T)$.
\end{enumerate}
\end{lemma}

\begin{corollary}
Suppose $V$ is finite-dimensional, $T\in\mathcal{L}(V)$ has $\dim V$ distinct eigenvalues. Then $T$ is diagonalisable.
\end{corollary}

\begin{theorem}
Suppose $V$ is finite-dimensional, $T\in\mathcal{L}(V)$. Then $T$ is diagonalisable if and only if the minimal polynomial of $T$ equals $(z-\lambda_1)\cdots(z-\lambda_m)$ for distinct $\lambda_1,\dots,\lambda_m\in\FF$.
\end{theorem}

\begin{corollary}
Suppose $T\in\mathcal{L}(V)$ is diagonalisable, $U\le V$ is invariant under $T$. Then $T|_U$ is a diagonalisable operator on $U$.
\end{corollary}

\begin{definition}[Gershgorin disks]
Suppose $T\in\mathcal{L}(V)$, $\{v_1,\dots,v_n\}$ is a basis of $V$. Let $A$ denote the matrix of $T$ with respect to this basis. A \vocab{Gershgorin disk} of $T$ with respect to the basis $\{v_1,\dots,v_n\}$ is a set of the form
\[\crbrac{z\in\FF\:\bigg|\:|z-a_{ii}|\le\sum_{j=1,j\neq i}^{n}|a_{ij}|},\]
where $i=1,\dots,n$.
\end{definition}

\begin{theorem}[Gershgorin disk theorem]
Suppose $T\in\mathcal{L}(V)$, $\{v_1,\dots,v_n\}$ is a basis of $V$. Then each eigenvalue of $T$ is contained in some Gershgorin disk of $T$ with respect to the basis $\{v_1,\dots,v_n\}$.
\end{theorem}
\pagebreak

\section{Commuting Operators}
\begin{definition}[Commute]
Two operators $S$ and $T$ on the same vector space \vocab{commute} if $ST=TS$.

Two square matrices $A$ and $B$ of the same size commute if $AB=BA$.
\end{definition}

\begin{lemma}[commuting operators correspond to commuting matrices]
Suppose $S,T\in\mathcal{L}(V)$ and $\{v_1,\dots,v_n\}$ is a basis of $V$. Then $S$ and $T$ commute if and only if $\mathcal{M}(S;\{v_1,\dots,v_n\})$ and $\mathcal{M}(T;\{v_1,\dots,v_n\})$ commute.
\end{lemma}

\begin{lemma}[Eigenspace is invariant under commuting operator]
Suppose $S,T\in\mathcal{L}(V)$ commute, $\lambda\in\FF$. Then $E(\lambda,S)$ is invariant under $T$.
\end{lemma}

\begin{proposition}
Two diagonalizable operators on the same vector space have diagonal matrices with respect to the same basis if and only if the two operators commute.
\end{proposition}

\begin{lemma}[Common eigenvector for commuting operators]
Every pairs of commuting operators on a finite-dimensional nonzero complex vector space has a common eigenvector.
\end{lemma}

\begin{lemma}[Commuting operators are simultaneously upper triangular]
Suppose $V$ is a finite-dimensional complex vector space, $S$ and $T$ are commuting operators on $V$. Then there is a basis of $V$ with respect to which both $S$ and $T$ have upper-triangular matrices.
\end{lemma}

\begin{proposition}[Eigenvalues of sum and product of commuting operators]
Suppose $V$ is a finite-dimensional complex vector space, $S$ and $T$ are commuting operators on $V$. Then
\begin{enumerate}[label=(\roman*)]
\item every eigenvalue of $S+T$ is an eigenvalue of $S$ plus an eigenvalue of $T$;
\item every eigenvalue of $ST$ is an eigenvalue of $S$ times an eigenvalue of $T$.
\end{enumerate}
\end{proposition}
\pagebreak

\section*{Exercises}
\begin{exercise}[\cite{axler} 5A Q1]
Suppose $T\in\mathcal{L}(V)$, $U\le V$. Prove that
\begin{enumerate}[label=(\roman*)]
\item if $U\subset \ker T$, then $U$ is invariant under $T$;
\item if $\im T\subset U$, then $U$ is invariant under $T$.
\end{enumerate}
\end{exercise}

\begin{solution} \
\begin{enumerate}[label=(\roman*)]
\item 
\item Let $u\in U$. Then $Tu\in\im T\subset U$ so $Tu\in U$.
\end{enumerate}
\end{solution}