\chapter{Linear Maps}\label{chap:linear-maps}
This is one of the most important chapters in linear algebra (which justifies its length).

\section{Vector Space of Linear Maps}
\begin{definition}[Linear map]
A \vocab{linear map}\index{linear map} from $V$ to $W$ is a function $T:V\to W$ with the following properties:
\begin{enumerate}[label=(\roman*)]
\item Additivity: $T(v+w)=Tv+Tw$ for all $v,w\in V$
\item Homogeneity: $T(\lambda v)=\lambda T(v)$ for all $\lambda\in\FF$, $v\in V$
\end{enumerate}
\end{definition}

\begin{notation}
The set of all linear maps from $V$ to $W$ is denoted $\mathcal{L}(V,W)$; the set of linear transformations on $V$ is denoted $\mathcal{L}(V)$.
\end{notation}

\begin{lemma}[Linear map lemma]
Suppose $\{v_1,\dots,v_n\}$ is a basis of $V$ and $w_1,\dots,w_n\in W$. Then there exists a unique linear map $T:V\to W$ such that
\[Tv_i=w_i\quad(i=1,\dots,n)\]
\end{lemma}

\begin{proof}
First we show the existence of a linear map $T$ with the desired property. Define $T:V\to W$ by
\[T(c_1v_1+\cdots+c_nv_n)=c_1w_1+\cdots+c_nw_n,\]
for some $c_i\in\FF$. Since $\{v_1,\dots,v_n\}$ is a basis of $V$, by \cref{lemma:basis-criterion}, each $v\in V$ can be uniquely expressed as a linear combination of $v_1,\dots,v_n$, thus the equation above does indeed define a function $T:V\to W$. For $i=1,\dots,n$, take $c_i=1$ and the other $c$'s equal to $0$ to show that $Tv_i=w_i$.

We now show that $T:V\to W$ is a linear map:
\begin{enumerate}[label=(\roman*)]
\item If $u,v\in V$ with $u=a_1v_1+\cdots+a_nv_n$ and $c_1v_1+\cdots+c_nv_n$, then
\begin{align*}
T(u+v)&=T\brac{(a_1+c_1)v_1+\cdots+(a_n+c_n)v_n}\\
&=(a_1+c_1)w_1+\cdots+(a_n+c_n)w_n\\
&=(a_1w_1+\cdots+a_nw_n)+(c_1w_1+\cdots+c_nw_n)\\
&=Tu+Tv
\end{align*}
so $T$ satisfies additivity.

\item If $\lambda\in\FF$ and $v=c_1v_1+\cdots+c_nv_n$, then
\begin{align*}
T(\lambda v)&=T(\lambda c_1v_1+\cdots+\lambda c_nv_n)\\
&=\lambda c_1w_1+\cdots+\lambda c_nw_n\\
&=\lambda(c_1w_1+\cdots+c_nw_n)\\
&=\lambda Tv
\end{align*}
so $T$ satisfies homogeneity.
\end{enumerate}

To prove uniqueness, now suppose that $T\in\mathcal{L}(V,W)$ and $Tv_i=w_i$ for $i=1,\dots,n$. Let $c_i\in\FF$. The homogeneity of $T$ implies that $T(c_iv_i)=c_iw_i$. The additivity of $T$ now implies that 
\[T(c_1v_1+\cdots+c_nv_n)=c_1w_1+\cdots+c_nw_n.\]
Thus T is uniquely determined on $\spn\{v_1,\dots,v_n\}$. Since $\{v_1,\dots,v_n\}$ is a basis of $V$, this implies that $T$ is uniquely determined on $V$.
\end{proof}

\subsection{Algebraic Operations on $\mathcal{L}(V,W)$}
\begin{proposition}
$\mathcal{L}(V,W)$ is a vector space, with the operations addition and scalar multiplication defined as follows: suppose $S,T\in\mathcal{L}(V,W)$, $\lambda\in\FF$,
\begin{enumerate}[label=(\roman*)]
\item $(S+T)(v)=Sv+Tv$
\item $(\lambda T)(v)=\lambda(Tv)$
\end{enumerate}
for all $v\in V$.
\end{proposition}

\begin{proof}
Exercise.
\end{proof}

\begin{definition}[Product of linear maps]
$T\in\mathcal{L}(U,V)$, $S\in\mathcal{L}(V,W)$, then the \vocab{product} $ST\in\mathcal{L}(U,W)$ is defined by
\[(ST)(u)=S(Tu)\quad(\forall u\in U)\]
\end{definition}

\begin{remark}
In other words, $ST$ is just the usual composition $S\circ T$ of two functions.
\end{remark}

\begin{remark}
$ST$ is defined only when $T$ maps into the domain of $S$.
\end{remark}

\begin{proposition}[Algebraic properties of products of linear maps] \
\begin{enumerate}[label=(\roman*)]
\item Associativity: $(T_1T_2)T_3=T_1(T_2T_3)$ for all linear maps $T_1,T_2,T_3$ such that the products make sense (meaning that $T_3$ maps into the domain of $T_2$, $T_2$ maps into the domain of $T_1$)
\item Iidentity: $TI=IT=T$ for all $T\in\mathcal{L}(V,W)$ (the first $I$ is the identity map on $V$, and the second $I$ is the identity map on $W$)
\item Distributive: $(S_1+S_2)T=S_1T+S_2T$ and $S(T_1+T_2)=ST_1+ST_2$ for all $T,T_1,T_2\in\mathcal{L}(U,V)$ and $S,S_1,S_2\in\mathcal{L}(V,W)$
\end{enumerate}
\end{proposition}

\begin{proof}
Exercise.
\end{proof}

\begin{proposition}\label{prop:linear-map-0-0}
$T\in\mathcal{L}(V,W)$. Then $T(0)=0$.
\end{proposition}

\begin{proof}
By additivity, we have
\[T(0)=T(0+0)=T(0)+T(0).\]
Add the additive inverse of $T(0)$ to each side of the equation above to conclude that $T(0)=0$.
\end{proof}

\section{Kernel and Image}
\begin{definition}[Kernel]
For $T\in\mathcal{L}(V,W)$, the \vocab{kernel}\index{kernel} of $T$ is the subset of $V$ consisting of those vectors that $T$ maps to $0$:
\[\ker T\coloneqq\{v\in V\mid Tv=0\}.\]
\end{definition}

\begin{proposition}
$T\in\mathcal{L}(V,W)$, $\ker T$ is a subspace of $V$.
\end{proposition}

\begin{proof}
By \cref{lemma:subspace-conditions}, we check the conditions of a subspace:
\begin{enumerate}[label=(\roman*)]
\item $T(0)=0$ by \cref{prop:linear-map-0-0}, so $0\in\ker T$.
\item For all $v,w\in\ker T$, 
\[T(v+w)=Tv+Tw=0\implies v+w\in\ker T\]
so $\ker T$ is closed under addition.
\item For all $v\in\ker T$, $\lambda\in\FF$,
\[T(\lambda v)=\lambda Tv=0\implies\lambda v\in\ker T\]
so $\ker T$ is closed under scalar multiplication.
\end{enumerate}
\end{proof}

\begin{definition}[Injectivity]
$T:V\to W$ is \vocab{injective}\index{injectivity (linear map)} if
\[Tu=Tv\implies u=v.\]
\end{definition}

\begin{proposition}
$T\in\mathcal{L}(V,W)$, $T$ is injective if and only if $\ker T=0$.
\end{proposition}

\begin{proof}
First suppose $T$ is injective. Suppose $v\in\ker T$. Then
\[T(v)=0=T(0)\implies v=0\]
by the injectivity of $T$. Hence $\ker T=0$ as desired.

Now suppose $\ker T=0$. Suppose $u,v\in V$ and $Tu=Tv$. Then
\[0=Tu-Tv=T(u-v)\]
by additivity of linear map. Thus $u-v\in\ker T=0$ so $u-v=0$, which implies that $u=v$. Hence $T$ is injective, as desired.
\end{proof}

\begin{definition}[Image]
For $T:V\to W$, the \vocab{image}\index{image} of $T$ is the subset of $W$ consisting of those vectors that are of the form $Tv$ for some $v\in V$:
\[\im T\coloneqq\{Tv\mid v\in V\}.\]
\end{definition}

\begin{proposition}
$T\in\mathcal{L}(V,W)$, $\im T$ is a subspace of $W$.
\end{proposition}

\begin{proof} \
\begin{enumerate}[label=(\roman*)]
\item $T(0)=0$ implies that $0\in\im T$.
\item If $w_1,w_2\in\im T$, then there exist $v_1,v_2\in V$ such that $Tv_1=w_1$ and $Tv_2=w_2$. Thus
\[T(v_1+v_2)=Tv_1+Tv_2=w_1+w_2.\]
Hence $w_1+w_2\in im T$, so $\im T$ is closed under addition.
\item If $w\in\im T$ and $\lambda\in\FF$, then there exists $v\in V$ such that $Tv=w$. Thus
\[T(\lambda v)=\lambda Tv=\lambda w.\]
Hence $\lambda w\in\im T$, so $\im T$ is closed under scalar multiplication.
\end{enumerate}
\end{proof}

\begin{definition}[Surjectivity]
$T:V\to W$ is \vocab{surjective}\index{surjectivity (linear map)} if $\im T=W$.
\end{definition}

\subsection{Fundamental Theorem of Linear Maps}
\begin{theorem}[Fundamental Theorem of Linear Maps]
$T\in\mathcal{L}(V,W)$, then $\im T$ is finite-dimensional and
\begin{equation}
\dim V=\dim\ker T+\dim\im T.
\end{equation}
\end{theorem}

\begin{proof}
Let $\{u_1,\dots,u_m\}$ be basis of $\ker T$, then $\dim\ker T=m$. The linearly independent list $u_1,\dots,u_m$ can be extended to a basis
\[\{u_1,\dots,u_m,v_1,\dots,v_n\}\]
of $V$, thus $\dim V=m+n$. To complete the proof, we need to show that $\im T$ is finite-dimensional and $\dim\im T=n$. We do so by proving that $Tv_1,\dots,Tv_n$ is a basis of $\im T$.

Let $v\in V$. Since $\{u_1,\dots,u_m,v_1,\dots,v_n\}$ spans $V$, we can write
\[v=a_1u_1+\cdots+a_mu_m+b_1v_1+\cdots+b_nv_n,\]
where $a_i,b_i\in\FF$. Applying $T$ to both sides of the equation, we get
\[Tv=b_1Tv_1+\cdots+b_nTv_n,\]
where the terms of the form $Tu_k$ disappeared because each $u_k$ is in $\ker T$. The last equation implies that $\{Tv_1,\dots,Tv_n\}$ spans $\im T$. In particular, $\im T$ is finite-dimensional.

To show $Tv_1,\dots,Tv_n$ is linearly independent, suppose $c_1,\dots,c_n\in\FF$ and
\[c_1Tv_1+\cdots+c_nTv_n=0.\]
Then
\[T(c_1v_1+\cdots+c_nv_n)=0.\]
and so $c_1v_1+\cdots+c_nv_n\in\ker T$. Since $u_1,\dots,u_m$ spans $\ker T$, we can write
\[c_1v_1+\cdots+c_nv_n=d_1u_1+\cdots+d_mu_m\]
where $d_i\in\FF$. This implies that all the c's (and d's) are $0$ (because $u_1,\dots,u_m,v_1,\dots,v_n$ is linearly independent). Thus $Tv_1,\dots,Tv_n$ is linearly independent and hence is a basis of $\im T$, as desired.
\end{proof}

Now we can show that no linear map from a fnite-dimensional vector space to a ``smaller'' vector space can be injective, where ``smaller'' is measured by dimension.

\begin{proposition}
Suppose $V$ and $W$ are fnite-dimensional vector spaces such that $\dim V>\dim W$. Then no linear map from $V$ to $W$ is injective.
\end{proposition}

\begin{proof}
Let $T\in\mathcal{L}(V,W)$. Then
\begin{align*}
\dim\ker T
&=\dim V-\dim\im T\\
&\ge\dim V-\dim W\\
&>0
\end{align*}
where the frst line above comes from the fundamental theorem of linear maps and the second line follows from 2.37. The inequality above states that $\dim\ker T>0$. This means that $\ker T$ contains vectors other than $0$. Thus $T$ is not injective.
\end{proof}

The next result shows that no linear map from a finite-dimensional vector space to a ``bigger'' vector space can be surjective, where ``bigger'' is measured by dimension.

\begin{proposition}
Suppose $V$ and $W$ are finite-dimensional vector spaces such that $\dim V<\dim W$. Then no linear map from $V$ to $W$ is surjective.
\end{proposition}

\begin{proof}

\end{proof}

\section{Matrices}
\begin{definition}[Matrix]
Suppose $m,n\in\NN$. A $m\times n$ \vocab{matrix}\index{matrix} $A$ is a rectangular array with $m$ rows and $n$ columns:
\[A=\begin{pmatrix}
a_{11} & \cdots & a_{1n}\\
\vdots & & \vdots\\
a_{m1} & \cdots & a_{mn}
\end{pmatrix}\]
where $a_{ij}\in\FF$. We also denote $A=(a_{ij})_{m\times n}$, and drop the subscript if there is no ambiguity.
\end{definition}

\begin{notation}
$M_{m\times n}(\FF)$ denotes the set of all $m\times n$ matrices with entries in $\FF$.
\end{notation}

\begin{definition}[Matrix of a linear map]
Suppose $T\in\mathcal{L}(V,W)$, $\{v_1,\dots,v_n\}$ is a basis of $V$, $\{w_1,\dots,w_m\}$ is a basis of $W$. The \vocab{matrix} of $T$\index{matrix of linear map} with respect to these bases is the $m\times n$ matrix $\mathcal{M}(T)$ whose entries $a_{ij}$ are defined by
\[Tv_j=a_{1j}w_1+\cdots+a_{mj}w_m.\]
\end{definition}

\begin{notation}
If the bases of $V$ and $W$ are not clear from the context, we adopt the notation $\mathcal{M}\brac{T,\{v_1,\dots,v_n\},\{w_1,\dots,w_m\}}$.
\end{notation}

That is,
\begin{align*}
Tv_1&=a_{11}w_1+a_{21}w_2+\cdots+a_{m1}w_m\\
Tv_2&=a_{12}w_1+a_{22}w_2+\cdots+a_{m2}w_m\\
&\vdots\\
Tv_n&=a_{1n}w_1+a_{2n}w_2+\cdots+a_{mn}w_m
\end{align*}

Thus we can write
\begin{align*}
\mathcal{M}(T)&=T\begin{pmatrix}
v_1&v_2&\cdots&v_n
\end{pmatrix}\\
&=\begin{pmatrix}
w_1&w_2&\cdots&w_m
\end{pmatrix}
\begin{pmatrix}
a_{11}&a_{12}&\cdots&a_{1n}\\
a_{21}&a_{22}&\cdots&a_{2n}\\
\vdots&\vdots&\ddots&\vdots\\
a_{m1}&a_{m2}&\cdots&a_{mn}
\end{pmatrix}\\
&=\begin{pmatrix}
\sum_{k=1}^{m}a_{k1}w_k & \cdots & \sum_{k=1}^{m}a_{kn}w_k
\end{pmatrix}
\end{align*}

\begin{definition}[Matrix addition]
The sum of two matrices of the same size is the matrix obtained by adding corresponding entries in the matrices:
\[\begin{pmatrix}
a_{11}&\cdots&a_{1n}\\
\vdots&&\vdots\\
a_{m1}&\cdots&a_{mn}
\end{pmatrix}+
\begin{pmatrix}
c_{11}&\cdots&c_{1n}\\
\vdots&&\vdots\\
c_{m1}&\cdots&c_{mn}
\end{pmatrix}=
\begin{pmatrix}
a_{11}+c_{11}&\cdots&a_{1n}+c_{1n}\\
\vdots&&\vdots\\
a_{m1}+c_{m1}&\cdots&a_{mn}+c_{mn}
\end{pmatrix}.\]
\end{definition}

\begin{proposition}
Suppose $S,T\in\mathcal{L}(V,W)$. Then $\mathcal{M}(S+T)=\mathcal{M}(S)+\mathcal{M}(T)$.
\end{proposition}

\begin{definition}[Scalar multiplication of matrix]
The product of a scalar and a matrix is the matrix obtained by multiplying each entry in the matrix by the scalar:
\[\lambda\begin{pmatrix}
a_{11}&\cdots&a_{1n}\\
\vdots&&\vdots\\
a_{m1}&\cdots&a_{mn}
\end{pmatrix}=
\begin{pmatrix}
\lambda a_{11}&\cdots&\lambda a_{1n}\\
\vdots&&\vdots\\
\lambda a_{m1}&\cdots&\lambda a_{mn}
\end{pmatrix}.\]
\end{definition}

\begin{proposition}
Suppose $\lambda\in\FF$ and $T\in\mathcal{L}(V,W)$. Then $\mathcal{M}(\lambda T)=\lambda \mathcal{M}(T)$.
\end{proposition}

\begin{proposition}
With addition and scalar multiplication
defned as above, $M_{m\times n}(\FF)$ is a vector space of dimension $mn$.
\end{proposition}

\begin{definition}[Matrix multiplication]
Given $A=(a_{ij})_{m\times n}$, $B=(b_{ij})_{n\times p}$. Then
\[AB=\brac{\sum_{k=1}^{n}a_{ik}b_{kj}}_{m\times p}\]
\end{definition}

\begin{notation}
$A_{i,\cdot}$ denotes the row vector corresponding to the $i$-th row of $A$; $A_{\cdot,j}$ denotes the column vector corresponding to the $j$-th column of $A$.
\end{notation}

\begin{proposition}
Suppose $A$ is $m\times n$ matrix, $B$ is $n\times p$ matrix. Then
\[(AB)_{j,k}=A_{j,\cdot}B_{\cdot,k}\]
for $i\le j\le m$, $1\le k\le p$. In other words, the entry in row $j$, column $k$ of $AB$ equals (row $j$ of $A$) times (column $k$ of $B$).
\end{proposition}

\begin{proposition}
Suppose $A$ is $m\times n$ matrix, $B$ is $n\times p$ matrix. Then
\[(AB)_{\cdot,k}=AB_{\cdot,k}\]
for $1\le k\le p$. In other words, column $k$ of $AB$ equals $A$ times column $k$ of $B$.
\end{proposition}

\begin{proposition}[Linear combination of columns]
Suppose $A$ is an $m\times n$ matrix, $b=\begin{pmatrix}
b_1\\\cdots\\b_n
\end{pmatrix}$. Then
\[Ab=b_1A_{\cdot,1}+\cdots+b_nA_{\cdot,n}.\]
In other words, $Ab$ is a linear combination of the columns of $A$, with the scalars that multiply the columns coming from $b$.
\end{proposition}

\begin{proposition}
If $T\in\mathcal{L}(U,V)$ and $S\in\mathcal{L}(V,W)$, then $\mathcal{M}(ST)=\mathcal{M}(S)\mathcal{M}(T)$.
\end{proposition}

\begin{definition}[Column rank, row rank]
Given $A\in M_{m\times n}(\FF)$, column vectors are
\[\{A(\cdot,1),\dots,A(\cdot,n)\}\]
and row vectors are
\[\{A(1,\cdot),\dots,A(m,\cdot)\}.\]
Then we define the \vocab{column rank} $c$ by
\[c=\dim\spn\{A(\cdot,k)\mid1\le k\le n\}\]
and \vocab{row rank} $r$ by
\[r=\dim\spn\{A(k,\cdot)\mid1\le k\le m\}.\]
\end{definition}

\begin{definition}[Transpose]
Suppose $A\in M_{m\times n}(\FF)$, then the \vocab{transpose} $A^T\in M_{n\times m}(\FF)$ is given by
\[A^T(i,j)=A(j,i).\]
\end{definition}

\begin{proposition}[Column-rank factorisation]
Suppose $A\in M_{m\times n}(\FF)$, column rank $c\ge1$. Then $A=CR$ where $C\in M_{m\times c}(\FF)$, $R\in M_{c\times n}(\FF)$.
\end{proposition}

\begin{proposition}[Column rank equals row rank]
Suppose $A\in M_{m\times n}(\FF)$. Then the column rank of $A$ equals to row rank of $A$.
\end{proposition}

Since column rank equals row rank, we can dispense with the terms ``column rank'' and ``row rank'', and just use the simpler
term ``rank''.

\begin{definition}[Rank]
The \vocab{rank}\index{rank} of a matrix $A\in M_{m\times n}(\FF)$ is the column rank of $A$.
\end{definition}

\section{Invertibility and Isomorphism}
\begin{definition}[Invertibility]
$T\in\mathcal{L}(V,W)$ is \vocab{invertible} if there exists $S\in\mathcal{L}(W,V)$ such that $ST=I_V$, $TS=I_W$, where $I_V$ and $I_W$ are the \vocab{identity maps} on $V$ and $W$ respectively; $S$ is known as the \vocab{inverse} of $T$.
\end{definition}

\begin{proposition}[Uniqueness of inverse]
An invertible linear map has a unique inverse.
\end{proposition}

\begin{proof}
Suppose $T\in\mathcal{L}(V,W)$ is invertible. $S_1$ and $S_2$ are inverses of $T$. Then
\[S_1=S_1I=S_1(TS_2)=(S_1T)S_2=IS_2=S_2.\]
Thus $S_1=S_2$.
\end{proof}

Now that we know that the inverse is unique, we can give it a notation.

\begin{notation}
If $T$ is invertible, then its inverse is denoted by $T^{-1}$.
\end{notation}

\begin{proposition}
$T\in\mathcal{L}(V,W)$ is invertible if and only if it is injective and surjective.
\end{proposition}

\begin{proof} \

\fbox{$\implies$} Suppose $T\in\mathcal{L}(V,W)$, where $Tu=Tv$, is invertible. Then there exists an inverse $T^{-1}$ such that
\[u=T^{-1}Tu=T^{-1}Tv=v.\]
To show $T$ is surjective, we have that for any $w\in W$, $w=T(T^{-1}w)$.

\fbox{$\impliedby$} Define $S\in\mathcal{L}(W,V)$ such that for each $w\in W$, $S(W)$ is the unique element such that $T(S(w))w$ (we can do this due to injectivity and surjectivity). Then we have that $T(ST)v=(TS)Tv=Tv$ and thus $STv=v$ so $ST=I$. It is easy to show that $S$ is a linear map.
\end{proof}

\begin{proposition}
Suppose that $V$ and $W$ are finite-dimensional vector spaces, $\dim V=\dim W$, and $T\in\mathcal{L}(V,W)$. Then
\[T\text{ is invertible}\iff T\text{ is injective}\iff T\text{ is surjective}.\]
\end{proposition}

\begin{corollary}
Suppose $V$ and $W$ are finite-dimensional vector spaces of the same dimension, $S\in\mathcal{L}(W,V)$, $T=\mathcal{L}(V,W)$. Then $ST=I$ if and only if $TS=I$.
\end{corollary}

\begin{definition}[Isomorphism]
An \vocab{isomorphism}\index{isomorphism (linear map)} is an invertible linear map. Two vector spaces $V$ and $W$ are \vocab{isomorphic} if there exists an isomorphism from one vector space onto the other one, denoted by $V\cong W$.
\end{definition}

\begin{remark}
Think of an isomorphism $T:V\to W$ as relabeling $v\in V$ as $Tv\in W$. This viewpoint explains why two isomorphic vector spaces have the same vector space properties. Isomorphism essentially means that two vector spaces are essentially the same.
\end{remark}

The following result shows that we need to look at only at the dimension to determine whether two vector spaces are isomorphic.

\begin{lemma}
Two finite-dimensional vector spaces $V$ and $W$ are isomorphic if and only if they have the same dimension:
\[V\cong W\iff\dim V=\dim W.\]
\end{lemma}

\begin{proposition}
Suppose $\{v_1,\dots,v_n\}$ is a basis of $V$, $\{w_1,\dots,w_m\}$ is a basis of $W$. Then $M$ is an isomorphism between $\mathcal{L}(V,W)$ and $\FF^{m,n}$.
\end{proposition}

\begin{corollary}\label{cor:dimension-product}
Suppose $V$ and $W$ are finite-dimensional. Then $\mathcal{L}(V,W)$ is finite-dimensional and
\[\dim\mathcal{L}(V,W)=(\dim V)(\dim W).\]
\end{corollary}

Previously we defned the matrix of a linear map. Now we defne the matrix of a vector.

\begin{definition}[Matrix of a vector]
Suppose $v\in V$, $\{v_1,\dots,v_n\}$ is a basis of $V$. The matrix of $v$\index{matrix of vector} with respect to this basis is the $n\times1$ matrix
\[\mathcal{M}(v)=\begin{pmatrix}
b_1\\\vdots\\b_n
\end{pmatrix}\]
where $b_1,\dots,b_n$ are scalars such that
\[v=b_1v_1+\cdots+b_nv_n.\]
\end{definition}

\begin{remark}
The matrix $\mathcal{M}(v)$ of a vector $v\in V$ depends on the basis $\{v_1,\dots,v_n\}$ and $v$. We can think of elements of $V$ as relabelled to $n\times1$ matrices, i.e., $V\to\FF^{n,1}$.
\end{remark}

\begin{lemma}
$T\in\mathcal{L}(V,W)$, $\{v_1,\dots,v_n\}$ is basis of $V$, $\{w_1,\dots,w_m\}$ is basis of $W$. Then $\mathcal{M}(T)_{\cdot,k}=\mathcal{M}(Tv_k)$ for $k=1,\dots,n$.
\end{lemma}

The following result shows that linear maps act like matrix multiplication.

\begin{proposition}
$T\in\mathcal{L}(V,W)$, $v\in V$. $\{v_1,\dots,v_n\}$ is basis of $V$, $\{w_1,\dots,w_m\}$ is basis of $W$. Then
\[\mathcal{M}(Tv)=\mathcal{M}(T)\mathcal{M}(v).\]
\end{proposition}

\begin{proof}
Applying the previous result,
\begin{align*}
\mathcal{M}(Tv)&=b_1\mathcal{M}(Tv_1)+\cdots+b_n\mathcal{M}(Tv_n)\\
&=b_1\mathcal{M}(T)_{\cdot,1}+\cdots+b_n\mathcal{M}(T)_{\cdot,n}\\
&=\mathcal{M}(T)\mathcal{M}(v).
\end{align*}
\end{proof}

\begin{proposition}
Suppose $V$ and $W$ are finite-dimensional, $T\in\mathcal{L}(V,W)$. Then
\[\dim\ker T=\rank \mathcal{M}(T).\]
\end{proposition}

\begin{definition}[Identity matrix]
For positive integer $n$, the $n\times n$ matrix
\[\begin{pmatrix}
1&&0\\
&\ddots&\\
0&&1
\end{pmatrix}\]
is called the \vocab{identity matrix}, denoted by $I_n$.
\end{definition}

\begin{definition}[Invertibility]
A square matrix $A$ is called invertible if there is a square matrix $B$ of the same size such that $AB=BA=I$; we call $B$ the inverse of $A$ and denote it by $A^{-1}$.
\end{definition}

\section{Products and Quotients of Vector Spaces}
\begin{definition}[Product]
Suppose $V_1,\dots,V_n$ are vector spaces over $\FF$. The \vocab{product}\index{product of vector spaces} $V_1\times\cdots\times V_n$ is defined by
\[V_1\times\cdots\times V_n=\{(v_1,\dots,v_n)\mid v_i\in V_i\}.\]
\end{definition}

\begin{proposition}
$V_1\times\cdots\times V_n$ is a vector space over $\FF$, with addition and scalar multiplication defined by
\begin{align*}
(u_1,\dots,u_n)+(v_1,\dots,v_n)&=(u_1+v_1,\dots,u_n+v_n)\\
\lambda(v_1,\dots,v_n)&=(\lambda v_1,\dots,\lambda v_n)
\end{align*}
\end{proposition}

The following result shows that the dimension of a product is the sum of dimensions.

\begin{proposition}
Suppose $V_1,\dots,V_n$ are finite-dimensional vector spaces. Then $V_1\times\cdots\times V_n$ is finite-dimensional and
\[\dim(V_1\times\cdots\times V_n)=\dim V_1+\cdots+\dim V_n.\]
\end{proposition}

Products are also related to direct sums, by the following result.

\begin{lemma}
Suppose that $V_1,\dots,V_n$ are subspaces of $V$. Define a linear map $\Gamma:V_1\times\cdots\times V_n\to V_1+\cdots+V_n$ by
\[\Gamma(v_1,\dots,v_n)=v_1+\cdots+v_n.\]
Then $V_1+\cdots+V_n$ is a direct sum if and only if $\Gamma$ is injective.
\end{lemma}

The next result says that a sum is a direct sum if and only if dimensions add up.

\begin{proposition}
Suppose $V$ is finite-dimensional and $V_1,\dots,V_n$ are subspaces of $V$. Then $V_1+\cdots+V_n$ is a direct sum if and only if
\[\dim(V_1+\cdots+V_n)=\dim V_1+\cdots+\dim V_n.\]
\end{proposition}

\begin{definition}[Coset]
Suppose $v\in V$, $U\subset V$. Then $v+U$ is called a \vocab{coset}\index{coset} of $U$, defined by
\[v+U\coloneqq\{v+u\mid u\in U\}.\]
\end{definition}

\begin{definition}[Quotient space]
Suppose $U\le V$. Then the \vocab{quotient space}\index{quotient space} $V/U$ is the set of all cosets of $U$:
\[V/U\coloneqq\{v+U\mid v\in V\}.\]
\end{definition}

\begin{example}
If $U=\{(x,2x)\in\RR^2\mid x\in\RR\}$, then $\RR^2/U$ is the set of all lines in $\RR^2$ that have gradient of $2$.
\end{example}

It is obvious that two translates of a subspace are equal or disjoint. We shall now prove this.

\begin{proposition}
Suppose $U\le V$, and $v,w\in V$. Then
\end{proposition}

\begin{proposition}
Suppose $U\le V$. Then $V/U$ is a vector space, with addition and scalar multiplication defined by
\begin{align*}
(v+U)+(w+U)&=(v+w)+U\\
\lambda(v+U)&=(\lambda v)+U
\end{align*}
for all $v,w\in V$, $\lambda\in\FF$.
\end{proposition}

\begin{definition}[Quotient map]
Suppose $U\le V$. The \vocab{quotient map}\index{quotient map} $\pi:V\to V/U$ is the linear map defined by
\[\pi(v)=v+U\]
for all $v\in V$.
\end{definition}

\begin{proposition}[Dimension of quotient space]
Suppose $V$ is finite-dimensional, $U\le V$. Then
\[\dim V/U=\dim V-\dim U.\]
\end{proposition}

\begin{definition}
Suppose $T\in\mathcal{L}(V,W)$. Define $\tilde{T}:V/\ker T\to W$ by
\[\tilde{T}(v+\ker T)=Tv.\]
\end{definition}

\section{Duality}
\subsection{Dual Space and Dual Map}
Linear maps into the scalar field $\FF$ play a special role in linear algebra, and thus they get a special name.

\begin{definition}[Linear funtional]
A \vocab{linear functional}\index{linear functional} on $V$ is a linear map from $V$ to $\FF$; that is, a linear functional is an element of $\mathcal{L}(V,\FF)$.
\end{definition}

The vector space $\mathcal{L}(V,\FF)$ also gets a special name and special notation.

\begin{definition}[Dual space]
The \vocab{dual space} of $V$, denoted by $V^*$, is the vector space of all linear functionals on $V$; that is, $V^*=\mathcal{L}(V,\FF)$.
\end{definition}

\begin{lemma}
Suppose $V$ is finite-dimensional. Then $V^*$ is also finite-dimensional, and
\[\dim V^*=\dim V.\]
\end{lemma}

\begin{proof}
By , we have
\[\dim V^*=\dim\mathcal{L}(V,\FF)=(\dim V)(\dim\FF)=\dim V\]
as desired.
\end{proof}

\begin{definition}[Dual basis]
If $\{v_1,\dots,v_n\}$ is a basis of $V$, then the \vocab{dual basis}\index{dual basis} of $\{v_1,\dots,v_n\}$ is the list $\{\phi_1,\dots,\phi_n\}$ of elements of $V^*$, where each $\phi_i$ is the linear functional on $V$ such that
\[\phi_i(v_j)=\begin{cases}
1&\text{if }i=j,\\
0&\text{if }i\neq j.
\end{cases}\]
\end{definition}

The following result states that dual basis gives coefficients for linear combination.

\begin{proposition}
Suppose $\{v_1,\dots,v_n\}$ is a basis of $V$, and $\{\phi_1,\dots,\phi_n\}$ is the dual basis. Then for each $v\in V$,
\[v=\phi_1(v)v_1+\cdots+\phi_n(v)v_n.\]
\end{proposition}

The following result states that the dual basis is a basis of the dual space.

\begin{proposition}
Suppose $V$ is finite-dimensional. Then the dual basis of a basis of $V$ is a basis of $V^*$.
\end{proposition}

\begin{definition}[Dual map]
Suppose $T\in\mathcal{L}(V,W)$. The \vocab{dual map}\index{dual map} of $T$ is the linear map $T^*\in\mathcal{L}(V,W)$ defined for each $\phi\in W^*$ by
\[T^*(\phi)=\phi\circ T.\]
\end{definition}

\begin{proposition}[Algebraic properties of dual map]
Suppose $T\in\mathcal{L}(V,W)$. Then
\begin{enumerate}[label=(\arabic*)]
\item $(S+T)^*=S^*+T^*$ for all $S\in\mathcal{L}(V,W)$
\item $(\lambda T)^*=\lambda T^*$ for all $\lambda\in\FF$
\item $(ST)^*=T^* S^*$ for all $S\in\mathcal{L}(V,W)$
\end{enumerate}
\end{proposition}

\subsection{Kernel and Image of Dual of Linear Map}
Our goal in this subsection is to describe $\ker T^*$ and $\im T^*$ in terms of $\im T$ and $\ker T$. To do this, we will need the next definition.

\begin{definition}[Annihilator]
For $U\subseteq V$, the \vocab{annihilator}\index{annihilator} of $U$ is defined by
\[\Ann(U)\coloneqq\{\phi\in V^*\mid\phi(u)=0,\forall u\in U\}.\]
\end{definition}

From the definition, you can probably see that the name is rather fitting.

\begin{proposition}
$\Ann(U)$ is a subspace of $V$.
\end{proposition}

\begin{proposition}[Dimension of annihilator]
Suppose $V$ is finite-dimensional, $U\le V$. Then
\[\dim\Ann(U)=\dim V-\dim U.\]
\end{proposition}

The following are conditions for the annihilator to equal $\{\vb{0}\}$ or the whole space.

\begin{proposition}
Suppose $V$ is finite-dimensional, $U\le V$. Then
\begin{enumerate}[label=(\roman*)]
\item $\Ann(U)=\{\vb{0}\}\iff U=V$;
\item $\Ann(U)=V^*\iff U=\{\vb{0}\}$.
\end{enumerate}
\end{proposition}

The following result concerns $\ker T^*$. Note that the proof of (1) does not use the hypothesis that $V$ and $W$ are finite-dimensional.

\begin{proposition}
Suppose $V$ and $W$ are finite-dimensional, $T\in\mathcal{L}(V,W)$. Then
\begin{enumerate}[label=(\roman*)]
\item $\ker T^*=\Ann(\im T)$;
\item $\dim\ker T^*=\dim\ker T+\dim W-\dim V$.
\end{enumerate}
\end{proposition}

The next result can be useful because sometimes it is easier to verify that $T^*$ is injective than to show directly that $T$ is surjective.

\begin{proposition}
Suppose $V$ and $W$ are finite-dimensional, $T\in\mathcal{L}(V,W)$. Then
\[T\text{ is surjective}\iff T^*\text{ is injective.}\]
\end{proposition}

The following result concerns $\im T^*$.

\begin{proposition}
Suppose $V$ and $W$ finite-dimensional, $T\in\mathcal{L}(V,W)$. Then
\begin{enumerate}[label=(\roman*)]
\item $\dim\im T^*=\dim\im T$;
\item $\dim T^*=\Ann(\ker T)$.
\end{enumerate}
\end{proposition}

\begin{proposition}
Suppose $V$ and $W$ are finite-dimensional, $T\in\mathcal{L}(V,W)$. Then
\[T\text{ is injective}\iff T^*\text{ is surjective.}\]
\end{proposition}

\subsection{Matrix of Dual of Linear Map}
\begin{proposition}
Suppose $V$ and $W$ are finite-dimensional, $T\in\mathcal{L}(V,W)$. Then
\[\mathcal{M}(T^*)=\brac{\mathcal{M}(T)}^t.\]
\end{proposition}


\pagebreak

\section*{Problems}
3A
\begin{prbm}
Suppose $b,c\in\RR$. Define $T:\RR^3\to\RR^2$ by
\[T(x,y,z)=(2x-4y+3z+b,6x+cxyz).\]
Show that $T$ is linear if and only if $b=c=0$.
\end{prbm}

\begin{solution}

\end{solution}

3D
\begin{prbm}
Suppose $T\in\mathcal{L}(V,W)$ is invertible. Show that $T^{-1}$ is invertible and
\[\brac{T^{-1}}^{-1}=T.\]
\end{prbm}

\begin{solution}
$T^{-1}$ is invertible because there exists $T$ such that $TT^{-1}=T^{-1}T=I$. So
\[T^{-1}T=TT^{-1}=I\]
thus $\brac{T^{-1}}^{-1}=T$.
\end{solution}

\begin{prbm}
Suppose $T\in\mathcal{L}(U,V)$ and $S\in\mathcal{L}(V,W)$ are both invertible linear maps. Prove that $ST\in\mathcal{L}(U,W)$ is invertible and that $(ST)^{-1}=T^{-1}S^{-1}$.
\end{prbm}

\begin{solution}
\[(ST)(T^{-1}S^{-1})=S(TT^{-1})S^{-1}=I=T^{-1}S^{-1}ST.\]
\end{solution}

\begin{prbm}
Suppose $V$ is finite-dimensional and $T\in\mathcal{L}(V,W)$. Prove that the following are equivalent:
\begin{enumerate}[label=(\roman*)]
\item $T$ is invertible;
\item $\{Tv_1,\dots,Tv_n\}$ is a basis of $V$ for every basis $\{v_1,\dots,v_n\}$ of $V$;
\item $\{Tv_1,\dots,Tv_n\}$ is a basis of $V$ for some basis $\{v_1,\dots,v_n\}$ of $V$.
\end{enumerate}
\end{prbm}

\begin{solution} \

(i)$\implies$(ii) It only suffices to prove linear independence. We can show this
\[a_1Tv_1+\cdots+a_nTv_n=0\iff a_1v_1+\cdots+a_nv_n=0\]
since $T$ is injective and thus the only solution is all $a_i$ are identically zero.

(ii)$\implies$(iii) Trivial.

(iii)$\implies$(i) By the linear map lemma, there exists $S\in\mathcal{L}(V)$ such that $S(Tv_i)=v_i$ for all $i$. Such $S$ is the inverse of $T$ (one can verify) and thus $T$ is invertible.
\end{solution}

3E
\begin{prbm}
Suppose $U\le V$, $V/U$ is finite-dimensional. Prove that $V$ is isomorphic to $U\times(V/U)$.
\end{prbm}

\begin{solution}
\[\dim V=\dim U+(\dim V-\dim U)=\dim U+\dim(V/U).\]
\end{solution}